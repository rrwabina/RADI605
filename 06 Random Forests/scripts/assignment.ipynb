{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RADI605: Modern Machine Learning**\n",
    "\n",
    "### Assignment: Random Forests\n",
    "**Romen Samuel Rodis Wabina** <br>\n",
    "Student, PhD Data Science in Healthcare and Clinical Informatics <br>\n",
    "Clinical Epidemiology and Biostatistics, Faculty of Medicine (Ramathibodi Hospital) <br>\n",
    "Mahidol University\n",
    "\n",
    "Note: In case of Python Markdown errors, you may access the assignment through this GitHub [Link](https://github.com/rrwabina/RADI605/blob/main/05%20Adaptive%20Boosting/scripts/assignment.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "from numpy import mean\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from mlxtend.plotting import plot_learning_curves\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_binary(df, columns = ['Risk1Yr']):\n",
    "    for column in columns:\n",
    "        df[column] = df[column].apply(lambda x: 0 if x == 'F' else 1)\n",
    "    return df\n",
    "\n",
    "def load_thoracic(path = '../data/ThoraricSurgery.csv'):\n",
    "    data = pd.read_csv(path)\n",
    "    data = data[data.columns[2:]]\n",
    "    data = data.drop(['PRE6', 'PRE14'], axis = 1)\n",
    "    label_columns = data.columns[2:12]\n",
    "    data = convert_binary(data, columns = ['Risk1Yr'])\n",
    "    data = convert_binary(data, columns = label_columns)\n",
    "    include_columns = data.columns[0:-1]\n",
    "    X, y = data[include_columns], data['Risk1Yr']\n",
    "    X, y = X.to_numpy(), y.to_numpy()\n",
    "    y[y == 0] = -1\n",
    "    return X, y, data\n",
    "\n",
    "X, y, data = load_thoracic()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When features are highly correlated, they contain redundant information, and the Random Forest may use the same feature in many of the trees, leading to overfitting. Additionally, highly correlated features can cause instability in the feature importance scores and make it difficult to interpret the results.\n",
    "\n",
    "It's always a good idea to check for highly correlated features before training a Random Forest and remove or combine them as needed. This can be done using techniques such as principal component analysis (PCA) or feature selection. By reducing the number of highly correlated features, you can simplify the model and make it more interpretable, and you may also see an improvement in performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_binary(df, columns = ['Risk1Yr']):\n",
    "    for column in columns:\n",
    "        df[column] = df[column].apply(lambda x: 0 if x == 'F' else 1)\n",
    "    return df\n",
    "\n",
    "def load_thoracic(path = '../data/ThoraricSurgery.csv'):\n",
    "    data = pd.read_csv(path)\n",
    "    data = data[data.columns[2:]]\n",
    "    data = data.drop(['PRE6', 'PRE14'], axis = 1)\n",
    "    label_columns = data.columns[2:12]\n",
    "    data = convert_binary(data, columns = ['Risk1Yr'])\n",
    "    data = convert_binary(data, columns = label_columns)\n",
    "    include_columns = data.columns[0:-1]\n",
    "    X, y = data[include_columns], data['Risk1Yr']\n",
    "    X, y = X.to_numpy(), y.to_numpy()\n",
    "    y[y == 0] = -1\n",
    "    return X, y, data\n",
    "\n",
    "def split_data(X, y, pca_included = False, smote_included = False):\n",
    "    print('Model Assumptions:')\n",
    "    if pca_included is True:\n",
    "        print('\\t The dataset used PCA for dimensionality reduction.')\n",
    "        pca = PCA(n_components = 8)\n",
    "        X = pca.fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "    if smote_included is True:\n",
    "        print('\\t The dataset used SMOTE to rectify class imbalance.')\n",
    "        smote = SMOTE(sampling_strategy = 'minority', k_neighbors = 5, random_state = 42)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "    else:\n",
    "        print('\\t The dataset did NOT use SMOTE.')\n",
    "        X_resampled, y_resampled = X_train, y_train \n",
    "    return X_resampled, y_resampled, X_test, y_test\n",
    "\n",
    "def init_parameters():\n",
    "    param_grid = { \n",
    "                'n_estimators': [10, 2000], \n",
    "                'criterion': ['gini', 'entropy'],\n",
    "                'max_depth': np.arange(1, 20),\n",
    "                'min_samples_split': np.arange(1, 5)\n",
    "              }\n",
    "    return param_grid\n",
    "\n",
    "def validation(rsearch, X_test, y_test):\n",
    "    predictions = rsearch.predict(X_test)\n",
    "    print('Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, predictions))\n",
    "    print(classification_report(y_test, predictions))\n",
    "    return predictions\n",
    "\n",
    "def train_randomforest(X_train, y_train, X_test, y_test, use_randomsearch = False, print_params = False):\n",
    "    param_grid = init_parameters()\n",
    "    if use_randomsearch is True:\n",
    "        print('\\t This model has been cross-validated through Random Search.')\n",
    "        rsearch = RandomizedSearchCV(estimator = RandomForestClassifier(), \n",
    "                                     param_distributions = param_grid, \n",
    "                                     cv = 10, n_iter = 10)\n",
    "        rsearch.fit(X_train, y_train)\n",
    "        if print_params is True:\n",
    "            print(rsearch.best_params_)\n",
    "        predictions = validation(rsearch, X_test, y_test)\n",
    "    else:\n",
    "        print('\\t This model did NOT cross-validate through Random Search.')\n",
    "        rsearch = RandomForestClassifier(criterion = 'gini', n_estimators = 100, \n",
    "                                         max_depth = 9, min_samples_split = 4)\n",
    "        rsearch.fit(X_train, y_train)\n",
    "        predictions = validation(rsearch, X_test, y_test)\n",
    "    return predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Assumptions:\n",
      "\t The dataset did NOT use SMOTE.\n",
      "\t This model did NOT cross-validate through Random Search.\n",
      "Confusion Matrix: \n",
      "[[75  0]\n",
      " [19  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.80      1.00      0.89        75\n",
      "           1       0.00      0.00      0.00        19\n",
      "\n",
      "    accuracy                           0.80        94\n",
      "   macro avg       0.40      0.50      0.44        94\n",
      "weighted avg       0.64      0.80      0.71        94\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, y, data = load_thoracic()\n",
    "X_train, y_train, X_test, y_test = split_data(X, y, pca_included = False, smote_included = False)\n",
    "predictions = train_randomforest(X_train, y_train, X_test, y_test, use_randomsearch = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Assumptions:\n",
      "\t The dataset used PCA for dimensionality reduction.\n",
      "\t The dataset did NOT use SMOTE.\n",
      "\t This model did NOT cross-validate through Random Search.\n",
      "Confusion Matrix: \n",
      "[[75  0]\n",
      " [19  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.80      1.00      0.89        75\n",
      "           1       0.00      0.00      0.00        19\n",
      "\n",
      "    accuracy                           0.80        94\n",
      "   macro avg       0.40      0.50      0.44        94\n",
      "weighted avg       0.64      0.80      0.71        94\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, y, data = load_thoracic()\n",
    "X_train, y_train, X_test, y_test = split_data(X, y, pca_included = True, smote_included = False)\n",
    "predictions = train_randomforest(X_train, y_train, X_test, y_test, use_randomsearch = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Assumptions:\n",
      "\t The dataset used SMOTE to rectify class imbalance.\n",
      "\t This model has been cross-validated through Random Search.\n",
      "Confusion Matrix: \n",
      "[[67  8]\n",
      " [17  2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.80      0.89      0.84        75\n",
      "           1       0.20      0.11      0.14        19\n",
      "\n",
      "    accuracy                           0.73        94\n",
      "   macro avg       0.50      0.50      0.49        94\n",
      "weighted avg       0.68      0.73      0.70        94\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, y, data = load_thoracic()\n",
    "X_train, y_train, X_test, y_test = split_data(X, y, pca_included = False, smote_included = True)\n",
    "predictions = train_randomforest(X_train, y_train, X_test, y_test, use_randomsearch = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Assumptions:\n",
      "\t The dataset used PCA for dimensionality reduction.\n",
      "\t The dataset used SMOTE to rectify class imbalance.\n",
      "\t This model has been cross-validated through Random Search.\n",
      "Confusion Matrix: \n",
      "[[68  7]\n",
      " [16  3]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.81      0.91      0.86        75\n",
      "           1       0.30      0.16      0.21        19\n",
      "\n",
      "    accuracy                           0.76        94\n",
      "   macro avg       0.55      0.53      0.53        94\n",
      "weighted avg       0.71      0.76      0.72        94\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, y, data = load_thoracic()\n",
    "X_train, y_train, X_test, y_test = split_data(X, y, pca_included = True, smote_included = True)\n",
    "predictions = train_randomforest(X_train, y_train, X_test, y_test, use_randomsearch = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cervical Cancer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Assumptions:\n",
      "\t The dataset did NOT use SMOTE.\n",
      "\t This model did NOT cross-validate through Random Search.\n",
      "Confusion Matrix: \n",
      "[[8 0]\n",
      " [2 5]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89         8\n",
      "           1       1.00      0.71      0.83         7\n",
      "\n",
      "    accuracy                           0.87        15\n",
      "   macro avg       0.90      0.86      0.86        15\n",
      "weighted avg       0.89      0.87      0.86        15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../data/sobar-72.csv', sep = ',', header = 0)\n",
    "X = data.iloc[:, 0:19].to_numpy()\n",
    "y = data.iloc[:, 19].to_numpy()\n",
    "X_train, y_train, X_test, y_test = split_data(X, y, pca_included = False, smote_included = False)\n",
    "predictions = train_randomforest(X_train, y_train, X_test, y_test, use_randomsearch = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Assumptions:\n",
      "\t The dataset used PCA for dimensionality reduction.\n",
      "\t The dataset did NOT use SMOTE.\n",
      "\t This model has been cross-validated through Random Search.\n",
      "Confusion Matrix: \n",
      "[[8 0]\n",
      " [1 6]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      1.00      0.94         8\n",
      "           1       1.00      0.86      0.92         7\n",
      "\n",
      "    accuracy                           0.93        15\n",
      "   macro avg       0.94      0.93      0.93        15\n",
      "weighted avg       0.94      0.93      0.93        15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../data/sobar-72.csv', sep = ',', header = 0)\n",
    "X = data.iloc[:, 0:19].to_numpy()\n",
    "y = data.iloc[:, 19].to_numpy()\n",
    "X_train, y_train, X_test, y_test = split_data(X, y, pca_included = True, smote_included = False)\n",
    "predictions = train_randomforest(X_train, y_train, X_test, y_test, use_randomsearch = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6a2dc670f3436433c0efae6fb324965c1072d8aef0b90287abce79ee9328779"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
