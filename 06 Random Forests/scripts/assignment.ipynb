{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RADI605: Modern Machine Learning**\n",
    "\n",
    "### Assignment: Random Forests\n",
    "**Romen Samuel Rodis Wabina** <br>\n",
    "Student, PhD Data Science in Healthcare and Clinical Informatics <br>\n",
    "Clinical Epidemiology and Biostatistics, Faculty of Medicine (Ramathibodi Hospital) <br>\n",
    "Mahidol University\n",
    "\n",
    "Note: In case of Python Markdown errors, you may access the assignment through this GitHub [Link](https://github.com/rrwabina/RADI605/blob/main/05%20Adaptive%20Boosting/scripts/assignment.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the given data, create a random forest with $T=3$, OOB rate = $20\\%$, and initialize any additional parameters that may be needed.\n",
    "\n",
    "| Sample M  | Gene #1       | Gene #2         | Gene #3         | Class           |\n",
    "|:---------:|---------------|-----------------|-----------------|-----------------|\n",
    "| 1         | -3            | 1               | 6               | 1               |\n",
    "| 2         | 3             | -1              | 6               | 1               |\n",
    "| 3         | 6             | 1               | 12              | 1               |\n",
    "| 4         | 1             | 0               | 2               | -1              |\n",
    "| 5         | 0             | 1               | 0               | -1              |\n",
    "\n",
    "Out of the total number of samples in the data, 20% of the samples will be left out for Out-of-Bag (OOB) error calculation. So, for the given data with 5 samples, we will have 1 sample as OOB. Then, the original data will be bootstrapped to create $T = 3$ bootstrapped datasets. Suppose our OOB sample is sample 4:\n",
    "\n",
    "| Sample M  | Gene #1       | Gene #2         | Gene #3         | Class           |\n",
    "|:---------:|---------------|-----------------|-----------------|-----------------|\n",
    "| 4         | 1             | 0               | 2               | -1              |\n",
    "\n",
    "Bootstrapping: To create $T = 3$ bootstrapped datasets, we will randomly sample with replacement from the original data T times. Each bootstrapped dataset will have the same number of samples as the original data. Suppose bootstrapped dataset $D_1$ is the following:\n",
    "\n",
    "| Sample M  | Gene #1       | Gene #2         | Gene #3         | Class           |\n",
    "|:---------:|---------------|-----------------|-----------------|-----------------|\n",
    "| 1         | -3            | 1               | 6               | 1               |\n",
    "| 2         | 3             | -1              | 6               | 1               |\n",
    "| 4         | 1             | 0               | 2               | -1              |\n",
    "| 5         | 0             | 1               | 0               | -1              |\n",
    "| 4         | 1             | 0               | 2               | -1              |\n",
    "\n",
    "To calculate the Gini index and Gini gain for bootstrapped $D_1$, we will first calculate the Gini index for each feature (Gene #1, Gene #2, Gene #3) and use the Gini gain to determine the best feature to split the data at the root node.\n",
    "\n",
    "### Root Node: The Gini index measures the impurity of a set of samples. For a binary classification problem, the Gini index can be calculated as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "    \\text{Gini}(D_1)    &= 1 - \\sum_{j = 1}^{L}p^2_j \\\\\n",
    "                        &= 1 - (0.4)^2 - (0.4)^2 \\\\\n",
    "                        &= 1 - 0.32 \\\\\n",
    "    \\text{Gini}(D_1)    &= 0.48\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "### Splits on Gene #1\n",
    "Next, we'll calculate the gini gain for Gene #1. The split on Gene #1 is -3.\n",
    "- $\\text{Class 1}  \\rightarrow \\frac{0}{1} = 0$ and $\\text{Class -1} \\rightarrow \\frac{1}{1} = 1 $\n",
    "\n",
    "Therefore, \n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "    \\text{Gini index split}(D_1)    &= 1 - \\sum_{j = 1}^{L}p^2_j \\\\\n",
    "                              &= 1 - (0)^2 - (1)^2 \\\\\n",
    "                              &= 1 - 1 \\\\\n",
    "    \\text{Gini index split}(D_1)    &= 0\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "    \\text{Gini Gain split}(D_1)    &= \\text{Gini}(D_1) - \\sum_{i = 1}^{n} \\frac{|D_{1i}|}{|D_1|} \\text{Gini}(D_1i)\\\\\n",
    "                                   &= 0.48 - (1/5) \\cdot 0 - (4/5) \\cdot 0.48 \\\\\n",
    "    \\text{Gini Gain split}(D_1)    &= 0.24\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "We'll repeat the calculation for the gini gain for Gene #1 where the split on Gene #1 is 3.\n",
    "- $\\text{Class 1}  \\rightarrow \\frac{1}{2} = 0.5$ and $\\text{Class -1} \\rightarrow \\frac{0}{2} = 0 $\n",
    "\n",
    "Therefore, \n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "    \\text{Gini index split}(D_1)    &= 1 - \\sum_{j = 1}^{L}p^2_j \\\\\n",
    "                                    &= 1 - (0.5)^2 - (0)^2 \\\\\n",
    "    \\text{Gini index split}(D_1)    &= 0.5\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "    \\text{Gini Gain split}(D_1)    &= \\text{Gini}(D_1) - \\sum_{i = 1}^{n} \\frac{|D_{1i}|}{|D_1|} \\text{Gini}(D_1i)\\\\\n",
    "                                   &= 0.48 - (2/5) \\cdot 0 - (3/5) \\cdot 0.48 \\\\\n",
    "    \\text{Gini Gain split}(D_1)    &= 0.072\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "Finally, calculate the gini gain for Gene #1 where the split on Gene #1 is 1.\n",
    "- $\\text{Class 1}  \\rightarrow \\frac{1}{2} = 0.5$ and $\\text{Class -1} \\rightarrow \\frac{1}{2} = 0.5 $\n",
    "\n",
    "Therefore, \n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "    \\text{Gini index split}(D_1)    &= 1 - \\sum_{j = 1}^{L}p^2_j \\\\\n",
    "                                    &= 1 - (0.5)^2 - (0.5)^2 \\\\\n",
    "    \\text{Gini index split}(D_1)    &= 0.5\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "    \\text{Gini Gain split}(D_1)    &= \\text{Gini}(D_1) - \\sum_{i = 1}^{n} \\frac{|D_{1i}|}{|D_1|} \\text{Gini}(D_1i)\\\\\n",
    "                                   &= 0.48 - (2/5) \\cdot 0.5 - (3/5) \\cdot 0.48 \\\\\n",
    "    \\text{Gini Gain split}(D_1)    &= 0.072\n",
    "\\end{aligned}\n",
    "\\end{equation*}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The split for value 0 in Gene #1 was not included because, in the calculation of gini gain, it resulted in a negative gain value. The CART algorithm used in Random Forest only chooses the split that results in the highest positive gain value. As a result, a split for value 0 in Gene #1 was not included. Since Gene #1 has the highest gini gain when split on value = $-3$, we'll use that split for the root of the tree.\n",
    "The tree would look like:\n",
    "\n",
    "           -3\n",
    "           /  \\\n",
    "         -1   1\n",
    "        /  \\  / \\\n",
    "      1    -1    1\n",
    "For Gene #2:\n",
    "\n",
    "Split at -1:\n",
    "- gini index for Class = 1: 1 - (3/4)^2 - (1/4)^2 = 0.375\n",
    "- gini index for Class = -1: 1 - (2/4)^2 - (2/4)^2 = 0.5\n",
    "- gini gain: 0.375 - (4/8) * 0.375 - (4/8) * 0.5 = 0.0625\n",
    "\n",
    "Split at 1:\n",
    "- gini index for Class = 1: 1 - (2/3)^2 - (1/3)^2 = 0.44444444\n",
    "- gini index for Class = -1: 1 - (2/5)^2 - (3/5)^2 = 0.48\n",
    "- gini gain: 0.44444444 - (3/8) * 0.44444444 - (5/8) * 0.48 = 0.01111111\n",
    "\n",
    "For Gene #3:\n",
    "\n",
    "Split at 2:\n",
    "- gini index for Class = 1: 1 - (2/2)^2 - (0/2)^2 = 0\n",
    "- gini index for Class = -1: 1 - (2/2)^2 - (0/2)^2 = 0\n",
    "- gini gain: 0 - (2/8) * 0 - (6/8) * 0 = 0\n",
    "\n",
    "Split at 6:\n",
    "- gini index for Class = 1: 1 - (2/2)^2 - (0/2)^2 = 0\n",
    "- gini index for Class = -1: 1 - (2/6)^2 - (4/6)^2 = 0.48\n",
    "- gini gain: 0 - (2/8) * 0 - (6/8) * 0.48 = -0.12\n",
    "\n",
    "The highest gini gain was obtained from Gene #2 with a split at -1, so the first split of the tree will be based on Gene #2 and split at -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "from numpy import mean\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from mlxtend.plotting import plot_learning_curves\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_binary(df, columns = ['Risk1Yr']):\n",
    "    for column in columns:\n",
    "        df[column] = df[column].apply(lambda x: 0 if x == 'F' else 1)\n",
    "    return df\n",
    "\n",
    "def load_thoracic(path = '../data/ThoraricSurgery.csv'):\n",
    "    data = pd.read_csv(path)\n",
    "    data = data[data.columns[2:]]\n",
    "    data = data.drop(['PRE6', 'PRE14'], axis = 1)\n",
    "    label_columns = data.columns[2:12]\n",
    "    data = convert_binary(data, columns = ['Risk1Yr'])\n",
    "    data = convert_binary(data, columns = label_columns)\n",
    "    include_columns = data.columns[0:-1]\n",
    "    X, y = data[include_columns], data['Risk1Yr']\n",
    "    X, y = X.to_numpy(), y.to_numpy()\n",
    "    y[y == 0] = -1\n",
    "    return X, y, data\n",
    "\n",
    "X, y, data = load_thoracic()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When features are highly correlated, they contain redundant information, and the Random Forest may use the same feature in many of the trees, leading to overfitting. Additionally, highly correlated features can cause instability in the feature importance scores and make it difficult to interpret the results.\n",
    "\n",
    "It's always a good idea to check for highly correlated features before training a Random Forest and remove or combine them as needed. This can be done using techniques such as principal component analysis (PCA) or feature selection. By reducing the number of highly correlated features, you can simplify the model and make it more interpretable, and you may also see an improvement in performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_binary(df, columns = ['Risk1Yr']):\n",
    "    for column in columns:\n",
    "        df[column] = df[column].apply(lambda x: -1 if x == 'F' else 1)\n",
    "    return df\n",
    "\n",
    "def load_thoracic(path = '../data/ThoraricSurgery.csv'):\n",
    "    data = pd.read_csv(path)\n",
    "    data = data[data.columns[2:]]\n",
    "    data = data.drop(['PRE6', 'PRE14'], axis = 1)\n",
    "    label_columns = data.columns[2:12]\n",
    "    data = convert_binary(data, columns = ['Risk1Yr'])\n",
    "    data = convert_binary(data, columns = label_columns)\n",
    "    include_columns = data.columns[0:-1]\n",
    "    X, y = data[include_columns], data['Risk1Yr']\n",
    "    X, y = X.to_numpy(), y.to_numpy()\n",
    "    y[y == 0] = -1\n",
    "    return X, y, data\n",
    "\n",
    "def split_data(X, y, pca_included = False, smote_included = False):\n",
    "    print('Model Assumptions:')\n",
    "    if pca_included is True:\n",
    "        print('\\t The dataset used PCA for dimensionality reduction.')\n",
    "        pca = PCA(n_components = 8)\n",
    "        X = pca.fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "    if smote_included is True:\n",
    "        print('\\t The dataset used SMOTE to rectify class imbalance.')\n",
    "        smote = SMOTE(sampling_strategy = 'minority', k_neighbors = 5, random_state = 42)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "    else:\n",
    "        print('\\t The dataset did NOT use SMOTE.')\n",
    "        X_resampled, y_resampled = X_train, y_train \n",
    "    scaler  = StandardScaler()\n",
    "    X_resampled = scaler.fit_transform(X_resampled)\n",
    "    X_test  = scaler.transform(X_test)\n",
    "    return X_resampled, y_resampled, X_test, y_test\n",
    "\n",
    "def init_parameters():\n",
    "    param_grid = { \n",
    "                'n_estimators': [10, 2000], \n",
    "                'criterion': ['gini', 'entropy'],\n",
    "                'max_depth': np.arange(1, 20),\n",
    "                'min_samples_split': np.arange(1, 5)\n",
    "              }\n",
    "    return param_grid\n",
    "\n",
    "def validation(rsearch, X_test, y_test):\n",
    "    predictions = rsearch.predict(X_test)\n",
    "    print('Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, predictions))\n",
    "    print(classification_report(y_test, predictions))\n",
    "    return predictions\n",
    "\n",
    "def train_randomforest(X_train, y_train, X_test, y_test, use_randomsearch = False, print_params = True):\n",
    "    param_grid = init_parameters()\n",
    "    if use_randomsearch is True:\n",
    "        print('\\t This model has been cross-validated through Random Search.')\n",
    "        rsearch = RandomizedSearchCV(estimator = RandomForestClassifier(), \n",
    "                                     param_distributions = param_grid, \n",
    "                                     cv = 10, n_iter = 10)\n",
    "        rsearch.fit(X_train, y_train)\n",
    "        if print_params is True:\n",
    "            print(rsearch.best_params_)\n",
    "        predictions = validation(rsearch, X_test, y_test)\n",
    "    else:\n",
    "        print('\\t This model did NOT cross-validate through Random Search.')\n",
    "        rsearch = RandomForestClassifier(criterion = 'gini', n_estimators = 100, \n",
    "                                         max_depth = 9, min_samples_split = 4)\n",
    "        rsearch.fit(X_train, y_train)\n",
    "        predictions = validation(rsearch, X_test, y_test)\n",
    "    return predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Assumptions:\n",
      "\t The dataset did NOT use SMOTE.\n",
      "\t This model did NOT cross-validate through Random Search.\n",
      "Confusion Matrix: \n",
      "[[75  0]\n",
      " [19  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.80      1.00      0.89        75\n",
      "           1       0.00      0.00      0.00        19\n",
      "\n",
      "    accuracy                           0.80        94\n",
      "   macro avg       0.40      0.50      0.44        94\n",
      "weighted avg       0.64      0.80      0.71        94\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, y, data = load_thoracic()\n",
    "X_train, y_train, X_test, y_test = split_data(X, y, pca_included = False, smote_included = False)\n",
    "predictions = train_randomforest(X_train, y_train, X_test, y_test, use_randomsearch = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Assumptions:\n",
      "\t The dataset used PCA for dimensionality reduction.\n",
      "\t The dataset did NOT use SMOTE.\n",
      "\t This model did NOT cross-validate through Random Search.\n",
      "Confusion Matrix: \n",
      "[[75  0]\n",
      " [19  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.80      1.00      0.89        75\n",
      "           1       0.00      0.00      0.00        19\n",
      "\n",
      "    accuracy                           0.80        94\n",
      "   macro avg       0.40      0.50      0.44        94\n",
      "weighted avg       0.64      0.80      0.71        94\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, y, data = load_thoracic()\n",
    "X_train, y_train, X_test, y_test = split_data(X, y, pca_included = True, smote_included = False)\n",
    "predictions = train_randomforest(X_train, y_train, X_test, y_test, use_randomsearch = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Assumptions:\n",
      "\t The dataset used SMOTE to rectify class imbalance.\n",
      "\t This model has been cross-validated through Random Search.\n",
      "{'n_estimators': 2000, 'min_samples_split': 3, 'max_depth': 17, 'criterion': 'gini'}\n",
      "Confusion Matrix: \n",
      "[[69  6]\n",
      " [18  1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.79      0.92      0.85        75\n",
      "           1       0.14      0.05      0.08        19\n",
      "\n",
      "    accuracy                           0.74        94\n",
      "   macro avg       0.47      0.49      0.46        94\n",
      "weighted avg       0.66      0.74      0.70        94\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, y, data = load_thoracic()\n",
    "X_train, y_train, X_test, y_test = split_data(X, y, pca_included = False, smote_included = True)\n",
    "predictions = train_randomforest(X_train, y_train, X_test, y_test, use_randomsearch = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Assumptions:\n",
      "\t The dataset used PCA for dimensionality reduction.\n",
      "\t The dataset used SMOTE to rectify class imbalance.\n",
      "\t This model has been cross-validated through Random Search.\n",
      "{'n_estimators': 2000, 'min_samples_split': 4, 'max_depth': 18, 'criterion': 'gini'}\n",
      "Confusion Matrix: \n",
      "[[64 11]\n",
      " [16  3]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.80      0.85      0.83        75\n",
      "           1       0.21      0.16      0.18        19\n",
      "\n",
      "    accuracy                           0.71        94\n",
      "   macro avg       0.51      0.51      0.50        94\n",
      "weighted avg       0.68      0.71      0.70        94\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, y, data = load_thoracic()\n",
    "X_train, y_train, X_test, y_test = split_data(X, y, pca_included = True, smote_included = True)\n",
    "predictions = train_randomforest(X_train, y_train, X_test, y_test, use_randomsearch = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cervical Cancer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Assumptions:\n",
      "\t The dataset did NOT use SMOTE.\n",
      "\t This model did NOT cross-validate through Random Search.\n",
      "Confusion Matrix: \n",
      "[[8 0]\n",
      " [2 5]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89         8\n",
      "           1       1.00      0.71      0.83         7\n",
      "\n",
      "    accuracy                           0.87        15\n",
      "   macro avg       0.90      0.86      0.86        15\n",
      "weighted avg       0.89      0.87      0.86        15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../data/sobar-72.csv', sep = ',', header = 0)\n",
    "X = data.iloc[:, 0:19].to_numpy()\n",
    "y = data.iloc[:, 19].to_numpy()\n",
    "X_train, y_train, X_test, y_test = split_data(X, y, pca_included = False, smote_included = False)\n",
    "predictions = train_randomforest(X_train, y_train, X_test, y_test, use_randomsearch = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Assumptions:\n",
      "\t The dataset used PCA for dimensionality reduction.\n",
      "\t The dataset did NOT use SMOTE.\n",
      "\t This model has been cross-validated through Random Search.\n",
      "{'n_estimators': 2000, 'min_samples_split': 2, 'max_depth': 12, 'criterion': 'gini'}\n",
      "Confusion Matrix: \n",
      "[[8 0]\n",
      " [1 6]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      1.00      0.94         8\n",
      "           1       1.00      0.86      0.92         7\n",
      "\n",
      "    accuracy                           0.93        15\n",
      "   macro avg       0.94      0.93      0.93        15\n",
      "weighted avg       0.94      0.93      0.93        15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../data/sobar-72.csv', sep = ',', header = 0)\n",
    "X = data.iloc[:, 0:19].to_numpy()\n",
    "y = data.iloc[:, 19].to_numpy()\n",
    "X_train, y_train, X_test, y_test = split_data(X, y, pca_included = True, smote_included = False)\n",
    "predictions = train_randomforest(X_train, y_train, X_test, y_test, use_randomsearch = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6a2dc670f3436433c0efae6fb324965c1072d8aef0b90287abce79ee9328779"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
