{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RADI605: Modern Machine Learning**\n",
    "\n",
    "### Assignment: Gradient Boosting\n",
    "**Romen Samuel Rodis Wabina** <br>\n",
    "Student, PhD Data Science in Healthcare and Clinical Informatics <br>\n",
    "Clinical Epidemiology and Biostatistics, Faculty of Medicine (Ramathibodi Hospital) <br>\n",
    "Mahidol University\n",
    "\n",
    "Note: In case of Python Markdown errors, you may access the assignment through this GitHub [Link](https://github.com/rrwabina/RADI605)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please identify some pros and cons of the EM algorithm compare with the K-means"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KMeans and EM clutering (also known as Mixture Models) are both unsupervised clustering models. K-Means groups data points using distance from the cluster centroid while the EM clustering uses a probabilistic assignment of data points to clusters. In this item, we listed all the advantages and disadvantages of EM algorithm and compare it with the K-Means.   \n",
    "\n",
    "### Advantages of EM Algorithm (+ KMeans comparison)\n",
    "1. The EM algorithm can **handle data with missing values** since it is a probabilistic model that can estimate missing values. To handle missing data, the algorithm iteratively estimates model parameters that best fit the observed data, while also estimating the missing values. This can lead to more accurate and robust estimates of the missing data, as well as more **reliable clustering** or density estimates. Moreover, EM algorithm, as an imputation technique, can overcome one of the most commonly faced problems in clinical patient survey research <code>(Ghomrawi et al., 2011)</code>. \n",
    "\n",
    "2. The EM algorithm can **model more complex data distributions** than the K-means algorithm, as it can handle non-spherical clusters and clusters with different shapes and orientations.\n",
    "\n",
    "3. The EM, as a probabilistic approach, **can quantify uncertainties** to measure the model's reliability to its predictions. The **K-Means, on one hand, cannot measure uncertainty since it uses a deterministic method** where each data point is assigned to a single cluster center based on the minimum distance. Several research have utilized EM algorithm as an uncertainty quantification technique <code>(Malan et al., 2020)</code>. For instance, <code>Gao et al. (2020)</code> adopted the mixture model (i.e., EM algorithm) to analyze the production data with reservoir properties. The authors utilized EM to produce production forecasts that are reliable, i.e., high prediction confidence, with production data observed in the blind test period.\n",
    "\n",
    "4. The Expectation Maximization algorithm can **effectively handle datasets that have high correlations** among variables, as well as those in which the variances of the variables are not equal. This advantage is possible because EM, by default, utilizes the **Mahalanobis Distance** as its distance measurement. The Mahalanobis distance measures the similarity between data points and the cluster centers - which takes into account the covariance structure of the data. By doing so, EM clustering is able to account for the correlations between the variables, leading to more accurate clustering results. \n",
    "\n",
    "    - The **Euclidean distance in KMeans, however, performs poorly compared to Mahalanobis distance in EM** because the Euclidean distance assumes that the variables are independent and identically distributed (IID), and it does not account for the covariance structure of the data. Therefore, the use of Euclidean distance limits KMeans' ability to identify complex non-linear structures <code>(Davidson, I. 2002)</code>. This KMean's problem was further evaluated by <code>Patel & Kushwaha (2020)</code> by comparing **K-Means and EM** to evaluate cluster representativeness of the two methods for heterogeneity in resource usage of **Cloud workloads**. Its conclusions are very similar to <code>(Davidson, I. 2002)</code> where clusters obtained using K-Means (with Euclidean distance) give a relatively abstracted information while EM offers better grouping with separate clusters.\n",
    "\n",
    "### Disadvantages of EM Algorithm (+ KMeans comparison)\n",
    "1. EM algorithm converges slowly with large fractions of missing data. <code>Little and Rubin (2002)</code> adopted EM algorithm as an imputation technique in predicting children's weight in a school entry. They have found out that higher rates of missing values within the data provide slow model training performance. \n",
    "\n",
    "2. The EM may lead to biased parameter estimates and underestimates the standard errors. \n",
    "\n",
    "Little and Rubin (2002) stated that there are two major drawbacks\n",
    "of EM algorithm. First, it will converge very slowly in cases with large fractions of missing data. Second,\n",
    "the M-step will be difficult in some cases and then the theoretical simplicity of EM will not convert to\n",
    "simplicity in practice. Another problem with EM is that it leads to biased parameter estimates and\n",
    "underestimates the standard errors\n",
    "\n",
    "K-Means is widely used in scientific and industrial applications due to its simplicity and speed. However its use of Euclidean distance as similarity measure limits its ability to identify complex non-linear usage structures. Euclidean distance, the within-cluster similarity measure in K-Means is unable to discover complex non-linear usage patterns <code>(Davidson, I. 2002)</code>.\n",
    "\n",
    "The EM clustering is an effective generative as well as probabilistic clustering method. It can handle data with missing or incomplete values, as it uses a probabilistic model to estimate the missing values.\n",
    "The EM algorithm provides estimates of the uncertainty in the clustering results, which can be useful in applications where the reliability of the clustering is important.\n",
    "\n",
    "\n",
    "The EM clustering may converge to a local optima instead of the global optimum due to the iterative nature of the algorithms. While both methods start with an tinitial estimate of the cluster centers, and iteratively refine these estimates to optimize a specific objective, there may bay be multiple local optima that both algorithms can converge to. Therefore, the EM algorithm can be sensitive to the initial parameter values.\n",
    "\n",
    "\n",
    "The EM algorithm can be sensitive to the initial parameter values, and may converge to local optima instead of the global optimum.\n",
    "The EM algorithm can be computationally expensive, especially for large datasets or high-dimensional data, as it requires iterative calculations of the expectation and maximization steps.\n",
    "The EM algorithm assumes that the data follows a specific distribution (such as a Gaussian distribution), which may not always be appropriate for the data being analyzed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "Davidson, I. (2002). Understanding K-means non-hierarchical clustering. Computer Science Department of State University of New York (SUNY), Albany.\n",
    "\n",
    "[1] Gao, G., Jiang, H., Vink, J. C., Chen, C., El Khamra, Y., & Ita, J. J. (2020). Gaussian mixture model fitting method for uncertainty quantification by conditioning to production data. Computational Geosciences, 24(2), 663-681.\n",
    "\n",
    "[2] Ghomrawi, H. M., Mandl, L. A., Rutledge, J., Alexiades, M. M., & Mazumdar, M. (2011). Is there a role for expectation maximization imputation in addressing missing data in research using WOMAC questionnaire?    Comparison to the standard mean approach and a tutorial. BMC musculoskeletal disorders, 12(1), 1-7.\n",
    "\n",
    "[3] Little, R. J., & Rubin, D. B. (2019). Statistical analysis with missing data (Vol. 793). John Wiley & Sons.\n",
    "\n",
    "[4] Malan, L., Smuts, C. M., Baumgartner, J., & Ricci, C. (2020). Missing data imputation via the expectation-maximization algorithm can improve principal component analysis aimed at deriving biomarker profiles and dietary patterns. Nutrition Research, 75, 67-76.\n",
    "\n",
    "[5] Patel, E., & Kushwaha, D. S. (2020). Clustering cloud workloads: K-means vs gaussian mixture model. Procedia Computer Science, 171, 158-167."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import xgboost as xgb\n",
    "import statsmodels.api as sm\n",
    "import math \n",
    "\n",
    "from functools import reduce\n",
    "from time import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the <code>impute-data.csv</code>, please perform the missing value imputation by using the Expectation Maximization in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/Impute-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values A6_Score: \t [1 0]\n",
      "Unique values A7_Score: \t ['1' '0' '?' nan]\n",
      "Unique values A8_Score: \t [1 0]\n",
      "Unique values A9_Score: \t ['1' '0' '?' nan]\n",
      "Unique values A10_Score: \t [0 1]\n"
     ]
    }
   ],
   "source": [
    "for column in data.columns:\n",
    "    print(f'Unique values {column}: \\t {data[column].unique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A6_Score      int64\n",
       "A7_Score     object\n",
       "A8_Score      int64\n",
       "A9_Score     object\n",
       "A10_Score     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A6_Score     0\n",
       "A7_Score     1\n",
       "A8_Score     0\n",
       "A9_Score     1\n",
       "A10_Score    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values A6_Score: \t [1 0]\n",
      "Unique values A7_Score: \t ['1' '0' nan]\n",
      "Unique values A8_Score: \t [1 0]\n",
      "Unique values A9_Score: \t ['1' '0' nan]\n",
      "Unique values A10_Score: \t [0 1]\n"
     ]
    }
   ],
   "source": [
    "for column in ['A7_Score', 'A9_Score']:\n",
    "    data[column] = data[column].replace('?', math.nan)\n",
    "for column in data.columns:\n",
    "    print(f'Unique values {column}: \\t {data[column].unique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A6_Score       int64\n",
       "A7_Score     float64\n",
       "A8_Score       int64\n",
       "A9_Score     float64\n",
       "A10_Score      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for column in ['A7_Score', 'A9_Score']:\n",
    "    data[column] = [int(x) if str(x).isdigit() else np.nan for x in data[column]]\n",
    "    # data[column] = data[column].astype('Int64')\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 0.],\n",
       "       [0., 0., 0., 1., 1.],\n",
       "       [0., 0., 0., 1., 1.],\n",
       "       [1., 0., 1., 1., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 1., 0.],\n",
       "       [1., 1., 1., 1., 0.],\n",
       "       [0., 1., 1., 0., 1.],\n",
       "       [1., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 1., 0.],\n",
       "       [1., 0., 1., 1., 0.],\n",
       "       [1., 1., 0., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 0., 1., 1., 1.],\n",
       "       [1., 0., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [0., 0., 1., 0., 1.],\n",
       "       [1., 0., 1., 1., 1.],\n",
       "       [0., 0., 1., 1., 0.],\n",
       "       [1., 1., 0., 1., 1.],\n",
       "       [1., 0., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def binary_cov(X):\n",
    "    nr, nc = X.shape\n",
    "    X_bar = X - X.mean(axis=0)\n",
    "    cov = X_bar.T @ X_bar / nr\n",
    "    cov[np.diag_indices_from(cov)] = X.mean(axis=0) * (1 - X.mean(axis=0))\n",
    "    return cov\n",
    "\n",
    "def logistic(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def impute_em(X, max_iter = 3000, eps = 1e-08):\n",
    "    nr, nc = X.shape\n",
    "    C = np.isnan(X) == False\n",
    "    one_to_nc = np.arange(1, nc + 1, step = 1)\n",
    "    M = one_to_nc * (C == False) - 1\n",
    "    O = one_to_nc * C - 1\n",
    "    Mu = np.nanmean(X, axis = 0)\n",
    "    observed_rows = np.where(np.isnan(sum(X.T)) == False)[0]\n",
    "    S = binary_cov(X[observed_rows, ])\n",
    "    if np.isnan(S).any():\n",
    "        S = np.diag(np.nanvar(X, axis = 0))\n",
    "    \n",
    "    # Start updating\n",
    "    Mu_tilde, S_tilde = {}, {}\n",
    "    X_tilde = X.copy()\n",
    "    no_conv = True\n",
    "    iteration = 0\n",
    "    while no_conv and iteration < max_iter:\n",
    "        for i in range(nr):\n",
    "            S_tilde[i] = np.zeros(nc ** 2).reshape(nc, nc)\n",
    "            if set(O[i, ]) != set(one_to_nc - 1): \n",
    "                M_i, O_i = M[i, ][M[i, ] != -1], O[i, ][O[i, ] != -1]\n",
    "                S_MM = S[np.ix_(M_i, M_i)]\n",
    "                S_MO = S[np.ix_(M_i, O_i)]\n",
    "                S_OM = S_MO.T\n",
    "                S_OO = S[np.ix_(O_i, O_i)]\n",
    "                \n",
    "                # Modify the computation of Mu_tilde[i] to use the logistic function instead of the matrix multiplication.\n",
    "                Mu_tilde[i] = logistic(Mu[np.ix_(M_i)] + S_MO @ np.linalg.inv(S_OO) @ (X_tilde[i, O_i] - Mu[np.ix_(O_i)]))\n",
    "                X_tilde[i, M_i] = np.clip(np.round(Mu_tilde[i]), 0, 1) \n",
    "                S_MM_O = S_MM - S_MO @ np.linalg.inv(S_OO) @ S_OM\n",
    "                S_tilde[i][np.ix_(M_i, M_i)] = S_MM_O\n",
    "        Mu_new  = np.mean(X_tilde, axis = 0)\n",
    "\n",
    "        # Modify the computation of S_new to use a binary covariance matrix\n",
    "        S_new = binary_cov(X_tilde) + reduce(np.add, S_tilde.values()) / nr\n",
    "        no_conv = np.linalg.norm(Mu - Mu_new) >= eps or np.linalg.norm(S - S_new, ord = 2) >= eps\n",
    "        Mu = Mu_new\n",
    "        S  = S_new\n",
    "        iteration += 1\n",
    "    \n",
    "    result = {\n",
    "                'mu': Mu,\n",
    "                'Sigma': S,\n",
    "                'X_imputed': X_tilde,\n",
    "                'C': C,\n",
    "                'iteration': iteration\n",
    "             }\n",
    "    \n",
    "    return result\n",
    "\n",
    "X = data.to_numpy()\n",
    "result_imputed = impute_em(X)\n",
    "result_imputed['X_imputed']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
