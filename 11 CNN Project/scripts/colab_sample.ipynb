{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN76t6jFlrVzjiad+tKLHGb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rrwabina/RADI605/blob/main/11%20CNN%20Project/scripts/RADI605_CNN_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBjL75DsgpjU",
        "outputId": "4ee9f2b3-6093-4b5e-e2f3-7ae93c942294"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnE_K4NmgeiC",
        "outputId": "1ba0effd-8d76-4216-80d9-a631b87da6e0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import make_grid\n",
        "from pathlib import Path\n",
        "import sys, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Subset\n",
        "from torchvision.utils import make_grid\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.backends.cudnn as cudnn\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "from PIL import Image\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "from collections import Counter\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_path = '/content/drive/MyDrive/nodule/data/labels/'\n",
        "\n",
        "label = [pd.DataFrame(pd.read_csv(os.path.join(label_path, file), delim_whitespace = True)) for file in os.listdir(label_path)]\n",
        "df = pd.concat(label, ignore_index = True)\n",
        "df['types'] = [string.split('/') for string in df['image']]\n",
        "df['types'] = [string[0] for string in df['types']]\n",
        "df['image'] = [string.split('/') for string in df['image']]\n",
        "df['image'] = [string[1][6:-4] for string in df['image']]\n",
        "len(df[df['label'] == 0])\n",
        "len(df) - len(df[df['label'] == 0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksWLM9Srg0Rn",
        "outputId": "26194d71-fad3-4b0e-bbc3-cb23184c250b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1351"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NoduleDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_dir, transform = None):\n",
        "        self.data_dir = data_dir\n",
        "        self.augment = transforms.Compose([\n",
        "            transforms.Resize((50, 50)),\n",
        "            transforms.RandomResizedCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "        \n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((50, 50)),\n",
        "            transforms.RandomCrop(32, padding = 2),\n",
        "            transforms.RandomRotation(90),\n",
        "            transforms.ToTensor()])\n",
        "        \n",
        "\n",
        "        self.images_dir = data_dir / 'images'\n",
        "        self.labels_dir = data_dir / 'labels'\n",
        "\n",
        "        self.train_images_dir = self.images_dir \n",
        "        self.val_images_dir   = self.images_dir  \n",
        "        self.test_images_dir  = self.images_dir  \n",
        "\n",
        "        self.train_labels_file = self.labels_dir  / 'trainlabels.txt'\n",
        "        self.val_labels_file   = self.labels_dir  / 'vallabels.txt'\n",
        "        self.test_labels_file  = self.labels_dir  / 'testlabels.txt'\n",
        "\n",
        "        self.train_data = self._load_data(self.train_images_dir, self.train_labels_file)\n",
        "        self.val_data   = self._load_data(self.val_images_dir, self.val_labels_file)\n",
        "        self.test_data  = self._load_data(self.test_images_dir, self.test_labels_file)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if index < len(self.train_data):\n",
        "            images_dir = self.train_images_dir\n",
        "            data = self.train_data\n",
        "        elif index < len(self.train_data) + len(self.val_data):\n",
        "            images_dir = self.val_images_dir\n",
        "            data = self.val_data\n",
        "            index -= len(self.train_data)\n",
        "        else:\n",
        "            images_dir = self.test_images_dir\n",
        "            data = self.test_data\n",
        "            index -= (len(self.train_data) + len(self.val_data))\n",
        "\n",
        "        img_path = images_dir / data[index][0]\n",
        "        with open(img_path, 'rb') as f:\n",
        "            image = Image.open(f).convert('RGB')\n",
        "\n",
        "        label = data[index][1]\n",
        "        return self.transform(image), label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.train_data) + len(self.val_data) + len(self.test_data)\n",
        "\n",
        "    def _load_data(self, images_dir, labels_file):\n",
        "        with open(labels_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        data = []\n",
        "        for line in lines[1:]:\n",
        "            filename, label = line.strip().split()\n",
        "            filename = filename \n",
        "            label = int(label)\n",
        "            data.append((filename, label))\n",
        "        return data\n",
        "\n",
        "    def get_datasets(self):\n",
        "        train_dataset = Subset(self, range(len(self.train_data)))\n",
        "        test_dataset  = Subset(self, range(len(self.train_data),  len(self.train_data) + len(self.test_data)))\n",
        "        valid_dataset = Subset(self, range(len(self.train_data) + len(self.test_data),   len(self)))\n",
        "        return train_dataset, test_dataset, valid_dataset\n",
        "\n",
        "def GET_NODULEDATASET():\n",
        "    train_indices = list(range(0, len(dataset.train_data)))\n",
        "    valid_indices = list(range(len(dataset.train_data),  len(dataset.train_data) + len(dataset.val_data)))\n",
        "    test_indices  = list(range(len(dataset.train_data) + len(dataset.val_data), len(dataset)))\n",
        "\n",
        "    train_dataset = Subset(dataset, train_indices)\n",
        "    valid_dataset = Subset(dataset, valid_indices)\n",
        "    test_dataset  = Subset(dataset, test_indices)\n",
        "    return train_dataset, valid_dataset, test_dataset\n",
        "\n",
        "data_dir = Path('/content/drive/MyDrive/nodule/data/')\n",
        "dataset  = NoduleDataset(data_dir)\n",
        "train_dataset, valid_dataset, test_dataset = GET_NODULEDATASET()\n",
        "\n",
        "train_classes = [label for _, label in train_dataset]\n",
        "class_count = Counter(train_classes)\n",
        "class_weights = torch.Tensor([len(train_classes)/c for c in pd.Series(class_count).sort_index().values])\n",
        "class_samples = [0] * len(class_weights)\n",
        "for _, label in train_dataset:\n",
        "    class_samples[label] += 1\n",
        "weights = [class_weights[label] / class_samples[label] for _, label in train_dataset]\n",
        "sampler = WeightedRandomSampler(weights = weights, num_samples = len(weights), replacement = False)\n",
        "\n",
        "train_loader  = DataLoader(train_dataset, batch_size = 32, sampler = sampler)\n",
        "valid_loader  = DataLoader(valid_dataset, batch_size = 32, shuffle = True )\n",
        "test_loader   = DataLoader(test_dataset,  batch_size = 32, shuffle = False)"
      ],
      "metadata": {
        "id": "T6b6JuUwg-Di"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings \n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class EarlyStopping:\n",
        "  def __init__(self, patience=1, delta=0, path = 'checkpoint.pt'):\n",
        "    self.patience = patience\n",
        "    self.delta = delta\n",
        "    self.path= path\n",
        "    self.counter = 0\n",
        "    self.best_score = None\n",
        "    self.early_stop = False\n",
        "\n",
        "  def __call__(self, val_loss, model):\n",
        "    if self.best_score is None:\n",
        "      self.best_score = val_loss\n",
        "      self.save_checkpoint(model)\n",
        "    elif val_loss > self.best_score:\n",
        "      self.counter +=1\n",
        "      if self.counter >= self.patience:\n",
        "        self.early_stop = True \n",
        "    else:\n",
        "      self.best_score = val_loss\n",
        "      self.save_checkpoint(model)\n",
        "      self.counter = 0      \n",
        "\n",
        "  def save_checkpoint(self, model):\n",
        "    torch.save(model.state_dict(), self.path)\n",
        "\n",
        "def binary_accuracy(preds, y):\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct       = (rounded_preds == y).sum() \n",
        "    acc           = torch.mean(torch.eq(preds, label).float())\n",
        "    return acc\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "def train(num_epochs, model, train_loader, valid_loader, test_loader, optimizer, criterion, device):\n",
        "    best_valid_loss = float('inf')\n",
        "    train_losses, valid_losses = [], []\n",
        "    train_accurs, valid_accurs = [], []\n",
        "    epoch_times = []\n",
        "    list_best_epochs = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time = time.time()\n",
        "\n",
        "        train_loss, train_accu = _train(model, train_loader, optimizer, criterion, device)\n",
        "        valid_loss, valid_accu = _evals(model, valid_loader, criterion, device)\n",
        "\n",
        "        print(f'Epoch: {epoch + 1} \\t Training: Loss {np.round(train_loss, 5)}   \\t Accuracy: {np.round(train_accu, 5)}\\\n",
        "                                   \\t Validation Loss  {np.round(valid_loss, 5)} \\t Accuracy: {np.round(valid_accu, 5)}')\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        train_accurs.append(train_accu)\n",
        "        valid_losses.append(valid_loss)\n",
        "        valid_accurs.append(valid_accu)\n",
        "\n",
        "        end_time = time.time()\n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            best_model = copy.deepcopy(model)\n",
        "            best_epoch = epoch\n",
        "        list_best_epochs.append(best_epoch)\n",
        "    test_loss, test_accu  = _evals(best_model, test_loader, criterion, device)\n",
        "    print(f'Final Best Model from Best Epoch {best_epoch} Test Loss = {test_loss}, Test Acc = {test_accu}')\n",
        "    return train_losses, valid_losses, train_accurs, valid_accurs, test_loss, test_accu, best_epoch, epoch_times\n",
        "\n",
        "def _train(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    epoch_train_loss = 0\n",
        "    epoch_train_accu = 0\n",
        "\n",
        "    for idx, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        \n",
        "        outputs = model(inputs)\n",
        "        outputs = torch.argmax(outputs, dim = 1)\n",
        "        # outputs = torch.sigmoid(outputs)[:, 1]\n",
        "        print(outputs.shape)\n",
        "        loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_train_loss += loss.item()\n",
        "        accuracy = binary_accuracy(outputs, labels)\n",
        "        epoch_train_accu += accuracy.item()\n",
        "\n",
        "    epoch_train_loss = epoch_train_loss / len(train_loader)\n",
        "    epoch_train_accu = epoch_train_accu / len(train_loader)\n",
        "    return epoch_train_loss, epoch_train_accu\n",
        "\n",
        "def _evals(model, valid_loader, criterion, device):\n",
        "    model.eval()\n",
        "    epoch_valid_loss = 0\n",
        "    epoch_valid_accu = 0\n",
        "\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for id, data in enumerate(valid_loader):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            outputs = torch.sigmoid(outputs)[:, 1]\n",
        "            loss = criterion(outputs, labels.float())\n",
        "            rounded_preds = torch.round(torch.sigmoid(outputs)).long().flatten().tolist()\n",
        "            all_predictions.extend(rounded_preds)\n",
        "\n",
        "            epoch_valid_loss += loss.item()\n",
        "            accuracy = binary_accuracy(outputs, labels)\n",
        "            epoch_valid_accu += accuracy.item()\n",
        "    epoch_valid_loss = epoch_valid_loss / len(valid_loader)\n",
        "    epoch_valid_accu = epoch_valid_accu / len(valid_loader)\n",
        "    return epoch_valid_loss, epoch_valid_accu"
      ],
      "metadata": {
        "id": "BcoAtCrPhEyO"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_vgg19 = torchvision.models.vgg19(weights = True).to(device)\n",
        "learning_rate = 1e-2\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model_vgg19.parameters(), lr = learning_rate, eps = 10e-06)\n",
        "\n",
        "train_losses, valid_losses, train_accurs, valid_accurs, test_loss, test_accu, best_epoch, epoch_times = train(10, model_vgg19, train_loader, valid_loader, test_loader, optimizer, criterion, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUHHbAqzEdaY",
        "outputId": "675491a3-37b0-4d7f-fc23-895448fbee79"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 \t Training: Loss 83.72987   \t Accuracy: 0.0                                   \t Validation Loss  82.59057 \t Accuracy: 0.0\n",
            "Epoch: 2 \t Training: Loss 83.79985   \t Accuracy: 0.0                                   \t Validation Loss  82.79232 \t Accuracy: 0.0\n",
            "Epoch: 3 \t Training: Loss 83.79985   \t Accuracy: 0.0                                   \t Validation Loss  82.72507 \t Accuracy: 0.0\n",
            "Epoch: 4 \t Training: Loss 83.79985   \t Accuracy: 0.0                                   \t Validation Loss  82.79232 \t Accuracy: 0.0\n",
            "Epoch: 5 \t Training: Loss 83.79985   \t Accuracy: 0.0                                   \t Validation Loss  82.85958 \t Accuracy: 0.0\n",
            "Epoch: 6 \t Training: Loss 83.79985   \t Accuracy: 0.0                                   \t Validation Loss  82.65782 \t Accuracy: 0.0\n",
            "Epoch: 7 \t Training: Loss 83.79985   \t Accuracy: 0.0                                   \t Validation Loss  82.79232 \t Accuracy: 0.0\n",
            "Epoch: 8 \t Training: Loss 83.79985   \t Accuracy: 0.0                                   \t Validation Loss  82.79232 \t Accuracy: 0.0\n",
            "Epoch: 9 \t Training: Loss 83.79985   \t Accuracy: 0.0                                   \t Validation Loss  82.85958 \t Accuracy: 0.0\n",
            "Epoch: 10 \t Training: Loss 83.79985   \t Accuracy: 0.0                                   \t Validation Loss  82.72507 \t Accuracy: 0.0\n",
            "Final Best Model from Best Epoch 0 Test Loss = 82.66488423066981, Test Acc = 0.0\n"
          ]
        }
      ]
    }
  ]
}