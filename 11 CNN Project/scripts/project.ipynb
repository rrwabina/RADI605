{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "from pathlib import Path\n",
    "import sys, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Subset\n",
    "from torchvision.utils import make_grid\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from PIL import Image\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from collections import Counter\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration: Baseline Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_path = 'D:/nodule/data/labels/'\n",
    "\n",
    "label = [pd.DataFrame(pd.read_csv(os.path.join(label_path, file), delim_whitespace = True)) for file in os.listdir(label_path)]\n",
    "df = pd.concat(label, ignore_index = True)\n",
    "df['types'] = [string.split('/') for string in df['image']]\n",
    "df['types'] = [string[0] for string in df['types']]\n",
    "df['image'] = [string.split('/') for string in df['image']]\n",
    "df['image'] = [string[1][6:-4] for string in df['image']]\n",
    "\n",
    "dict = {}\n",
    "image_path = 'D:/nodule/data/images/'\n",
    "folders = os.listdir(image_path)\n",
    "images = ([os.listdir(os.path.join(image_path, folder)) for folder in folders])\n",
    "test_images  = images[0]\n",
    "train_images = images[1]\n",
    "valid_images = images[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoduleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_path, image_path, transforms = None):    \n",
    "        self.transform  = transforms\n",
    "        self.data_path  = data_path\n",
    "        self.images_dir = data_path / 'images' / Path(image_path)\n",
    "\n",
    "        if   image_path == 'train': \n",
    "            self.labels_dir = data_path / 'labels' / 'trainlabels.txt'\n",
    "        elif image_path == 'val':\n",
    "            self.labels_dir = data_path / 'labels' / 'vallabels.txt'\n",
    "        elif image_path == 'test' :\n",
    "            self.labels_dir = data_path / 'labels' / 'testlabels.txt'\n",
    "        self.dataset = self._load_data(self.images_dir, self.labels_dir)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        dataset  = self.dataset\n",
    "        img_path = self.images_dir / dataset[index][0]\n",
    "        with open(img_path, 'rb') as f:\n",
    "            image = Image.open(f).convert('RGB')\n",
    "\n",
    "        label = dataset[index][1]\n",
    "        return self.transform(image), label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset) \n",
    "\n",
    "    def _load_data(self, images_dir, labels_file):\n",
    "        with open(labels_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        data = []\n",
    "        for line in lines[1:]:\n",
    "            filename, label = line.strip().split()\n",
    "            filename = os.path.basename(filename)\n",
    "            label = int(label)\n",
    "            data.append((filename, label))\n",
    "        return data\n",
    "    \n",
    "train_transform = transforms.Compose([\n",
    "        transforms.RandomRotation(10),      \n",
    "        transforms.RandomHorizontalFlip(),  \n",
    "        transforms.Resize((50, 50)), \n",
    "        transforms.RandomCrop(32, padding = 2),         \n",
    "        transforms.CenterCrop(40),         \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "        transforms.Resize((50, 50)),\n",
    "        transforms.CenterCrop(40),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "data_path  = Path('D:/nodule/data/')\n",
    "train_dataset  = NoduleDataset(data_path, image_path = 'train', transforms = train_transform)\n",
    "valid_dataset  = NoduleDataset(data_path, image_path = 'val',   transforms = valid_transform)\n",
    "test_dataset   = NoduleDataset(data_path, image_path = 'test',  transforms = valid_transform)\n",
    "\n",
    "train_classes = [label for _, label in train_dataset]\n",
    "class_count = Counter(train_classes)\n",
    "class_weights = torch.Tensor([len(train_classes)/c for c in pd.Series(class_count).sort_index().values])\n",
    "class_samples = [0] * len(class_weights)\n",
    "\n",
    "for _, label in train_dataset:\n",
    "    class_samples[label] += 1\n",
    "weights = [class_weights[label] / class_samples[label] for _, label in train_dataset]\n",
    "sampler = WeightedRandomSampler(weights = weights, num_samples = len(weights), replacement = True)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "train_loader   = DataLoader(train_dataset, batch_size = 32, sampler = sampler)\n",
    "valid_loader   = DataLoader(valid_dataset, batch_size = 32, shuffle = True   )\n",
    "test_loader    = DataLoader(test_dataset,  batch_size = 32, shuffle = False  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoduleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((50, 50)),\n",
    "            transforms.RandomCrop(32, padding = 2),\n",
    "            transforms.RandomRotation(90),\n",
    "            transforms.CenterCrop(40),\n",
    "            transforms.ToTensor(), \n",
    "            transforms.Normalize(mean = [0.485, 0.456, 0.406],\n",
    "                                 std  = [0.229, 0.224, 0.225])])\n",
    "        \n",
    "        self.images_dir = data_dir / 'images'\n",
    "        self.labels_dir = data_dir / 'labels'\n",
    "\n",
    "        self.train_images_dir = self.images_dir \n",
    "        self.val_images_dir   = self.images_dir  \n",
    "        self.test_images_dir  = self.images_dir  \n",
    "\n",
    "        self.train_labels_file = self.labels_dir  / 'trainlabels.txt'\n",
    "        self.val_labels_file   = self.labels_dir  / 'vallabels.txt'\n",
    "        self.test_labels_file  = self.labels_dir  / 'testlabels.txt'\n",
    "\n",
    "        self.train_data = self._load_data(self.train_images_dir, self.train_labels_file)\n",
    "        self.val_data   = self._load_data(self.val_images_dir, self.val_labels_file)\n",
    "        self.test_data  = self._load_data(self.test_images_dir, self.test_labels_file)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index < len(self.train_data):\n",
    "            images_dir = self.train_images_dir\n",
    "            data = self.train_data\n",
    "        elif index < len(self.train_data) + len(self.val_data):\n",
    "            images_dir = self.val_images_dir\n",
    "            data = self.val_data\n",
    "            index -= len(self.train_data)\n",
    "        else:\n",
    "            images_dir = self.test_images_dir\n",
    "            data = self.test_data\n",
    "            index -= (len(self.train_data) + len(self.val_data))\n",
    "\n",
    "        img_path = images_dir / data[index][0]\n",
    "        with open(img_path, 'rb') as f:\n",
    "            image = Image.open(f).convert('RGB')\n",
    "\n",
    "        label = data[index][1]\n",
    "        return self.transform(image), label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_data) + len(self.val_data) + len(self.test_data)\n",
    "\n",
    "    def _load_data(self, images_dir, labels_file):\n",
    "        with open(labels_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        data = []\n",
    "        for line in lines[1:]:\n",
    "            filename, label = line.strip().split()\n",
    "            filename = filename \n",
    "            label = int(label)\n",
    "            data.append((filename, label))\n",
    "        return data\n",
    "\n",
    "    def get_datasets(self):\n",
    "        train_dataset = Subset(self, range(len(self.train_data)))\n",
    "        test_dataset  = Subset(self, range(len(self.train_data),  len(self.train_data) + len(self.test_data)))\n",
    "        valid_dataset = Subset(self, range(len(self.train_data) + len(self.test_data),   len(self)))\n",
    "        return train_dataset, test_dataset, valid_dataset\n",
    "\n",
    "def GET_NODULEDATASET():\n",
    "    train_indices = list(range(0, len(dataset.train_data)))\n",
    "    valid_indices = list(range(len(dataset.train_data),  len(dataset.train_data) + len(dataset.val_data)))\n",
    "    test_indices  = list(range(len(dataset.train_data) + len(dataset.val_data), len(dataset)))\n",
    "\n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    valid_dataset = Subset(dataset, valid_indices)\n",
    "    test_dataset  = Subset(dataset, test_indices)\n",
    "    return train_dataset, valid_dataset, test_dataset\n",
    "\n",
    "data_dir = Path('D:/nodule/data/')\n",
    "dataset  = NoduleDataset(data_dir)\n",
    "train_dataset, valid_dataset, test_dataset = GET_NODULEDATASET()\n",
    "\n",
    "train_classes = [label for _, label in train_dataset]\n",
    "class_count = Counter(train_classes)\n",
    "class_weights = torch.Tensor([len(train_classes)/c for c in pd.Series(class_count).sort_index().values])\n",
    "class_samples = [0] * len(class_weights)\n",
    "\n",
    "for _, label in train_dataset:\n",
    "    class_samples[label] += 1\n",
    "weights = [class_weights[label] / class_samples[label] for _, label in train_dataset]\n",
    "sampler = WeightedRandomSampler(weights = weights, num_samples = len(weights), replacement = True)\n",
    "\n",
    "train_loader  = DataLoader(train_dataset, batch_size = 32, sampler = sampler)\n",
    "valid_loader  = DataLoader(valid_dataset, batch_size = 32, shuffle = True )\n",
    "test_loader   = DataLoader(test_dataset,  batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EarlyStopping:\n",
    "  def __init__(self, patience  = 1, delta = 0, path = 'checkpoint.pt'):\n",
    "    self.patience = patience\n",
    "    self.delta = delta\n",
    "    self.path= path\n",
    "    self.counter = 0\n",
    "    self.best_score = None\n",
    "    self.early_stop = False\n",
    "\n",
    "  def __call__(self, val_loss, model):\n",
    "    if self.best_score is None:\n",
    "      self.best_score = val_loss\n",
    "      self.save_checkpoint(model)\n",
    "    elif val_loss > self.best_score:\n",
    "      self.counter +=1\n",
    "      if self.counter >= self.patience:\n",
    "        self.early_stop = True \n",
    "    else:\n",
    "      self.best_score = val_loss\n",
    "      self.save_checkpoint(model)\n",
    "      self.counter = 0      \n",
    "\n",
    "  def save_checkpoint(self, model):\n",
    "    torch.save(model.state_dict(), self.path)\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct       = (rounded_preds == y).sum() \n",
    "    acc           = torch.mean(torch.eq(preds, y).float())\n",
    "    return acc\n",
    "\n",
    "def get_metrics(prediction, label):\n",
    "    prediction  = prediction.cpu().detach().numpy()\n",
    "    label = label.cpu().detach().numpy()\n",
    "    \n",
    "    tp = np.sum((prediction == 1) & (label == 1))\n",
    "    tn = np.sum((prediction == 0) & (label == 0))\n",
    "    fp = np.sum((prediction == 1) & (label == 0))\n",
    "    fn = np.sum((prediction == 0) & (label == 1))\n",
    "\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    auc = roc_auc_score(label, prediction)\n",
    "    fpr, tpr, _ = roc_curve(label, prediction)\n",
    "\n",
    "    return {'sensitivity': sensitivity,\n",
    "            'specificity': specificity,\n",
    "            'auc': auc,\n",
    "            'fpr': fpr,\n",
    "            'tpr': tpr}\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "def train(num_epochs, model, train_loader, valid_loader, test_loader, optimizer, criterion, device):\n",
    "    best_valid_loss = float('inf')\n",
    "    train_losses, valid_losses = [], []\n",
    "    train_accurs, valid_accurs = [], []\n",
    "    epoch_times = []\n",
    "    list_best_epochs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss, train_accu = _train(model, train_loader, optimizer, criterion, device)\n",
    "        valid_loss, valid_accu = _evals(model, valid_loader, criterion, device)\n",
    "\n",
    "        print(f'Epoch: {epoch + 1} \\t Training: Loss {np.round(train_loss, 5)}   \\t Accuracy: {np.round(train_accu, 5)} \\t Validation Loss  {np.round(valid_loss, 5)} \\t Accuracy: {np.round(valid_accu, 5)}')\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accurs.append(train_accu)\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_accurs.append(valid_accu)\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_epoch = epoch\n",
    "        list_best_epochs.append(best_epoch)\n",
    "    test_loss, test_accu  = _evals(best_model, test_loader, criterion, device)\n",
    "    print(f'Final Best Model from Best Epoch {best_epoch + 1} Test Loss = {test_loss}, Test Accuracy = {test_accu}')\n",
    "    return train_losses, valid_losses, train_accurs, valid_accurs, test_loss, test_accu, best_epoch, epoch_times\n",
    "\n",
    "def _train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "    epoch_train_accu = 0\n",
    "\n",
    "    for idx, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        outputs = torch.sigmoid(outputs)[:, 1]\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "        accuracy = binary_accuracy(outputs, labels)\n",
    "        epoch_train_accu += accuracy.item()\n",
    "\n",
    "    epoch_train_loss = epoch_train_loss / len(train_loader) \n",
    "    epoch_train_accu = epoch_train_accu / len(train_loader) \n",
    "    return epoch_train_loss, epoch_train_accu\n",
    "\n",
    "def _evals(model, valid_loader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_valid_loss = 0\n",
    "    epoch_valid_accu = 0\n",
    "\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for id, data in enumerate(valid_loader):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            outputs = torch.sigmoid(outputs)[:, 1]\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            rounded_preds = torch.round(torch.sigmoid(outputs)).long().flatten().tolist()\n",
    "            all_predictions.extend(rounded_preds)\n",
    "\n",
    "            epoch_valid_loss += loss.item()\n",
    "            accuracy = binary_accuracy(outputs, labels)\n",
    "            epoch_valid_accu += accuracy.item()\n",
    "    epoch_valid_loss = epoch_valid_loss / len(valid_loader)\n",
    "    epoch_valid_accu = epoch_valid_accu / len(valid_loader)\n",
    "    return epoch_valid_loss, epoch_valid_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training: Loss 0.69466   \t Accuracy: 0.0 \t Validation Loss  0.69315 \t Accuracy: 0.0\n",
      "Epoch: 2 \t Training: Loss 0.69315   \t Accuracy: 0.0 \t Validation Loss  0.69315 \t Accuracy: 0.0\n",
      "Final Best Model from Best Epoch 1 Test Loss = 0.6931471801271626, Test Accuracy = 0.0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "model_vgg19 = torchvision.models.vgg19(weights = True).to(device)\n",
    "model_vgg19.classifier[-1] = torch.nn.Linear(4096, 2)\n",
    "model_vgg19.classifier.add_module('logsoftmax', nn.LogSoftmax(dim = 1))\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model_vgg19.parameters(), lr = 0.01, eps = 10e-06)\n",
    "train_losses, valid_losses, train_accurs, valid_accurs, test_loss, test_accu, best_epoch, epoch_times = train(2, model_vgg19, train_loader, valid_loader, test_loader, optimizer, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training: Loss 15.7254   \t Accuracy: 0.83666 \t Validation Loss  82.79232 \t Accuracy: 0.17208\n",
      "Epoch: 2 \t Training: Loss 16.21933   \t Accuracy: 0.83781 \t Validation Loss  82.72507 \t Accuracy: 0.17275\n",
      "Final Best Model from Best Epoch 1 Test Loss = 82.66488408107384, Test Acc = 0.17335115869839987\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "model_vgg19 = torchvision.models.vgg19(weights = True).to(device)\n",
    "model_vgg19.classifier[-1] = torch.nn.Linear(4096, 2)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model_vgg19.parameters(), lr = 1e-2, eps = 10e-06)\n",
    "train_losses, valid_losses, train_accurs, valid_accurs, test_loss, test_accu, best_epoch, epoch_times = train(2, model_vgg19, train_loader, valid_loader, test_loader, optimizer, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training: Loss 16.83681   \t Accuracy: 0.82554                                   \t Validation Loss  82.79232 \t Accuracy: 0.17208\n",
      "Epoch: 2 \t Training: Loss 16.27045   \t Accuracy: 0.8373                                   \t Validation Loss  82.79232 \t Accuracy: 0.17208\n",
      "Final Best Model from Best Epoch 0 Test Loss = 82.66488408107384, Test Acc = 0.17335115869839987\n"
     ]
    }
   ],
   "source": [
    "model_alexnet = models.alexnet(pretrained = True)\n",
    "\n",
    "model_alexnet.features[0] = torch.nn.Conv2d(3,  64, kernel_size = (3, 3), stride = (1, 1), padding = (1, 1))\n",
    "model_alexnet.features[2] = torch.nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0, dilation = 1, ceil_mode = False)\n",
    "model_alexnet.features[3] = torch.nn.Conv2d(64, 192, kernel_size = (3, 3), stride = (1, 1), padding = (1, 1))\n",
    "model_alexnet.features[5] = torch.nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0, dilation = 1, ceil_mode = False)\n",
    "\n",
    "model_alexnet.classifier[6] = torch.nn.Linear(4096, 2)\n",
    "model_alexnet.classifier[6].requires_grad = True\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model_alexnet.classifier.parameters(), lr = 0.001, eps = 10e-06)\n",
    "train_losses, valid_losses, train_accurs, valid_accurs, test_loss, test_accu, best_epoch, epoch_times = train(2, model_alexnet, train_loader, valid_loader, test_loader, optimizer, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training: Loss 0.48521   \t Accuracy: 0.0 \t Validation Loss  1.13984 \t Accuracy: 0.0\n",
      "Epoch: 2 \t Training: Loss 0.4807   \t Accuracy: 0.0 \t Validation Loss  1.14051 \t Accuracy: 0.0\n",
      "Final Best Model from Best Epoch 1 Test Loss = 1.13991066755033, Test Accuracy = 0.0\n"
     ]
    }
   ],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3,  32, 3, 1, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1, padding = 1)\n",
    "\n",
    "        self.fc1 = nn.Linear(64 * 10 * 10, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 2)\n",
    "\n",
    "        self.dropout = nn.Dropout(p = 0.5)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(32)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.batchnorm1(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.batchnorm2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = nn.functional.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "torch.manual_seed(101)\n",
    "model_cnn = LeNet()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model_cnn.parameters(), lr = 0.1)\n",
    "train_losses, valid_losses, train_accurs, valid_accurs, test_loss, test_accu, best_epoch, epoch_times = train(2, model_cnn, train_loader, valid_loader, test_loader, optimizer, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training: Loss 82.80669   \t Accuracy: 0.16584 \t Validation Loss  17.07317 \t Accuracy: 0.82927\n",
      "Epoch: 2 \t Training: Loss 83.10966   \t Accuracy: 0.1689 \t Validation Loss  17.34218 \t Accuracy: 0.82658\n",
      "Epoch: 3 \t Training: Loss 83.52505   \t Accuracy: 0.16475 \t Validation Loss  17.14042 \t Accuracy: 0.8286\n",
      "Epoch: 4 \t Training: Loss 83.05215   \t Accuracy: 0.16948 \t Validation Loss  17.14042 \t Accuracy: 0.8286\n",
      "Epoch: 5 \t Training: Loss 83.78067   \t Accuracy: 0.16219 \t Validation Loss  17.27493 \t Accuracy: 0.82725\n",
      "Epoch: 6 \t Training: Loss 83.93405   \t Accuracy: 0.16066 \t Validation Loss  17.40943 \t Accuracy: 0.82591\n",
      "Epoch: 7 \t Training: Loss 83.28221   \t Accuracy: 0.16718 \t Validation Loss  17.40943 \t Accuracy: 0.82591\n",
      "Epoch: 8 \t Training: Loss 82.7454   \t Accuracy: 0.17255 \t Validation Loss  17.34218 \t Accuracy: 0.82658\n",
      "Epoch: 9 \t Training: Loss 83.32694   \t Accuracy: 0.16673 \t Validation Loss  17.27493 \t Accuracy: 0.82725\n",
      "Epoch: 10 \t Training: Loss 84.60506   \t Accuracy: 0.15395 \t Validation Loss  17.14042 \t Accuracy: 0.8286\n",
      "Epoch: 11 \t Training: Loss 84.39417   \t Accuracy: 0.15606 \t Validation Loss  17.27493 \t Accuracy: 0.82725\n",
      "Epoch: 12 \t Training: Loss 83.91488   \t Accuracy: 0.16085 \t Validation Loss  17.07317 \t Accuracy: 0.82927\n",
      "Epoch: 13 \t Training: Loss 84.18967   \t Accuracy: 0.1581 \t Validation Loss  17.14042 \t Accuracy: 0.8286\n",
      "Epoch: 14 \t Training: Loss 82.13829   \t Accuracy: 0.17862 \t Validation Loss  17.20768 \t Accuracy: 0.82792\n",
      "Epoch: 15 \t Training: Loss 84.01074   \t Accuracy: 0.15989 \t Validation Loss  17.20768 \t Accuracy: 0.82792\n",
      "Epoch: 16 \t Training: Loss 83.47393   \t Accuracy: 0.16526 \t Validation Loss  17.34218 \t Accuracy: 0.82658\n",
      "Epoch: 17 \t Training: Loss 83.66564   \t Accuracy: 0.16334 \t Validation Loss  17.14042 \t Accuracy: 0.8286\n",
      "Epoch: 18 \t Training: Loss 83.59535   \t Accuracy: 0.16405 \t Validation Loss  17.34218 \t Accuracy: 0.82658\n",
      "Epoch: 19 \t Training: Loss 84.87347   \t Accuracy: 0.15127 \t Validation Loss  17.34218 \t Accuracy: 0.82658\n",
      "Epoch: 20 \t Training: Loss 83.37807   \t Accuracy: 0.16622 \t Validation Loss  17.20768 \t Accuracy: 0.82792\n",
      "Final Best Model from Best Epoch 1 Test Loss = 17.335115862827674, Test Accuracy = 0.8266488418859594\n"
     ]
    }
   ],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes = 2):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size = 11, stride = 4, padding = 5),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2),\n",
    "\n",
    "            nn.Conv2d(64, 192, kernel_size = 5, padding = 2),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "\n",
    "            nn.Conv2d(192, 384, kernel_size = 3, padding = 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "\n",
    "            nn.Conv2d(384, 256, kernel_size = 3, padding = 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "\n",
    "            nn.Conv2d(256, 256, kernel_size = 3, padding = 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "        )\n",
    "        self.classifier1 = nn.Linear(256, num_classes)\n",
    "        self.classifier2 = nn.Linear(2, num_classes)\n",
    "        self.classifier3 = nn.Linear(2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier1(x)\n",
    "        x = self.classifier2(x)\n",
    "        x = self.classifier3(x)\n",
    "        return x\n",
    "\n",
    "torch.manual_seed(101)\n",
    "model_cnn = AlexNet()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model_cnn.parameters(), lr = 0.1)\n",
    "train_losses, valid_losses, train_accurs, valid_accurs, test_loss, test_accu, best_epoch, epoch_times = train(20, model_cnn, train_loader, valid_loader, test_loader, optimizer, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training: Loss 0.16315   \t Accuracy: 0.83075 \t Validation Loss  1.70732 \t Accuracy: 0.82927\n",
      "Epoch: 2 \t Training: Loss 0.1689   \t Accuracy: 0.8311 \t Validation Loss  1.73422 \t Accuracy: 0.82658\n",
      "Epoch: 3 \t Training: Loss 0.16475   \t Accuracy: 0.83525 \t Validation Loss  1.71404 \t Accuracy: 0.8286\n",
      "Epoch: 4 \t Training: Loss 0.16948   \t Accuracy: 0.83052 \t Validation Loss  1.71404 \t Accuracy: 0.8286\n",
      "Epoch: 5 \t Training: Loss 0.16219   \t Accuracy: 0.83781 \t Validation Loss  1.72749 \t Accuracy: 0.82725\n",
      "Epoch: 6 \t Training: Loss 0.16066   \t Accuracy: 0.83934 \t Validation Loss  1.74094 \t Accuracy: 0.82591\n",
      "Epoch: 7 \t Training: Loss 0.16718   \t Accuracy: 0.83282 \t Validation Loss  1.74094 \t Accuracy: 0.82591\n",
      "Epoch: 8 \t Training: Loss 0.17255   \t Accuracy: 0.82745 \t Validation Loss  1.73422 \t Accuracy: 0.82658\n",
      "Epoch: 9 \t Training: Loss 0.16673   \t Accuracy: 0.83327 \t Validation Loss  1.72749 \t Accuracy: 0.82725\n",
      "Epoch: 10 \t Training: Loss 0.15395   \t Accuracy: 0.84605 \t Validation Loss  1.71404 \t Accuracy: 0.8286\n",
      "Final Best Model from Best Epoch 1 Test Loss = 17.335115862827674, Test Accuracy = 0.8266488418859594\n"
     ]
    }
   ],
   "source": [
    "class SamuelNet(nn.Module):\n",
    "    def __init__(self, num_classes = 2):\n",
    "        super(SamuelNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size = 11, stride = 4, padding = 5),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2),\n",
    "\n",
    "            nn.Conv2d(64, 192, kernel_size = 5, padding = 2),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "\n",
    "            nn.Conv2d(192, 384, kernel_size = 3, padding = 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "\n",
    "            nn.Conv2d(384, 256, kernel_size = 3, padding = 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "\n",
    "            nn.Conv2d(256, 256, kernel_size = 3, padding = 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "        )\n",
    "        self.classifier1 = nn.Linear(256, num_classes)\n",
    "        self.classifier2 = nn.Linear(2, num_classes)\n",
    "        self.classifier3 = nn.Linear(2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier1(x)\n",
    "        x = self.classifier2(x)\n",
    "        x = self.classifier3(x)\n",
    "        return x\n",
    "\n",
    "torch.manual_seed(101)\n",
    "model_cnn = SamuelNet()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.RMSprop(model_cnn.parameters(), lr = 0.1)\n",
    "train_losses, valid_losses, train_accurs, valid_accurs, test_loss, test_accu, best_epoch, epoch_times = train(10, model_cnn, train_loader, valid_loader, test_loader, optimizer, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training: Loss 0.52097   \t Accuracy: 0.0 \t Validation Loss  1.14004 \t Accuracy: 0.0\n",
      "Epoch: 2 \t Training: Loss 0.51335   \t Accuracy: 0.0 \t Validation Loss  1.13937 \t Accuracy: 0.0\n",
      "Epoch: 3 \t Training: Loss 0.51908   \t Accuracy: 0.0 \t Validation Loss  1.1408 \t Accuracy: 0.0\n",
      "Epoch: 4 \t Training: Loss 0.50959   \t Accuracy: 0.0 \t Validation Loss  1.13958 \t Accuracy: 0.0\n",
      "Epoch: 5 \t Training: Loss 0.51807   \t Accuracy: 0.0 \t Validation Loss  1.13986 \t Accuracy: 0.0\n",
      "Epoch: 6 \t Training: Loss 0.51767   \t Accuracy: 0.0 \t Validation Loss  1.13984 \t Accuracy: 0.0\n",
      "Epoch: 7 \t Training: Loss 0.51837   \t Accuracy: 0.0 \t Validation Loss  1.14253 \t Accuracy: 0.0\n",
      "Epoch: 8 \t Training: Loss 0.50923   \t Accuracy: 0.0 \t Validation Loss  1.14119 \t Accuracy: 0.0\n",
      "Epoch: 9 \t Training: Loss 0.52017   \t Accuracy: 0.0 \t Validation Loss  1.14119 \t Accuracy: 0.0\n",
      "Epoch: 10 \t Training: Loss 0.51965   \t Accuracy: 0.0 \t Validation Loss  1.13849 \t Accuracy: 0.0\n",
      "Epoch: 11 \t Training: Loss 0.51714   \t Accuracy: 0.0 \t Validation Loss  1.14119 \t Accuracy: 0.0\n",
      "Epoch: 12 \t Training: Loss 0.51486   \t Accuracy: 0.0 \t Validation Loss  1.13849 \t Accuracy: 0.0\n",
      "Epoch: 13 \t Training: Loss 0.51625   \t Accuracy: 0.0 \t Validation Loss  1.14051 \t Accuracy: 0.0\n",
      "Epoch: 14 \t Training: Loss 0.51606   \t Accuracy: 0.0 \t Validation Loss  1.13849 \t Accuracy: 0.0\n",
      "Epoch: 15 \t Training: Loss 0.51879   \t Accuracy: 0.0 \t Validation Loss  1.14186 \t Accuracy: 0.0\n",
      "Epoch: 16 \t Training: Loss 0.526   \t Accuracy: 0.0 \t Validation Loss  1.13984 \t Accuracy: 0.0\n",
      "Epoch: 17 \t Training: Loss 0.51774   \t Accuracy: 0.0 \t Validation Loss  1.14051 \t Accuracy: 0.0\n",
      "Epoch: 18 \t Training: Loss 0.51975   \t Accuracy: 0.0 \t Validation Loss  1.14119 \t Accuracy: 0.0\n",
      "Epoch: 19 \t Training: Loss 0.51523   \t Accuracy: 0.0 \t Validation Loss  1.14051 \t Accuracy: 0.0\n",
      "Epoch: 20 \t Training: Loss 0.52139   \t Accuracy: 0.0 \t Validation Loss  1.14119 \t Accuracy: 0.0\n",
      "Final Best Model from Best Epoch 14 Test Loss = 1.13991066755033, Test Accuracy = 0.0\n"
     ]
    }
   ],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes = 2):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size = 11, stride = 4, padding = 5),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size = 5, padding = 2),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "\n",
    "            nn.Conv2d(64, 32, kernel_size = 3, padding = 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            # nn.Conv2d(384, 256, kernel_size = 3, padding = 1),\n",
    "            # nn.ReLU(inplace = True),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size = 3, padding = 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "        )\n",
    "        self.relu    = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5, inplace = True)\n",
    "        \n",
    "        self.classifier1 = nn.Linear(32, 32)\n",
    "        self.classifier2 = nn.Linear(32,  32)\n",
    "        self.classifier3 = nn.Linear(32, num_classes)\n",
    "        self.classifier4 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.classifier1(x))\n",
    "        x = self.relu(self.classifier2(x))\n",
    "        x = self.classifier3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier4(x)\n",
    "        return x\n",
    "\n",
    "torch.manual_seed(101)\n",
    "model_cnn = AlexNet()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.RMSprop(model_cnn.parameters(), lr = 0.1)\n",
    "train_losses, valid_losses, train_accurs, valid_accurs, test_loss, test_accu, best_epoch, epoch_times = train(20, model_cnn, train_loader, valid_loader, test_loader, optimizer, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "class EarlyStopping:\n",
    "  def __init__(self, patience=1, delta=0, path = 'checkpoint.pt'):\n",
    "    self.patience = patience\n",
    "    self.delta = delta\n",
    "    self.path= path\n",
    "    self.counter = 0\n",
    "    self.best_score = None\n",
    "    self.early_stop = False\n",
    "\n",
    "  def __call__(self, val_loss, model):\n",
    "    if self.best_score is None:\n",
    "      self.best_score = val_loss\n",
    "      self.save_checkpoint(model)\n",
    "    elif val_loss > self.best_score:\n",
    "      self.counter +=1\n",
    "      if self.counter >= self.patience:\n",
    "        self.early_stop = True \n",
    "    else:\n",
    "      self.best_score = val_loss\n",
    "      self.save_checkpoint(model)\n",
    "      self.counter = 0      \n",
    "\n",
    "  def save_checkpoint(self, model):\n",
    "    torch.save(model.state_dict(), self.path)\n",
    "\n",
    "\n",
    "def fit_one_epoch(train_loader, epoch, num_epochs): \n",
    "    step_train = 0\n",
    "\n",
    "    train_losses = list() \n",
    "    train_acc = list()\n",
    "    model_vgg19.train()\n",
    "    for i, (images, targets) in enumerate(tqdm(train_loader)):\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        logits = model_vgg19(images)\n",
    "        loss = criterion(logits, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_losses.append(loss.item())\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        num_correct = sum(predictions.eq(targets))\n",
    "        running_train_acc = float(num_correct) / float(images.shape[0])\n",
    "        train_acc.append(running_train_acc)\n",
    "        \n",
    "    train_loss = torch.tensor(train_losses).mean()    \n",
    "    print(f'Epoch {epoch}/{num_epochs-1}')  \n",
    "    print(f'Training loss: {train_loss:.2f}')\n",
    "\n",
    "def val_one_epoch(val_loader):\n",
    "        val_losses = list()\n",
    "        val_accs = list()\n",
    "        \n",
    "        model_vgg19.eval()\n",
    "        step_val = 0\n",
    "        with torch.no_grad():\n",
    "            for (images, targets) in val_loader:\n",
    "                images = images.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                logits = model_vgg19(images)\n",
    "                loss = criterion(logits, targets)\n",
    "                val_losses.append(loss.item())      \n",
    "            \n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "                num_correct = sum(predictions.eq(targets))\n",
    "                running_val_acc = float(num_correct) / float(images.shape[0])\n",
    "\n",
    "                val_accs.append(running_val_acc)\n",
    "          \n",
    "            val_loss = torch.tensor(val_losses).mean()\n",
    "            val_acc = torch.tensor(val_accs).mean() \n",
    "        \n",
    "            print(f'Validation loss: {val_loss:.2f}')  \n",
    "            print(f'Validation accuracy: {val_acc:.2f}') \n",
    "\n",
    "def fit(train_loader, val_loader, num_epochs = 10, unfreeze_after = 5, checkpoint_dir = 'checkpoint.pt'):\n",
    "    for epoch in range(num_epochs):\n",
    "        fit_one_epoch(train_loader, epoch, num_epochs)\n",
    "        val_one_epoch(val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
