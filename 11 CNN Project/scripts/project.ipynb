{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "from pathlib import Path\n",
    "import sys, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Subset\n",
    "from torchvision.utils import make_grid\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from PIL import Image\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from collections import Counter\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration: Baseline Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_path = 'D:/nodule/data/labels/'\n",
    "\n",
    "label = [pd.DataFrame(pd.read_csv(os.path.join(label_path, file), delim_whitespace = True)) for file in os.listdir(label_path)]\n",
    "df = pd.concat(label, ignore_index = True)\n",
    "df['types'] = [string.split('/') for string in df['image']]\n",
    "df['types'] = [string[0] for string in df['types']]\n",
    "df['image'] = [string.split('/') for string in df['image']]\n",
    "df['image'] = [string[1][6:-4] for string in df['image']]\n",
    "\n",
    "dict = {}\n",
    "image_path = 'D:/nodule/data/images/'\n",
    "folders = os.listdir(image_path)\n",
    "images = ([os.listdir(os.path.join(image_path, folder)) for folder in folders])\n",
    "test_images  = images[0]\n",
    "train_images = images[1]\n",
    "valid_images = images[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoduleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_path, image_path, transforms = None):    \n",
    "        self.transform  = transforms\n",
    "        self.data_path  = data_path\n",
    "        self.images_dir = data_path / 'images' / Path(image_path)\n",
    "\n",
    "        if   image_path == 'train': \n",
    "            self.labels_dir = data_path / 'labels' / 'trainlabels.txt'\n",
    "        elif image_path == 'val':\n",
    "            self.labels_dir = data_path / 'labels' / 'vallabels.txt'\n",
    "        elif image_path == 'test' :\n",
    "            self.labels_dir = data_path / 'labels' / 'testlabels.txt'\n",
    "        self.dataset = self._load_data(self.images_dir, self.labels_dir)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        dataset  = self.dataset\n",
    "        img_path = self.images_dir / dataset[index][0]\n",
    "        with open(img_path, 'rb') as f:\n",
    "            image = Image.open(f).convert('RGB')\n",
    "\n",
    "        label = dataset[index][1]\n",
    "        return self.transform(image), label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset) \n",
    "\n",
    "    def _load_data(self, images_dir, labels_file):\n",
    "        with open(labels_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        data = []\n",
    "        for line in lines[1:]:\n",
    "            filename, label = line.strip().split()\n",
    "            filename = os.path.basename(filename)\n",
    "            label = int(label)\n",
    "            data.append((filename, label))\n",
    "        return data\n",
    "    \n",
    "train_transform = transforms.Compose([\n",
    "        transforms.RandomRotation(10),      \n",
    "        transforms.RandomHorizontalFlip(),  \n",
    "        transforms.Resize((50, 50)), \n",
    "        transforms.RandomCrop(32, padding = 2),         \n",
    "        transforms.CenterCrop(40),         \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "        transforms.Resize((50, 50)),\n",
    "        transforms.CenterCrop(40),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "data_path  = Path('D:/nodule/data/')\n",
    "train_dataset  = NoduleDataset(data_path, image_path = 'train', transforms = train_transform)\n",
    "valid_dataset  = NoduleDataset(data_path, image_path = 'val',   transforms = valid_transform)\n",
    "test_dataset   = NoduleDataset(data_path, image_path = 'test',  transforms = valid_transform)\n",
    "\n",
    "train_classes = [label for _, label in train_dataset]\n",
    "class_count = Counter(train_classes)\n",
    "class_weights = torch.Tensor([len(train_classes)/c for c in pd.Series(class_count).sort_index().values])\n",
    "class_samples = [0] * len(class_weights)\n",
    "\n",
    "for _, label in train_dataset:\n",
    "    class_samples[label] += 1\n",
    "weights = [class_weights[label] / class_samples[label] for _, label in train_dataset]\n",
    "sampler = WeightedRandomSampler(weights = weights, num_samples = len(weights), replacement = True)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "train_loader   = DataLoader(train_dataset, batch_size = 32, sampler = sampler)\n",
    "valid_loader   = DataLoader(valid_dataset, batch_size = 32, shuffle = True   )\n",
    "test_loader    = DataLoader(test_dataset,  batch_size = 32, shuffle = False  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoduleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((50, 50)),\n",
    "            transforms.RandomCrop(32, padding = 2),\n",
    "            transforms.RandomRotation(90),\n",
    "            transforms.CenterCrop(40),\n",
    "            transforms.ToTensor(), \n",
    "            transforms.Normalize(mean = [0.485, 0.456, 0.406],\n",
    "                                 std  = [0.229, 0.224, 0.225])])\n",
    "        \n",
    "        self.images_dir = data_dir / 'images'\n",
    "        self.labels_dir = data_dir / 'labels'\n",
    "\n",
    "        self.train_images_dir = self.images_dir \n",
    "        self.val_images_dir   = self.images_dir  \n",
    "        self.test_images_dir  = self.images_dir  \n",
    "\n",
    "        self.train_labels_file = self.labels_dir  / 'trainlabels.txt'\n",
    "        self.val_labels_file   = self.labels_dir  / 'vallabels.txt'\n",
    "        self.test_labels_file  = self.labels_dir  / 'testlabels.txt'\n",
    "\n",
    "        self.train_data = self._load_data(self.train_images_dir, self.train_labels_file)\n",
    "        self.val_data   = self._load_data(self.val_images_dir, self.val_labels_file)\n",
    "        self.test_data  = self._load_data(self.test_images_dir, self.test_labels_file)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index < len(self.train_data):\n",
    "            images_dir = self.train_images_dir\n",
    "            data = self.train_data\n",
    "        elif index < len(self.train_data) + len(self.val_data):\n",
    "            images_dir = self.val_images_dir\n",
    "            data = self.val_data\n",
    "            index -= len(self.train_data)\n",
    "        else:\n",
    "            images_dir = self.test_images_dir\n",
    "            data = self.test_data\n",
    "            index -= (len(self.train_data) + len(self.val_data))\n",
    "\n",
    "        img_path = images_dir / data[index][0]\n",
    "        with open(img_path, 'rb') as f:\n",
    "            image = Image.open(f).convert('RGB')\n",
    "\n",
    "        label = data[index][1]\n",
    "        return self.transform(image), label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_data) + len(self.val_data) + len(self.test_data)\n",
    "\n",
    "    def _load_data(self, images_dir, labels_file):\n",
    "        with open(labels_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        data = []\n",
    "        for line in lines[1:]:\n",
    "            filename, label = line.strip().split()\n",
    "            filename = filename \n",
    "            label = int(label)\n",
    "            data.append((filename, label))\n",
    "        return data\n",
    "\n",
    "    def get_datasets(self):\n",
    "        train_dataset = Subset(self, range(len(self.train_data)))\n",
    "        test_dataset  = Subset(self, range(len(self.train_data),  len(self.train_data) + len(self.test_data)))\n",
    "        valid_dataset = Subset(self, range(len(self.train_data) + len(self.test_data),   len(self)))\n",
    "        return train_dataset, test_dataset, valid_dataset\n",
    "\n",
    "def GET_NODULEDATASET():\n",
    "    train_indices = list(range(0, len(dataset.train_data)))\n",
    "    valid_indices = list(range(len(dataset.train_data),  len(dataset.train_data) + len(dataset.val_data)))\n",
    "    test_indices  = list(range(len(dataset.train_data) + len(dataset.val_data), len(dataset)))\n",
    "\n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    valid_dataset = Subset(dataset, valid_indices)\n",
    "    test_dataset  = Subset(dataset, test_indices)\n",
    "    return train_dataset, valid_dataset, test_dataset\n",
    "\n",
    "data_dir = Path('D:/nodule/data/')\n",
    "dataset  = NoduleDataset(data_dir)\n",
    "train_dataset, valid_dataset, test_dataset = GET_NODULEDATASET()\n",
    "\n",
    "train_classes = [label for _, label in train_dataset]\n",
    "class_count = Counter(train_classes)\n",
    "class_weights = torch.Tensor([len(train_classes)/c for c in pd.Series(class_count).sort_index().values])\n",
    "class_samples = [0] * len(class_weights)\n",
    "\n",
    "for _, label in train_dataset:\n",
    "    class_samples[label] += 1\n",
    "weights = [class_weights[label] / class_samples[label] for _, label in train_dataset]\n",
    "sampler = WeightedRandomSampler(weights = weights, num_samples = len(weights), replacement = True)\n",
    "\n",
    "train_loader  = DataLoader(train_dataset, batch_size = 32, sampler = sampler)\n",
    "valid_loader  = DataLoader(valid_dataset, batch_size = 32, shuffle = True )\n",
    "test_loader   = DataLoader(test_dataset,  batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EarlyStopping:\n",
    "  def __init__(self, patience  = 1, delta = 0, path = 'checkpoint.pt'):\n",
    "    self.patience = patience\n",
    "    self.delta = delta\n",
    "    self.path= path\n",
    "    self.counter = 0\n",
    "    self.best_score = None\n",
    "    self.early_stop = False\n",
    "\n",
    "  def __call__(self, val_loss, model):\n",
    "    if self.best_score is None:\n",
    "      self.best_score = val_loss\n",
    "      self.save_checkpoint(model)\n",
    "    elif val_loss > self.best_score:\n",
    "      self.counter +=1\n",
    "      if self.counter >= self.patience:\n",
    "        self.early_stop = True \n",
    "    else:\n",
    "      self.best_score = val_loss\n",
    "      self.save_checkpoint(model)\n",
    "      self.counter = 0      \n",
    "\n",
    "  def save_checkpoint(self, model):\n",
    "    torch.save(model.state_dict(), self.path)\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(preds)\n",
    "    correct       = (rounded_preds == y).float()\n",
    "    acc           = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def get_metrics(prediction, label):\n",
    "    prediction  = prediction.cpu().detach().numpy()\n",
    "    label = label.cpu().detach().numpy()\n",
    "    \n",
    "    tp = np.sum((prediction == 1) & (label == 1))\n",
    "    tn = np.sum((prediction == 0) & (label == 0))\n",
    "    fp = np.sum((prediction == 1) & (label == 0))\n",
    "    fn = np.sum((prediction == 0) & (label == 1))\n",
    "\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    \n",
    "    try:\n",
    "        auc = roc_auc_score(label, prediction)\n",
    "    except ValueError:\n",
    "        auc = None\n",
    "    fpr, tpr, _ = roc_curve(label, prediction)\n",
    "\n",
    "    return {'sensitivity': sensitivity,\n",
    "            'specificity': specificity,\n",
    "            'auc': auc,\n",
    "            'fpr': fpr,\n",
    "            'tpr': tpr}\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "def train(num_epochs, model, train_loader, valid_loader, test_loader, optimizer, criterion, device, accuracy = True):\n",
    "    best_valid_loss = float('inf')\n",
    "    train_losses, valid_losses = [], []\n",
    "    train_accurs, valid_accurs = [], []\n",
    "    train_metric, valid_metric = [], []\n",
    "\n",
    "    epoch_times = []\n",
    "    list_best_epochs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss, train_accu, train_metr = _train(model, train_loader, optimizer, criterion, device)\n",
    "        valid_loss, valid_accu, valid_metr = _evals(model, valid_loader, criterion, device)\n",
    "\n",
    "        if accuracy:\n",
    "            print(f'Epoch: {epoch + 1} \\t Training: Loss {np.round(train_loss, 5)}   \\t Accuracy: {np.round(train_accu, 5)} \\t Validation Loss  {np.round(valid_loss, 5)} \\t Accuracy: {np.round(valid_accu, 5)}')\n",
    "        else:\n",
    "           print(f'Epoch: {epoch + 1} \\t Training: Loss {np.round(train_loss, 5)} \\t Validation Loss  {np.round(valid_loss, 5)}')\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accurs.append(train_accu)\n",
    "        train_metric.append(train_metr)\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_accurs.append(valid_accu)\n",
    "        valid_metric.append(valid_metr)\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_epoch = epoch\n",
    "        list_best_epochs.append(best_epoch)\n",
    "    test_loss, test_accu, test_metr  = _evals(best_model, test_loader, criterion, device)\n",
    "    print(f'Final Best Model from Best Epoch {best_epoch + 1} Test Loss = {test_loss}, Test Accuracy = {test_accu}')\n",
    "    return train_losses, valid_losses, train_accurs, valid_accurs, test_loss, test_accu, train_metric, valid_metric, best_epoch, epoch_times\n",
    "\n",
    "def _train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "    epoch_train_accu = 0\n",
    "\n",
    "    for idx, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        outputs = torch.sigmoid(outputs)[:, 0]\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "        accuracy = binary_accuracy(outputs, labels)\n",
    "        metric  = get_metrics(outputs, labels)\n",
    "        epoch_train_accu += accuracy.item()\n",
    "\n",
    "    epoch_train_loss = epoch_train_loss / len(train_loader) \n",
    "    epoch_train_accu = epoch_train_accu / len(train_loader) \n",
    "    return epoch_train_loss, epoch_train_accu, metric\n",
    "\n",
    "def _evals(model, valid_loader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_valid_loss = 0\n",
    "    epoch_valid_accu = 0\n",
    "\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for id, data in enumerate(valid_loader):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            outputs = torch.sigmoid(outputs)[:, 0]\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            rounded_preds = torch.round(torch.sigmoid(outputs)).long().flatten().tolist()\n",
    "            all_predictions.extend(rounded_preds)\n",
    "\n",
    "            epoch_valid_loss += loss.item()\n",
    "            accuracy = binary_accuracy(outputs, labels)\n",
    "            metric  = get_metrics(outputs, labels)\n",
    "            epoch_valid_accu += accuracy.item()\n",
    "    epoch_valid_loss = epoch_valid_loss / len(valid_loader)\n",
    "    epoch_valid_accu = epoch_valid_accu / len(valid_loader)\n",
    "    return epoch_valid_loss, epoch_valid_accu, metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training: Loss 0.69466   \t Accuracy: 0.0 \t Validation Loss  0.69315 \t Accuracy: 0.0\n",
      "Epoch: 2 \t Training: Loss 0.69315   \t Accuracy: 0.0 \t Validation Loss  0.69315 \t Accuracy: 0.0\n",
      "Final Best Model from Best Epoch 1 Test Loss = 0.6931471801271626, Test Accuracy = 0.0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "model_vgg19 = torchvision.models.vgg19(weights = True).to(device)\n",
    "model_vgg19.classifier[-1] = torch.nn.Linear(4096, 2)\n",
    "model_vgg19.classifier.add_module('logsoftmax', nn.LogSoftmax(dim = 1))\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model_vgg19.parameters(), lr = 0.01, eps = 10e-06)\n",
    "train_losses, valid_losses, train_accurs, valid_accurs, test_loss, test_accu, best_epoch, epoch_times = train(2, model_vgg19, train_loader, valid_loader, test_loader, optimizer, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training: Loss 15.7254   \t Accuracy: 0.83666 \t Validation Loss  82.79232 \t Accuracy: 0.17208\n",
      "Epoch: 2 \t Training: Loss 16.21933   \t Accuracy: 0.83781 \t Validation Loss  82.72507 \t Accuracy: 0.17275\n",
      "Final Best Model from Best Epoch 1 Test Loss = 82.66488408107384, Test Acc = 0.17335115869839987\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "model_vgg19 = torchvision.models.vgg19(weights = True).to(device)\n",
    "model_vgg19.classifier[-1] = torch.nn.Linear(4096, 2)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model_vgg19.parameters(), lr = 1e-2, eps = 10e-06)\n",
    "train_losses, valid_losses, train_accurs, valid_accurs, test_loss, test_accu, best_epoch, epoch_times = train(2, model_vgg19, train_loader, valid_loader, test_loader, optimizer, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training: Loss 16.83681   \t Accuracy: 0.82554                                   \t Validation Loss  82.79232 \t Accuracy: 0.17208\n",
      "Epoch: 2 \t Training: Loss 16.27045   \t Accuracy: 0.8373                                   \t Validation Loss  82.79232 \t Accuracy: 0.17208\n",
      "Final Best Model from Best Epoch 0 Test Loss = 82.66488408107384, Test Acc = 0.17335115869839987\n"
     ]
    }
   ],
   "source": [
    "model_alexnet = models.alexnet(pretrained = True)\n",
    "\n",
    "model_alexnet.features[0] = torch.nn.Conv2d(3,  64, kernel_size = (3, 3), stride = (1, 1), padding = (1, 1))\n",
    "model_alexnet.features[2] = torch.nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0, dilation = 1, ceil_mode = False)\n",
    "model_alexnet.features[3] = torch.nn.Conv2d(64, 192, kernel_size = (3, 3), stride = (1, 1), padding = (1, 1))\n",
    "model_alexnet.features[5] = torch.nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0, dilation = 1, ceil_mode = False)\n",
    "\n",
    "model_alexnet.classifier[6] = torch.nn.Linear(4096, 2)\n",
    "model_alexnet.classifier[6].requires_grad = True\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model_alexnet.classifier.parameters(), lr = 0.001, eps = 10e-06)\n",
    "train_losses, valid_losses, train_accurs, valid_accurs, test_loss, test_accu, best_epoch, epoch_times = train(2, model_alexnet, train_loader, valid_loader, test_loader, optimizer, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training: Loss 0.48521   \t Accuracy: 0.0 \t Validation Loss  1.13984 \t Accuracy: 0.0\n",
      "Epoch: 2 \t Training: Loss 0.4807   \t Accuracy: 0.0 \t Validation Loss  1.14051 \t Accuracy: 0.0\n",
      "Final Best Model from Best Epoch 1 Test Loss = 1.13991066755033, Test Accuracy = 0.0\n"
     ]
    }
   ],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3,  32, 3, 1, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1, padding = 1)\n",
    "\n",
    "        self.fc1 = nn.Linear(64 * 10 * 10, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 2)\n",
    "\n",
    "        self.dropout = nn.Dropout(p = 0.5)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(32)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.batchnorm1(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.batchnorm2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = nn.functional.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "torch.manual_seed(101)\n",
    "model_cnn = LeNet()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model_cnn.parameters(), lr = 0.1)\n",
    "train_losses, valid_losses, train_accurs, valid_accurs, test_loss, test_accu, best_epoch, epoch_times = train(2, model_cnn, train_loader, valid_loader, test_loader, optimizer, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training: Loss 82.80669   \t Accuracy: 0.16584 \t Validation Loss  17.07317 \t Accuracy: 0.82927\n",
      "Epoch: 2 \t Training: Loss 83.10966   \t Accuracy: 0.1689 \t Validation Loss  17.34218 \t Accuracy: 0.82658\n",
      "Epoch: 3 \t Training: Loss 83.52505   \t Accuracy: 0.16475 \t Validation Loss  17.14042 \t Accuracy: 0.8286\n",
      "Epoch: 4 \t Training: Loss 83.05215   \t Accuracy: 0.16948 \t Validation Loss  17.14042 \t Accuracy: 0.8286\n",
      "Epoch: 5 \t Training: Loss 83.78067   \t Accuracy: 0.16219 \t Validation Loss  17.27493 \t Accuracy: 0.82725\n",
      "Epoch: 6 \t Training: Loss 83.93405   \t Accuracy: 0.16066 \t Validation Loss  17.40943 \t Accuracy: 0.82591\n",
      "Epoch: 7 \t Training: Loss 83.28221   \t Accuracy: 0.16718 \t Validation Loss  17.40943 \t Accuracy: 0.82591\n",
      "Epoch: 8 \t Training: Loss 82.7454   \t Accuracy: 0.17255 \t Validation Loss  17.34218 \t Accuracy: 0.82658\n",
      "Epoch: 9 \t Training: Loss 83.32694   \t Accuracy: 0.16673 \t Validation Loss  17.27493 \t Accuracy: 0.82725\n",
      "Epoch: 10 \t Training: Loss 84.60506   \t Accuracy: 0.15395 \t Validation Loss  17.14042 \t Accuracy: 0.8286\n",
      "Epoch: 11 \t Training: Loss 84.39417   \t Accuracy: 0.15606 \t Validation Loss  17.27493 \t Accuracy: 0.82725\n",
      "Epoch: 12 \t Training: Loss 83.91488   \t Accuracy: 0.16085 \t Validation Loss  17.07317 \t Accuracy: 0.82927\n",
      "Epoch: 13 \t Training: Loss 84.18967   \t Accuracy: 0.1581 \t Validation Loss  17.14042 \t Accuracy: 0.8286\n",
      "Epoch: 14 \t Training: Loss 82.13829   \t Accuracy: 0.17862 \t Validation Loss  17.20768 \t Accuracy: 0.82792\n",
      "Epoch: 15 \t Training: Loss 84.01074   \t Accuracy: 0.15989 \t Validation Loss  17.20768 \t Accuracy: 0.82792\n",
      "Epoch: 16 \t Training: Loss 83.47393   \t Accuracy: 0.16526 \t Validation Loss  17.34218 \t Accuracy: 0.82658\n",
      "Epoch: 17 \t Training: Loss 83.66564   \t Accuracy: 0.16334 \t Validation Loss  17.14042 \t Accuracy: 0.8286\n",
      "Epoch: 18 \t Training: Loss 83.59535   \t Accuracy: 0.16405 \t Validation Loss  17.34218 \t Accuracy: 0.82658\n",
      "Epoch: 19 \t Training: Loss 84.87347   \t Accuracy: 0.15127 \t Validation Loss  17.34218 \t Accuracy: 0.82658\n",
      "Epoch: 20 \t Training: Loss 83.37807   \t Accuracy: 0.16622 \t Validation Loss  17.20768 \t Accuracy: 0.82792\n",
      "Final Best Model from Best Epoch 1 Test Loss = 17.335115862827674, Test Accuracy = 0.8266488418859594\n"
     ]
    }
   ],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes = 2):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size = 11, stride = 4, padding = 5),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2),\n",
    "\n",
    "            nn.Conv2d(64, 192, kernel_size = 5, padding = 2),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "\n",
    "            nn.Conv2d(192, 384, kernel_size = 3, padding = 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "\n",
    "            nn.Conv2d(384, 256, kernel_size = 3, padding = 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "\n",
    "            nn.Conv2d(256, 256, kernel_size = 3, padding = 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "        )\n",
    "        self.classifier1 = nn.Linear(256, num_classes)\n",
    "        self.classifier2 = nn.Linear(2, num_classes)\n",
    "        self.classifier3 = nn.Linear(2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier1(x)\n",
    "        x = self.classifier2(x)\n",
    "        x = self.classifier3(x)\n",
    "        return x\n",
    "\n",
    "torch.manual_seed(101)\n",
    "model_cnn = AlexNet()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model_cnn.parameters(), lr = 0.1)\n",
    "train_losses, valid_losses, train_accurs, valid_accurs, test_loss, test_accu, best_epoch, epoch_times = train(20, model_cnn, train_loader, valid_loader, test_loader, optimizer, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training: Loss 0.16315   \t Accuracy: 0.83075 \t Validation Loss  1.70732 \t Accuracy: 0.82927\n",
      "Epoch: 2 \t Training: Loss 0.1689   \t Accuracy: 0.8311 \t Validation Loss  1.73422 \t Accuracy: 0.82658\n",
      "Epoch: 3 \t Training: Loss 0.16475   \t Accuracy: 0.83525 \t Validation Loss  1.71404 \t Accuracy: 0.8286\n",
      "Epoch: 4 \t Training: Loss 0.16948   \t Accuracy: 0.83052 \t Validation Loss  1.71404 \t Accuracy: 0.8286\n",
      "Epoch: 5 \t Training: Loss 0.16219   \t Accuracy: 0.83781 \t Validation Loss  1.72749 \t Accuracy: 0.82725\n",
      "Epoch: 6 \t Training: Loss 0.16066   \t Accuracy: 0.83934 \t Validation Loss  1.74094 \t Accuracy: 0.82591\n",
      "Epoch: 7 \t Training: Loss 0.16718   \t Accuracy: 0.83282 \t Validation Loss  1.74094 \t Accuracy: 0.82591\n",
      "Epoch: 8 \t Training: Loss 0.17255   \t Accuracy: 0.82745 \t Validation Loss  1.73422 \t Accuracy: 0.82658\n",
      "Epoch: 9 \t Training: Loss 0.16673   \t Accuracy: 0.83327 \t Validation Loss  1.72749 \t Accuracy: 0.82725\n",
      "Epoch: 10 \t Training: Loss 0.15395   \t Accuracy: 0.84605 \t Validation Loss  1.71404 \t Accuracy: 0.8286\n",
      "Final Best Model from Best Epoch 1 Test Loss = 17.335115862827674, Test Accuracy = 0.8266488418859594\n"
     ]
    }
   ],
   "source": [
    "class SamuelNet(nn.Module):\n",
    "    def __init__(self, num_classes = 2):\n",
    "        super(SamuelNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size = 11, stride = 4, padding = 5),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2),\n",
    "\n",
    "            nn.Conv2d(64, 192, kernel_size = 5, padding = 2),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "\n",
    "            nn.Conv2d(192, 384, kernel_size = 3, padding = 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "\n",
    "            nn.Conv2d(384, 256, kernel_size = 3, padding = 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "\n",
    "            nn.Conv2d(256, 256, kernel_size = 3, padding = 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "        )\n",
    "        self.classifier1 = nn.Linear(256, num_classes)\n",
    "        self.classifier2 = nn.Linear(2, num_classes)\n",
    "        self.classifier3 = nn.Linear(2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier1(x)\n",
    "        x = self.classifier2(x)\n",
    "        x = self.classifier3(x)\n",
    "        return x\n",
    "\n",
    "torch.manual_seed(101)\n",
    "model_cnn = SamuelNet()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.RMSprop(model_cnn.parameters(), lr = 0.1)\n",
    "train_losses, valid_losses, train_accurs, valid_accurs, test_loss, test_accu, best_epoch, epoch_times = train(10, model_cnn, train_loader, valid_loader, test_loader, optimizer, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training: Loss 0.59994   \t Accuracy: 0.8288 \t Validation Loss  0.63598 \t Accuracy: 0.82725\n",
      "Epoch: 2 \t Training: Loss 0.5582   \t Accuracy: 0.84209 \t Validation Loss  0.64121 \t Accuracy: 0.82658\n",
      "Epoch: 3 \t Training: Loss 0.52862   \t Accuracy: 0.83052 \t Validation Loss  0.65187 \t Accuracy: 0.78381\n",
      "Epoch: 4 \t Training: Loss 0.49307   \t Accuracy: 0.83333 \t Validation Loss  0.65411 \t Accuracy: 0.76466\n",
      "Epoch: 5 \t Training: Loss 0.46103   \t Accuracy: 0.83531 \t Validation Loss  0.64935 \t Accuracy: 0.78609\n",
      "Epoch: 6 \t Training: Loss 0.44167   \t Accuracy: 0.82733 \t Validation Loss  0.67351 \t Accuracy: 0.66898\n",
      "Epoch: 7 \t Training: Loss 0.40419   \t Accuracy: 0.83442 \t Validation Loss  0.67353 \t Accuracy: 0.64096\n",
      "Epoch: 8 \t Training: Loss 0.39412   \t Accuracy: 0.83768 \t Validation Loss  0.66913 \t Accuracy: 0.66051\n",
      "Epoch: 9 \t Training: Loss 0.38287   \t Accuracy: 0.83442 \t Validation Loss  0.67979 \t Accuracy: 0.62428\n",
      "Epoch: 10 \t Training: Loss 0.37337   \t Accuracy: 0.83915 \t Validation Loss  0.65594 \t Accuracy: 0.7313\n",
      "Final Best Model from Best Epoch 1 Test Loss = 0.6355759232651954, Test Accuracy = 0.8266488418859594\n"
     ]
    }
   ],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes = 2):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size = 11, stride = 4, padding = 5),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2),\n",
    "\n",
    "            nn.Conv2d(32, 256, kernel_size = 5, padding = 2),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "\n",
    "            nn.Conv2d(256, 32, kernel_size = 3, padding = 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.BatchNorm2d(32),\n",
    "\n",
    "            nn.Conv2d(32, 8, kernel_size = 3, padding = 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "        )\n",
    "        self.relu    = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5, inplace = True)\n",
    "        self.classifier1 = nn.Linear(8, 8)\n",
    "        self.classifier2 = nn.Linear(8, 4)\n",
    "        self.classifier3 = nn.Linear(4, num_classes)\n",
    "        self.classifier4 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.classifier1(x))\n",
    "        x = self.relu(self.classifier2(x))\n",
    "        x = self.classifier3(x)\n",
    "        return x\n",
    "\n",
    "torch.manual_seed(101)\n",
    "model_cnn = AlexNet()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model_cnn.parameters(), lr = 0.0001)\n",
    "train_losses, valid_losses, train_accurs, valid_accurs, test_loss, test_accu, best_epoch, epoch_times = train(10, model_cnn, train_loader, valid_loader, test_loader, optimizer, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training: Loss 0.47196   \t Accuracy: 0.83992 \t Validation Loss  1.19423 \t Accuracy: 0.17409\n",
      "Epoch: 2 \t Training: Loss 0.46504   \t Accuracy: 0.83646 \t Validation Loss  0.92657 \t Accuracy: 0.17275\n",
      "Epoch: 3 \t Training: Loss 0.46407   \t Accuracy: 0.83289 \t Validation Loss  0.72725 \t Accuracy: 0.26049\n",
      "Epoch: 4 \t Training: Loss 0.44666   \t Accuracy: 0.84011 \t Validation Loss  0.68376 \t Accuracy: 0.51175\n",
      "Epoch: 5 \t Training: Loss 0.44792   \t Accuracy: 0.83736 \t Validation Loss  0.72203 \t Accuracy: 0.41324\n",
      "Epoch: 6 \t Training: Loss 0.43253   \t Accuracy: 0.83877 \t Validation Loss  0.60734 \t Accuracy: 0.71359\n",
      "Epoch: 7 \t Training: Loss 0.44568   \t Accuracy: 0.82656 \t Validation Loss  0.72533 \t Accuracy: 0.34339\n",
      "Epoch: 8 \t Training: Loss 0.4385   \t Accuracy: 0.83825 \t Validation Loss  0.67933 \t Accuracy: 0.56914\n",
      "Epoch: 9 \t Training: Loss 0.42681   \t Accuracy: 0.83468 \t Validation Loss  0.64987 \t Accuracy: 0.6905\n",
      "Epoch: 10 \t Training: Loss 0.42705   \t Accuracy: 0.83289 \t Validation Loss  0.60577 \t Accuracy: 0.79533\n",
      "Final Best Model from Best Epoch 10 Test Loss = 0.5972615573920456, Test Accuracy = 0.791443850479874\n"
     ]
    }
   ],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3,  32, 3, 1, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1, padding = 1)\n",
    "\n",
    "        self.fc1 = nn.Linear(64 * 10 * 10, 256)\n",
    "        self.fc2 = nn.Linear(256, 32)\n",
    "        self.fc3 = nn.Linear(32,   2)\n",
    "\n",
    "        self.dropout = nn.Dropout(p = 0.5)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(32)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.batchnorm1(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.batchnorm2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "torch.manual_seed(101)\n",
    "model_cnn = LeNet()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model_cnn.parameters(), lr = 0.001)\n",
    "train_losses, valid_losses, train_accurs, valid_accurs, test_loss, test_accu, best_epoch, epoch_times = train(10, model_cnn, train_loader, valid_loader, test_loader, optimizer, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training: Loss 0.47196   \t Accuracy: 0.83992 \t Validation Loss  1.19423 \t Accuracy: 0.17409\n",
      "Epoch: 2 \t Training: Loss 0.46504   \t Accuracy: 0.83646 \t Validation Loss  0.92657 \t Accuracy: 0.17275\n",
      "Epoch: 3 \t Training: Loss 0.46407   \t Accuracy: 0.83289 \t Validation Loss  0.72725 \t Accuracy: 0.26049\n",
      "Epoch: 4 \t Training: Loss 0.44666   \t Accuracy: 0.84011 \t Validation Loss  0.68376 \t Accuracy: 0.51175\n",
      "Epoch: 5 \t Training: Loss 0.44792   \t Accuracy: 0.83736 \t Validation Loss  0.72203 \t Accuracy: 0.41324\n",
      "Epoch: 6 \t Training: Loss 0.43253   \t Accuracy: 0.83877 \t Validation Loss  0.60734 \t Accuracy: 0.71359\n",
      "Epoch: 7 \t Training: Loss 0.44568   \t Accuracy: 0.82656 \t Validation Loss  0.72533 \t Accuracy: 0.34339\n",
      "Epoch: 8 \t Training: Loss 0.4385   \t Accuracy: 0.83825 \t Validation Loss  0.67933 \t Accuracy: 0.56914\n",
      "Epoch: 9 \t Training: Loss 0.42681   \t Accuracy: 0.83468 \t Validation Loss  0.64987 \t Accuracy: 0.6905\n",
      "Epoch: 10 \t Training: Loss 0.42705   \t Accuracy: 0.83289 \t Validation Loss  0.60577 \t Accuracy: 0.79533\n",
      "Epoch: 11 \t Training: Loss 0.42061   \t Accuracy: 0.83704 \t Validation Loss  0.59563 \t Accuracy: 0.82438\n",
      "Epoch: 12 \t Training: Loss 0.41819   \t Accuracy: 0.84075 \t Validation Loss  0.5754 \t Accuracy: 0.82725\n",
      "Epoch: 13 \t Training: Loss 0.42111   \t Accuracy: 0.83992 \t Validation Loss  0.60905 \t Accuracy: 0.80524\n",
      "Epoch: 14 \t Training: Loss 0.41424   \t Accuracy: 0.83992 \t Validation Loss  0.58728 \t Accuracy: 0.82792\n",
      "Epoch: 15 \t Training: Loss 0.4024   \t Accuracy: 0.84509 \t Validation Loss  0.61626 \t Accuracy: 0.78103\n",
      "Epoch: 16 \t Training: Loss 0.39446   \t Accuracy: 0.85008 \t Validation Loss  0.58935 \t Accuracy: 0.81725\n",
      "Epoch: 17 \t Training: Loss 0.39887   \t Accuracy: 0.84631 \t Validation Loss  0.6136 \t Accuracy: 0.82012\n",
      "Epoch: 18 \t Training: Loss 0.3933   \t Accuracy: 0.85353 \t Validation Loss  0.60305 \t Accuracy: 0.71377\n",
      "Epoch: 19 \t Training: Loss 0.39859   \t Accuracy: 0.84816 \t Validation Loss  0.58421 \t Accuracy: 0.81972\n",
      "Epoch: 20 \t Training: Loss 0.3894   \t Accuracy: 0.85219 \t Validation Loss  0.56682 \t Accuracy: 0.8203\n",
      "Epoch: 21 \t Training: Loss 0.38642   \t Accuracy: 0.85238 \t Validation Loss  0.5967 \t Accuracy: 0.75502\n",
      "Epoch: 22 \t Training: Loss 0.38846   \t Accuracy: 0.85417 \t Validation Loss  0.59184 \t Accuracy: 0.79524\n",
      "Epoch: 23 \t Training: Loss 0.3737   \t Accuracy: 0.85813 \t Validation Loss  0.61022 \t Accuracy: 0.806\n",
      "Epoch: 24 \t Training: Loss 0.3822   \t Accuracy: 0.85206 \t Validation Loss  0.63957 \t Accuracy: 0.79067\n",
      "Epoch: 25 \t Training: Loss 0.37725   \t Accuracy: 0.85372 \t Validation Loss  0.6069 \t Accuracy: 0.79313\n",
      "Epoch: 26 \t Training: Loss 0.38753   \t Accuracy: 0.84682 \t Validation Loss  0.611 \t Accuracy: 0.78914\n",
      "Epoch: 27 \t Training: Loss 0.38483   \t Accuracy: 0.85755 \t Validation Loss  0.57381 \t Accuracy: 0.74596\n",
      "Epoch: 28 \t Training: Loss 0.37822   \t Accuracy: 0.8518 \t Validation Loss  0.61858 \t Accuracy: 0.69777\n",
      "Epoch: 29 \t Training: Loss 0.36909   \t Accuracy: 0.85449 \t Validation Loss  0.63426 \t Accuracy: 0.7374\n",
      "Epoch: 30 \t Training: Loss 0.38023   \t Accuracy: 0.85052 \t Validation Loss  0.58703 \t Accuracy: 0.80515\n",
      "Epoch: 31 \t Training: Loss 0.37988   \t Accuracy: 0.85429 \t Validation Loss  0.61664 \t Accuracy: 0.79067\n",
      "Epoch: 32 \t Training: Loss 0.34819   \t Accuracy: 0.87002 \t Validation Loss  0.52069 \t Accuracy: 0.82725\n",
      "Epoch: 33 \t Training: Loss 0.36998   \t Accuracy: 0.85513 \t Validation Loss  0.60884 \t Accuracy: 0.72453\n",
      "Epoch: 34 \t Training: Loss 0.35541   \t Accuracy: 0.86331 \t Validation Loss  0.59289 \t Accuracy: 0.82039\n",
      "Epoch: 35 \t Training: Loss 0.37579   \t Accuracy: 0.85257 \t Validation Loss  0.60373 \t Accuracy: 0.71875\n",
      "Epoch: 36 \t Training: Loss 0.36586   \t Accuracy: 0.86196 \t Validation Loss  0.6071 \t Accuracy: 0.75265\n",
      "Epoch: 37 \t Training: Loss 0.34069   \t Accuracy: 0.87097 \t Validation Loss  0.60459 \t Accuracy: 0.82792\n",
      "Epoch: 38 \t Training: Loss 0.34267   \t Accuracy: 0.87462 \t Validation Loss  0.59211 \t Accuracy: 0.82658\n",
      "Epoch: 39 \t Training: Loss 0.3526   \t Accuracy: 0.86126 \t Validation Loss  0.63342 \t Accuracy: 0.79954\n",
      "Epoch: 40 \t Training: Loss 0.32938   \t Accuracy: 0.87296 \t Validation Loss  0.61672 \t Accuracy: 0.81344\n",
      "Epoch: 41 \t Training: Loss 0.34918   \t Accuracy: 0.86548 \t Validation Loss  0.63875 \t Accuracy: 0.82487\n",
      "Epoch: 42 \t Training: Loss 0.34904   \t Accuracy: 0.86906 \t Validation Loss  0.59747 \t Accuracy: 0.82792\n",
      "Epoch: 43 \t Training: Loss 0.31586   \t Accuracy: 0.875 \t Validation Loss  0.63534 \t Accuracy: 0.82505\n",
      "Epoch: 44 \t Training: Loss 0.32793   \t Accuracy: 0.87749 \t Validation Loss  0.61487 \t Accuracy: 0.82725\n",
      "Epoch: 45 \t Training: Loss 0.32045   \t Accuracy: 0.88113 \t Validation Loss  0.61561 \t Accuracy: 0.8286\n",
      "Epoch: 46 \t Training: Loss 0.31524   \t Accuracy: 0.88286 \t Validation Loss  0.64046 \t Accuracy: 0.82658\n",
      "Epoch: 47 \t Training: Loss 0.33123   \t Accuracy: 0.87577 \t Validation Loss  0.62972 \t Accuracy: 0.82725\n",
      "Epoch: 48 \t Training: Loss 0.33456   \t Accuracy: 0.87008 \t Validation Loss  0.61728 \t Accuracy: 0.82591\n",
      "Epoch: 49 \t Training: Loss 0.3144   \t Accuracy: 0.88599 \t Validation Loss  0.63881 \t Accuracy: 0.82523\n",
      "Epoch: 50 \t Training: Loss 0.33133   \t Accuracy: 0.87334 \t Validation Loss  0.66225 \t Accuracy: 0.82591\n",
      "Final Best Model from Best Epoch 32 Test Loss = 0.5244472412502065, Test Accuracy = 0.8266488418859594\n"
     ]
    }
   ],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3,  32, 3, 1, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1, padding = 1)\n",
    "\n",
    "        self.fc1 = nn.Linear(64 * 10 * 10, 256)\n",
    "        self.fc2 = nn.Linear(256, 32)\n",
    "        self.fc3 = nn.Linear(32,   2)\n",
    "\n",
    "        self.dropout = nn.Dropout(p = 0.5)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(32)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.batchnorm1(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.batchnorm2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "torch.manual_seed(101)\n",
    "model_cnn = LeNet()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model_cnn.parameters(), lr = 0.001)\n",
    "train_losses, valid_losses, train_accurs, valid_accurs, test_loss, test_accu, best_epoch, epoch_times = train(50, model_cnn, train_loader, valid_loader, test_loader, optimizer, criterion, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
