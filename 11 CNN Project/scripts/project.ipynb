{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "from pathlib import Path\n",
    "import sys, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Subset\n",
    "from torchvision.utils import make_grid\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from PIL import Image\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from collections import Counter\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration: Baseline Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1351"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_path = 'D:/nodule/data/labels/'\n",
    "\n",
    "label = [pd.DataFrame(pd.read_csv(os.path.join(label_path, file), delim_whitespace = True)) for file in os.listdir(label_path)]\n",
    "df = pd.concat(label, ignore_index = True)\n",
    "df['types'] = [string.split('/') for string in df['image']]\n",
    "df['types'] = [string[0] for string in df['types']]\n",
    "df['image'] = [string.split('/') for string in df['image']]\n",
    "df['image'] = [string[1][6:-4] for string in df['image']]\n",
    "len(df[df['label'] == 0])\n",
    "len(df) - len(df[df['label'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoduleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, transform = None):\n",
    "        self.data_dir = data_dir\n",
    "        self.augment = transforms.Compose([\n",
    "            transforms.Resize((50, 50)),\n",
    "            transforms.RandomResizedCrop(50),\n",
    "            transforms.CenterCrop(40),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((50, 50)),\n",
    "            transforms.RandomCrop(32, padding = 2),\n",
    "            transforms.RandomRotation(90),\n",
    "            transforms.CenterCrop(40),\n",
    "            transforms.ToTensor(), \n",
    "            transforms.Normalize(mean = [0.485, 0.456, 0.406],\n",
    "                                 std  = [0.229, 0.224, 0.225])])\n",
    "        \n",
    "\n",
    "        self.images_dir = data_dir / 'images'\n",
    "        self.labels_dir = data_dir / 'labels'\n",
    "\n",
    "        self.train_images_dir = self.images_dir \n",
    "        self.val_images_dir   = self.images_dir  \n",
    "        self.test_images_dir  = self.images_dir  \n",
    "\n",
    "        self.train_labels_file = self.labels_dir  / 'trainlabels.txt'\n",
    "        self.val_labels_file   = self.labels_dir  / 'vallabels.txt'\n",
    "        self.test_labels_file  = self.labels_dir  / 'testlabels.txt'\n",
    "\n",
    "        self.train_data = self._load_data(self.train_images_dir, self.train_labels_file)\n",
    "        self.val_data   = self._load_data(self.val_images_dir, self.val_labels_file)\n",
    "        self.test_data  = self._load_data(self.test_images_dir, self.test_labels_file)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index < len(self.train_data):\n",
    "            images_dir = self.train_images_dir\n",
    "            data = self.train_data\n",
    "        elif index < len(self.train_data) + len(self.val_data):\n",
    "            images_dir = self.val_images_dir\n",
    "            data = self.val_data\n",
    "            index -= len(self.train_data)\n",
    "        else:\n",
    "            images_dir = self.test_images_dir\n",
    "            data = self.test_data\n",
    "            index -= (len(self.train_data) + len(self.val_data))\n",
    "\n",
    "        img_path = images_dir / data[index][0]\n",
    "        with open(img_path, 'rb') as f:\n",
    "            image = Image.open(f).convert('RGB')\n",
    "\n",
    "        label = data[index][1]\n",
    "        return self.transform(image), label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_data) + len(self.val_data) + len(self.test_data)\n",
    "\n",
    "    def _load_data(self, images_dir, labels_file):\n",
    "        with open(labels_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        data = []\n",
    "        for line in lines[1:]:\n",
    "            filename, label = line.strip().split()\n",
    "            filename = filename \n",
    "            label = int(label)\n",
    "            data.append((filename, label))\n",
    "        return data\n",
    "\n",
    "    def get_datasets(self):\n",
    "        train_dataset = Subset(self, range(len(self.train_data)))\n",
    "        test_dataset  = Subset(self, range(len(self.train_data),  len(self.train_data) + len(self.test_data)))\n",
    "        valid_dataset = Subset(self, range(len(self.train_data) + len(self.test_data),   len(self)))\n",
    "        return train_dataset, test_dataset, valid_dataset\n",
    "\n",
    "def GET_NODULEDATASET():\n",
    "    train_indices = list(range(0, len(dataset.train_data)))\n",
    "    valid_indices = list(range(len(dataset.train_data),  len(dataset.train_data) + len(dataset.val_data)))\n",
    "    test_indices  = list(range(len(dataset.train_data) + len(dataset.val_data), len(dataset)))\n",
    "\n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    valid_dataset = Subset(dataset, valid_indices)\n",
    "    test_dataset  = Subset(dataset, test_indices)\n",
    "    return train_dataset, valid_dataset, test_dataset\n",
    "\n",
    "data_dir = Path('D:/nodule/data/')\n",
    "dataset  = NoduleDataset(data_dir)\n",
    "train_dataset, valid_dataset, test_dataset = GET_NODULEDATASET()\n",
    "\n",
    "train_classes = [label for _, label in train_dataset]\n",
    "class_count = Counter(train_classes)\n",
    "class_weights = torch.Tensor([len(train_classes)/c for c in pd.Series(class_count).sort_index().values])\n",
    "class_samples = [0] * len(class_weights)\n",
    "\n",
    "for _, label in train_dataset:\n",
    "    class_samples[label] += 1\n",
    "weights = [class_weights[label] / class_samples[label] for _, label in train_dataset]\n",
    "sampler = WeightedRandomSampler(weights = weights, num_samples = len(weights), replacement = True)\n",
    "\n",
    "train_loader  = DataLoader(train_dataset, batch_size = 32, sampler = sampler)\n",
    "valid_loader  = DataLoader(valid_dataset, batch_size = 32, shuffle = True )\n",
    "test_loader   = DataLoader(test_dataset,  batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EarlyStopping:\n",
    "  def __init__(self, patience  = 1, delta = 0, path = 'checkpoint.pt'):\n",
    "    self.patience = patience\n",
    "    self.delta = delta\n",
    "    self.path= path\n",
    "    self.counter = 0\n",
    "    self.best_score = None\n",
    "    self.early_stop = False\n",
    "\n",
    "  def __call__(self, val_loss, model):\n",
    "    if self.best_score is None:\n",
    "      self.best_score = val_loss\n",
    "      self.save_checkpoint(model)\n",
    "    elif val_loss > self.best_score:\n",
    "      self.counter +=1\n",
    "      if self.counter >= self.patience:\n",
    "        self.early_stop = True \n",
    "    else:\n",
    "      self.best_score = val_loss\n",
    "      self.save_checkpoint(model)\n",
    "      self.counter = 0      \n",
    "\n",
    "  def save_checkpoint(self, model):\n",
    "    torch.save(model.state_dict(), self.path)\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct       = (rounded_preds == y).sum() \n",
    "    acc           = torch.mean(torch.eq(preds, y).float())\n",
    "    return acc\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "def train(num_epochs, model, train_loader, valid_loader, test_loader, optimizer, criterion, device):\n",
    "    best_valid_loss = float('inf')\n",
    "    train_losses, valid_losses = [], []\n",
    "    train_accurs, valid_accurs = [], []\n",
    "    epoch_times = []\n",
    "    list_best_epochs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss, train_accu = _train(model, train_loader, optimizer, criterion, device)\n",
    "        valid_loss, valid_accu = _evals(model, valid_loader, criterion, device)\n",
    "\n",
    "        print(f'Epoch: {epoch + 1} \\t Training: Loss {np.round(train_loss, 5)}   \\t Accuracy: {np.round(train_accu, 5)}\\\n",
    "                                   \\t Validation Loss  {np.round(valid_loss, 5)} \\t Accuracy: {np.round(valid_accu, 5)}')\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accurs.append(train_accu)\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_accurs.append(valid_accu)\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_epoch = epoch\n",
    "        list_best_epochs.append(best_epoch)\n",
    "    test_loss, test_accu  = _evals(best_model, test_loader, criterion, device)\n",
    "    print(f'Final Best Model from Best Epoch {best_epoch} Test Loss = {test_loss}, Test Acc = {test_accu}')\n",
    "    return train_losses, valid_losses, train_accurs, valid_accurs, test_loss, test_accu, best_epoch, epoch_times\n",
    "\n",
    "def _train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "    epoch_train_accu = 0\n",
    "\n",
    "    for idx, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        outputs = torch.sigmoid(outputs)[:, 1]\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "        accuracy = binary_accuracy(outputs, labels)\n",
    "        epoch_train_accu += accuracy.item()\n",
    "\n",
    "    epoch_train_loss = epoch_train_loss / len(train_loader)\n",
    "    epoch_train_accu = epoch_train_accu / len(train_loader)\n",
    "    return epoch_train_loss, epoch_train_accu\n",
    "\n",
    "def _evals(model, valid_loader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_valid_loss = 0\n",
    "    epoch_valid_accu = 0\n",
    "\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for id, data in enumerate(valid_loader):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            outputs = torch.sigmoid(outputs)[:, 1]\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            rounded_preds = torch.round(torch.sigmoid(outputs)).long().flatten().tolist()\n",
    "            all_predictions.extend(rounded_preds)\n",
    "\n",
    "            epoch_valid_loss += loss.item()\n",
    "            accuracy = binary_accuracy(outputs, labels)\n",
    "            epoch_valid_accu += accuracy.item()\n",
    "    epoch_valid_loss = epoch_valid_loss / len(valid_loader)\n",
    "    epoch_valid_accu = epoch_valid_accu / len(valid_loader)\n",
    "    return epoch_valid_loss, epoch_valid_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg19 = torchvision.models.vgg19(weights = True).to(device)\n",
    "model_vgg19.classifier[-1] = torch.nn.Linear(4096, 2)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model_vgg19.parameters(), lr = 1e-2, eps = 10e-06)\n",
    "\n",
    "train_losses, valid_losses, train_accurs, valid_accurs, test_loss, test_accu, best_epoch, epoch_times = train(2, model_vgg19, train_loader, valid_loader, test_loader, optimizer, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training: Loss 16.83681   \t Accuracy: 0.82554                                   \t Validation Loss  82.79232 \t Accuracy: 0.17208\n",
      "Epoch: 2 \t Training: Loss 16.27045   \t Accuracy: 0.8373                                   \t Validation Loss  82.79232 \t Accuracy: 0.17208\n",
      "Final Best Model from Best Epoch 0 Test Loss = 82.66488408107384, Test Acc = 0.17335115869839987\n"
     ]
    }
   ],
   "source": [
    "model_alexnet = models.alexnet(pretrained = True)\n",
    "\n",
    "model_alexnet.features[0] = torch.nn.Conv2d(3,  64, kernel_size = (3, 3), stride = (1, 1), padding = (1, 1))\n",
    "model_alexnet.features[2] = torch.nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0, dilation = 1, ceil_mode = False)\n",
    "model_alexnet.features[3] = torch.nn.Conv2d(64, 192, kernel_size = (3, 3), stride = (1, 1), padding = (1, 1))\n",
    "model_alexnet.features[5] = torch.nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0, dilation = 1, ceil_mode = False)\n",
    "\n",
    "model_alexnet.classifier[6] = torch.nn.Linear(4096, 2)\n",
    "model_alexnet.classifier[6].requires_grad = True\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model_alexnet.classifier.parameters(), lr = 0.001, eps = 10e-06)\n",
    "train_losses, valid_losses, train_accurs, valid_accurs, test_loss, test_accu, best_epoch, epoch_times = train(2, model_alexnet, train_loader, valid_loader, test_loader, optimizer, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training: Loss 0.49794   \t Accuracy: 0.0                                   \t Validation Loss  1.28798 \t Accuracy: 0.0\n",
      "Epoch: 2 \t Training: Loss 0.4538   \t Accuracy: 0.0                                   \t Validation Loss  1.34386 \t Accuracy: 0.0\n",
      "Final Best Model from Best Epoch 0 Test Loss = 1.312815418430403, Test Acc = 0.0\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3,  32, 3, 1, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1, padding = 1)\n",
    "\n",
    "        self.fc1 = nn.Linear(64 * 10 * 10, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 2)\n",
    "\n",
    "        self.dropout = nn.Dropout(p = 0.5)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(32)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.batchnorm1(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.batchnorm2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(101)\n",
    "model_cnn = LeNet()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model_cnn.parameters(), lr = 0.001)\n",
    "train_losses, valid_losses, train_accurs, valid_accurs, test_loss, test_accu, best_epoch, epoch_times = train(2, model_cnn, train_loader, valid_loader, test_loader, optimizer, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "class EarlyStopping:\n",
    "  def __init__(self, patience=1, delta=0, path = 'checkpoint.pt'):\n",
    "    self.patience = patience\n",
    "    self.delta = delta\n",
    "    self.path= path\n",
    "    self.counter = 0\n",
    "    self.best_score = None\n",
    "    self.early_stop = False\n",
    "\n",
    "  def __call__(self, val_loss, model):\n",
    "    if self.best_score is None:\n",
    "      self.best_score = val_loss\n",
    "      self.save_checkpoint(model)\n",
    "    elif val_loss > self.best_score:\n",
    "      self.counter +=1\n",
    "      if self.counter >= self.patience:\n",
    "        self.early_stop = True \n",
    "    else:\n",
    "      self.best_score = val_loss\n",
    "      self.save_checkpoint(model)\n",
    "      self.counter = 0      \n",
    "\n",
    "  def save_checkpoint(self, model):\n",
    "    torch.save(model.state_dict(), self.path)\n",
    "\n",
    "\n",
    "def fit_one_epoch(train_loader, epoch, num_epochs): \n",
    "    step_train = 0\n",
    "\n",
    "    train_losses = list() \n",
    "    train_acc = list()\n",
    "    model_vgg19.train()\n",
    "    for i, (images, targets) in enumerate(tqdm(train_loader)):\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        logits = model_vgg19(images)\n",
    "        loss = criterion(logits, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_losses.append(loss.item())\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        num_correct = sum(predictions.eq(targets))\n",
    "        running_train_acc = float(num_correct) / float(images.shape[0])\n",
    "        train_acc.append(running_train_acc)\n",
    "        \n",
    "    train_loss = torch.tensor(train_losses).mean()    \n",
    "    print(f'Epoch {epoch}/{num_epochs-1}')  \n",
    "    print(f'Training loss: {train_loss:.2f}')\n",
    "\n",
    "def val_one_epoch(val_loader):\n",
    "        val_losses = list()\n",
    "        val_accs = list()\n",
    "        \n",
    "        model_vgg19.eval()\n",
    "        step_val = 0\n",
    "        with torch.no_grad():\n",
    "            for (images, targets) in val_loader:\n",
    "                images = images.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                logits = model_vgg19(images)\n",
    "                loss = criterion(logits, targets)\n",
    "                val_losses.append(loss.item())      \n",
    "            \n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "                num_correct = sum(predictions.eq(targets))\n",
    "                running_val_acc = float(num_correct) / float(images.shape[0])\n",
    "\n",
    "                val_accs.append(running_val_acc)\n",
    "          \n",
    "            val_loss = torch.tensor(val_losses).mean()\n",
    "            val_acc = torch.tensor(val_accs).mean() \n",
    "        \n",
    "            print(f'Validation loss: {val_loss:.2f}')  \n",
    "            print(f'Validation accuracy: {val_acc:.2f}') \n",
    "\n",
    "def fit(train_loader, val_loader, num_epochs = 10, unfreeze_after = 5, checkpoint_dir = 'checkpoint.pt'):\n",
    "    for epoch in range(num_epochs):\n",
    "        fit_one_epoch(train_loader, epoch, num_epochs)\n",
    "        val_one_epoch(val_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 | Iteration  1 Loss: 0.12126\n",
      "Epoch  2 | Iteration  1 Loss: 6.06244\n",
      "Epoch  3 | Iteration  1 Loss: 0.03650\n",
      "Epoch  4 | Iteration  1 Loss: 0.05241\n",
      "Epoch  5 | Iteration  1 Loss: 0.01630\n",
      "Epoch  6 | Iteration  1 Loss: 0.00935\n",
      "Epoch  7 | Iteration  1 Loss: 0.00890\n",
      "Epoch  8 | Iteration  1 Loss: 0.00195\n",
      "Epoch  9 | Iteration  1 Loss: 0.00617\n",
      "Epoch 10 | Iteration  1 Loss: 0.00534\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.RMSprop(model_vgg19.parameters(), lr = learning_rate, eps = 10e-06)\n",
    "\n",
    "losses = []\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for idx, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs, labels\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_vgg19(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if idx == 0:\n",
    "            print('Epoch %2d | Iteration %2d Loss: %.5f' % (epoch + 1, idx + 1, running_loss / 100))\n",
    "            running_loss = 0.0    \n",
    "        elif idx == 5:\n",
    "            break\n",
    "        losses.append(loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
