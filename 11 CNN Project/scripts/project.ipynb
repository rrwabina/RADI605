{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "from pathlib import Path\n",
    "import sys, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Subset\n",
    "from torchvision.utils import make_grid\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from PIL import Image\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from collections import Counter\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration: Baseline Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1351"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_path = 'D:/nodule/data/labels/'\n",
    "\n",
    "label = [pd.DataFrame(pd.read_csv(os.path.join(label_path, file), delim_whitespace = True)) for file in os.listdir(label_path)]\n",
    "df = pd.concat(label, ignore_index = True)\n",
    "df['types'] = [string.split('/') for string in df['image']]\n",
    "df['types'] = [string[0] for string in df['types']]\n",
    "df['image'] = [string.split('/') for string in df['image']]\n",
    "df['image'] = [string[1][6:-4] for string in df['image']]\n",
    "len(df[df['label'] == 0])\n",
    "len(df) - len(df[df['label'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoduleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, transform = None):\n",
    "        self.data_dir = data_dir\n",
    "        self.augment = transforms.Compose([\n",
    "            transforms.Resize((50, 50)),\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((50, 50)),\n",
    "            transforms.RandomCrop(32, padding = 2),\n",
    "            transforms.RandomRotation(90),\n",
    "            transforms.ToTensor()])\n",
    "        \n",
    "\n",
    "        self.images_dir = data_dir / 'images'\n",
    "        self.labels_dir = data_dir / 'labels'\n",
    "\n",
    "        self.train_images_dir = self.images_dir \n",
    "        self.val_images_dir   = self.images_dir  \n",
    "        self.test_images_dir  = self.images_dir  \n",
    "\n",
    "        self.train_labels_file = self.labels_dir  / 'trainlabels.txt'\n",
    "        self.val_labels_file   = self.labels_dir  / 'vallabels.txt'\n",
    "        self.test_labels_file  = self.labels_dir  / 'testlabels.txt'\n",
    "\n",
    "        self.train_data = self._load_data(self.train_images_dir, self.train_labels_file)\n",
    "        self.val_data   = self._load_data(self.val_images_dir, self.val_labels_file)\n",
    "        self.test_data  = self._load_data(self.test_images_dir, self.test_labels_file)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index < len(self.train_data):\n",
    "            images_dir = self.train_images_dir\n",
    "            data = self.train_data\n",
    "        elif index < len(self.train_data) + len(self.val_data):\n",
    "            images_dir = self.val_images_dir\n",
    "            data = self.val_data\n",
    "            index -= len(self.train_data)\n",
    "        else:\n",
    "            images_dir = self.test_images_dir\n",
    "            data = self.test_data\n",
    "            index -= (len(self.train_data) + len(self.val_data))\n",
    "\n",
    "        img_path = images_dir / data[index][0]\n",
    "        with open(img_path, 'rb') as f:\n",
    "            image = Image.open(f).convert('RGB')\n",
    "\n",
    "        label = data[index][1]\n",
    "        return self.transform(image), label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_data) + len(self.val_data) + len(self.test_data)\n",
    "\n",
    "    def _load_data(self, images_dir, labels_file):\n",
    "        with open(labels_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        data = []\n",
    "        for line in lines[1:]:\n",
    "            filename, label = line.strip().split()\n",
    "            filename = filename \n",
    "            label = int(label)\n",
    "            data.append((filename, label))\n",
    "        return data\n",
    "\n",
    "    def get_datasets(self):\n",
    "        train_dataset = Subset(self, range(len(self.train_data)))\n",
    "        test_dataset  = Subset(self, range(len(self.train_data),  len(self.train_data) + len(self.test_data)))\n",
    "        valid_dataset = Subset(self, range(len(self.train_data) + len(self.test_data),   len(self)))\n",
    "        return train_dataset, test_dataset, valid_dataset\n",
    "\n",
    "def GET_NODULEDATASET():\n",
    "    train_indices = list(range(0, len(dataset.train_data)))\n",
    "    valid_indices = list(range(len(dataset.train_data),  len(dataset.train_data) + len(dataset.val_data)))\n",
    "    test_indices  = list(range(len(dataset.train_data) + len(dataset.val_data), len(dataset)))\n",
    "\n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    valid_dataset = Subset(dataset, valid_indices)\n",
    "    test_dataset  = Subset(dataset, test_indices)\n",
    "    return train_dataset, valid_dataset, test_dataset\n",
    "\n",
    "data_dir = Path('D:/nodule/data/')\n",
    "dataset  = NoduleDataset(data_dir)\n",
    "train_dataset, valid_dataset, test_dataset = GET_NODULEDATASET()\n",
    "\n",
    "train_classes = [label for _, label in train_dataset]\n",
    "class_count = Counter(train_classes)\n",
    "class_weights = torch.Tensor([len(train_classes)/c for c in pd.Series(class_count).sort_index().values])\n",
    "class_samples = [0] * len(class_weights)\n",
    "for _, label in train_dataset:\n",
    "    class_samples[label] += 1\n",
    "weights = [class_weights[label] / class_samples[label] for _, label in train_dataset]\n",
    "sampler = WeightedRandomSampler(weights = weights, num_samples = len(weights), replacement = False)\n",
    "\n",
    "train_loader  = DataLoader(train_dataset, batch_size = 32, sampler = sampler)\n",
    "valid_loader  = DataLoader(valid_dataset, batch_size = 32, shuffle = True )\n",
    "test_loader   = DataLoader(test_dataset,  batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg19 = torchvision.models.vgg19(weights = False).to(device)\n",
    "learning_rate = 1e-2\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_vgg19.parameters(), lr = learning_rate, eps = 10e-06)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    model_vgg19.train()\n",
    "    for idx, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model_vgg19(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if idx == 0:\n",
    "            print('Epoch %2d | Iteration %2d Train Loss: %.5f' % (epoch + 1, idx + 1, running_loss / 100))\n",
    "            running_loss = 0.0    \n",
    "        elif idx == 5:\n",
    "            break\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    model_vgg19.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        for val_data in valid_loader:\n",
    "            val_inputs, val_labels = val_data\n",
    "            val_outputs = model_vgg19(val_inputs)\n",
    "            val_loss += criterion(val_outputs, val_labels).item()\n",
    "        val_losses.append(val_loss / len(valid_loader))\n",
    "        print('Epoch %2d | Validation Loss: %.5f' % (epoch + 1, val_loss / len(valid_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Train Loss: 2507568158.2826667 \t Train Accuracy: 0.8477760736196319\n",
      "Epoch: 0 \t Valid Loss: 11695604.68292683 \t Valid Accuracy: 1.0\n",
      "Epoch: 0 \t Valid Loss: 11736353.460784314 \t Valid Accuracy: 1.0\n",
      "Final Best Model from Best Epoch 0 Test Loss = 11736353.460784314, Test Acc = 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([2507568158.2826667],\n",
       " [11695604.68292683],\n",
       " [0.8477760736196319],\n",
       " [1.0],\n",
       " 11736353.460784314,\n",
       " 1.0,\n",
       " 0,\n",
       " [])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EarlyStopping:\n",
    "  def __init__(self, patience=1, delta=0, path = 'checkpoint.pt'):\n",
    "    self.patience = patience\n",
    "    self.delta = delta\n",
    "    self.path= path\n",
    "    self.counter = 0\n",
    "    self.best_score = None\n",
    "    self.early_stop = False\n",
    "\n",
    "  def __call__(self, val_loss, model):\n",
    "    if self.best_score is None:\n",
    "      self.best_score = val_loss\n",
    "      self.save_checkpoint(model)\n",
    "    elif val_loss > self.best_score:\n",
    "      self.counter +=1\n",
    "      if self.counter >= self.patience:\n",
    "        self.early_stop = True \n",
    "    else:\n",
    "      self.best_score = val_loss\n",
    "      self.save_checkpoint(model)\n",
    "      self.counter = 0      \n",
    "\n",
    "  def save_checkpoint(self, model):\n",
    "    torch.save(model.state_dict(), self.path)\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    predicted = torch.argmax(preds, dim=1) \n",
    "    rounded_preds = torch.round(torch.sigmoid(predicted))\n",
    "    correct       = (rounded_preds == y).sum() \n",
    "    acc           = torch.mean(torch.eq(predicted, label).float())\n",
    "    return acc\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "def train(num_epochs, model, train_loader, valid_loader, test_loader, optimizer, criterion, device):\n",
    "    best_valid_loss = float('inf')\n",
    "    train_losses, valid_losses = [], []\n",
    "    train_accurs, valid_accurs = [], []\n",
    "    epoch_times = []\n",
    "    list_best_epochs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss, train_accu = _train(epoch, model, train_loader, optimizer, criterion, device)\n",
    "        valid_loss, valid_accu = _evals(epoch, model, valid_loader, criterion, device)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accurs.append(train_accu)\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_accurs.append(valid_accu)\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_epoch = epoch\n",
    "        list_best_epochs.append(best_epoch)\n",
    "    test_loss, test_accu  = _evals(epoch, best_model, test_loader, criterion, device)\n",
    "    print(f'Final Best Model from Best Epoch {best_epoch} Test Loss = {test_loss}, Test Acc = {test_accu}')\n",
    "    return train_losses, valid_losses, train_accurs, valid_accurs, test_loss, test_accu, best_epoch, epoch_times\n",
    "\n",
    "def _train(epoch, model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "    epoch_train_accu = 0\n",
    "\n",
    "    for idx, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "        accuracy = binary_accuracy(outputs, labels)\n",
    "        epoch_train_accu += accuracy.item()\n",
    "\n",
    "    epoch_train_loss = epoch_train_loss / len(train_loader)\n",
    "    epoch_train_accu = epoch_train_accu / len(train_loader)\n",
    "    print(f'Epoch: {epoch} \\t Train Loss: {epoch_train_loss} \\t Train Accuracy: {epoch_train_accu}')\n",
    "    return epoch_train_loss, epoch_train_accu\n",
    "\n",
    "def _evals(epoch, model, valid_loader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_valid_loss = 0\n",
    "    epoch_valid_accu = 0\n",
    "\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for id, data in enumerate(valid_loader):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            rounded_preds = torch.round(torch.sigmoid(outputs)).long().flatten().tolist()\n",
    "            all_predictions.extend(rounded_preds)\n",
    "\n",
    "            epoch_valid_loss += loss.item()\n",
    "            accuracy = binary_accuracy(outputs, labels)\n",
    "            epoch_valid_accu += accuracy.item()\n",
    "    epoch_valid_loss = epoch_valid_loss / len(valid_loader)\n",
    "    epoch_valid_accu = epoch_valid_accu / len(valid_loader)\n",
    "    print(f'Epoch: {epoch} \\t Valid Loss: {epoch_valid_loss} \\t Valid Accuracy: {epoch_valid_accu}')\n",
    "    return epoch_valid_loss, epoch_valid_accu\n",
    "\n",
    "\n",
    "model_vgg19 = torchvision.models.vgg19(weights = True).to(device)\n",
    "learning_rate = 1e-2\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_vgg19.parameters(), lr = learning_rate, eps = 10e-06)\n",
    "\n",
    "train_losses, valid_losses, train_accurs, valid_accurs, test_loss, test_accu, best_epoch, epoch_times = train(1, model_vgg19, train_loader, valid_loader, test_loader, optimizer, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "class EarlyStopping:\n",
    "  def __init__(self, patience=1, delta=0, path = 'checkpoint.pt'):\n",
    "    self.patience = patience\n",
    "    self.delta = delta\n",
    "    self.path= path\n",
    "    self.counter = 0\n",
    "    self.best_score = None\n",
    "    self.early_stop = False\n",
    "\n",
    "  def __call__(self, val_loss, model):\n",
    "    if self.best_score is None:\n",
    "      self.best_score = val_loss\n",
    "      self.save_checkpoint(model)\n",
    "    elif val_loss > self.best_score:\n",
    "      self.counter +=1\n",
    "      if self.counter >= self.patience:\n",
    "        self.early_stop = True \n",
    "    else:\n",
    "      self.best_score = val_loss\n",
    "      self.save_checkpoint(model)\n",
    "      self.counter = 0      \n",
    "\n",
    "  def save_checkpoint(self, model):\n",
    "    torch.save(model.state_dict(), self.path)\n",
    "\n",
    "\n",
    "def fit_one_epoch(train_loader, epoch, num_epochs): \n",
    "    step_train = 0\n",
    "\n",
    "    train_losses = list() \n",
    "    train_acc = list()\n",
    "    model_vgg19.train()\n",
    "    for i, (images, targets) in enumerate(tqdm(train_loader)):\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        logits = model_vgg19(images)\n",
    "        loss = criterion(logits, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_losses.append(loss.item())\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        num_correct = sum(predictions.eq(targets))\n",
    "        running_train_acc = float(num_correct) / float(images.shape[0])\n",
    "        train_acc.append(running_train_acc)\n",
    "        \n",
    "    train_loss = torch.tensor(train_losses).mean()    \n",
    "    print(f'Epoch {epoch}/{num_epochs-1}')  \n",
    "    print(f'Training loss: {train_loss:.2f}')\n",
    "\n",
    "def val_one_epoch(val_loader):\n",
    "        val_losses = list()\n",
    "        val_accs = list()\n",
    "        \n",
    "        model_vgg19.eval()\n",
    "        step_val = 0\n",
    "        with torch.no_grad():\n",
    "            for (images, targets) in val_loader:\n",
    "                images = images.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                logits = model_vgg19(images)\n",
    "                loss = criterion(logits, targets)\n",
    "                val_losses.append(loss.item())      \n",
    "            \n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "                num_correct = sum(predictions.eq(targets))\n",
    "                running_val_acc = float(num_correct) / float(images.shape[0])\n",
    "\n",
    "                val_accs.append(running_val_acc)\n",
    "          \n",
    "            val_loss = torch.tensor(val_losses).mean()\n",
    "            val_acc = torch.tensor(val_accs).mean() \n",
    "        \n",
    "            print(f'Validation loss: {val_loss:.2f}')  \n",
    "            print(f'Validation accuracy: {val_acc:.2f}') \n",
    "\n",
    "def fit(train_loader, val_loader, num_epochs = 10, unfreeze_after = 5, checkpoint_dir = 'checkpoint.pt'):\n",
    "    for epoch in range(num_epochs):\n",
    "        fit_one_epoch(train_loader, epoch, num_epochs)\n",
    "        val_one_epoch(val_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "epochs = 10\n",
    "train_losses, test_losses   = [], []\n",
    "train_correct, test_correct = [], []\n",
    "\n",
    "learning_rate = 1e-3\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_vgg19.parameters(), lr = learning_rate, eps = 10e-06)\n",
    "\n",
    "for i in range(epochs):\n",
    "    trn_corr = 0\n",
    "    tst_corr = 0\n",
    "    model_vgg19.train()\n",
    "    for b, (X_train, y_train) in enumerate(train_loader):\n",
    "        b += 1\n",
    "        y_pred = model_vgg19(X_train)\n",
    "        loss = criterion(y_pred, y_train)\n",
    "        predicted = torch.max(y_pred.data, 1)[1]\n",
    "        batch_corr = (predicted == y_train).sum()\n",
    "        trn_corr += batch_corr\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f'Epoch: {i:2}  Batch: {b:4} [{10*b:6}]  Loss: {loss.item():10.8f}  \\tAccuracy: {trn_corr.item()/(b):7.3f}%')\n",
    "    train_losses.append(loss)\n",
    "    train_correct.append(trn_corr)\n",
    "\n",
    "    model_vgg19.eval()  \n",
    "    with torch.no_grad():\n",
    "        for b, (X_test, y_test) in enumerate(valid_loader):\n",
    "            y_val = model_vgg19(X_test)\n",
    "\n",
    "            predicted = torch.max(y_val.data, 1)[1] \n",
    "            tst_corr += (predicted == y_test).sum()\n",
    "    \n",
    "    loss = criterion(y_val, y_test)\n",
    "    test_losses.append(loss)\n",
    "    test_correct.append(tst_corr)\n",
    "print(f'\\nDuration: {time.time() - start_time:.0f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 | Iteration  1 Loss: 0.12126\n",
      "Epoch  2 | Iteration  1 Loss: 6.06244\n",
      "Epoch  3 | Iteration  1 Loss: 0.03650\n",
      "Epoch  4 | Iteration  1 Loss: 0.05241\n",
      "Epoch  5 | Iteration  1 Loss: 0.01630\n",
      "Epoch  6 | Iteration  1 Loss: 0.00935\n",
      "Epoch  7 | Iteration  1 Loss: 0.00890\n",
      "Epoch  8 | Iteration  1 Loss: 0.00195\n",
      "Epoch  9 | Iteration  1 Loss: 0.00617\n",
      "Epoch 10 | Iteration  1 Loss: 0.00534\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.RMSprop(model_vgg19.parameters(), lr = learning_rate, eps = 10e-06)\n",
    "\n",
    "losses = []\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for idx, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs, labels\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_vgg19(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if idx == 0:\n",
    "            print('Epoch %2d | Iteration %2d Loss: %.5f' % (epoch + 1, idx + 1, running_loss / 100))\n",
    "            running_loss = 0.0    \n",
    "        elif idx == 5:\n",
    "            break\n",
    "        losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3, 1)\n",
    "        self.fc1 = nn.Linear(5*5*16, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84,10)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = F.relu(self.conv1(X))\n",
    "        X = F.max_pool2d(X, 2, 2)\n",
    "        X = F.relu(self.conv2(X))\n",
    "        X = F.max_pool2d(X, 2, 2)\n",
    "        X = X.view(-1, 46656)\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = F.relu(self.fc2(X))\n",
    "        X = self.fc3(X)\n",
    "        return X\n",
    "    \n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(in_features=16*5*5, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
    "        self.fc3 = nn.Linear(in_features=84, out_features=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool2(nn.functional.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 46656)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
