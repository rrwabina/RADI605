{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RADI605: Modern Machine Learning**\n",
    "\n",
    "### Assignment: Expectation Maximization\n",
    "**Romen Samuel Rodis Wabina** <br>\n",
    "Student, PhD Data Science in Healthcare and Clinical Informatics <br>\n",
    "Clinical Epidemiology and Biostatistics, Faculty of Medicine (Ramathibodi Hospital) <br>\n",
    "Mahidol University\n",
    "\n",
    "Note: In case of Python Markdown errors, you may access the assignment through this GitHub [Link](https://github.com/rrwabina/RADI605)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please identify some pros and cons of the EM algorithm compare with the K-means"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KMeans and EM clutering (also known as Mixture Models) are both unsupervised clustering models. K-Means groups data points using distance from the cluster centroid while the EM clustering uses a probabilistic assignment of data points to clusters. In this item, we listed all the advantages and disadvantages of EM algorithm and compare it with the K-Means.   \n",
    "\n",
    "### Advantages of EM Algorithm (+ KMeans comparison)\n",
    "1. The EM algorithm can **handle data with missing values** since it is a probabilistic model that can estimate missing values. To handle missing data, the algorithm iteratively estimates model parameters that best fit the observed data, while also estimating the missing values. This can lead to more accurate and robust estimates of the missing data, as well as more **reliable clustering** or density estimates. Moreover, EM algorithm, as an imputation technique, can overcome one of the most commonly faced problems in clinical patient survey research <code>(Ghomrawi et al., 2011)</code>. \n",
    "\n",
    "2. The EM algorithm can **model more complex data distributions** than the K-means algorithm, as it can handle non-spherical clusters and clusters with different shapes and orientations.\n",
    "\n",
    "3. The EM, as a probabilistic approach, **can quantify uncertainties** to measure the model's reliability to its predictions. The **K-Means, on one hand, cannot measure uncertainty since it uses a deterministic method** where each data point is assigned to a single cluster center based on the minimum distance. Several research have utilized EM algorithm as an uncertainty quantification technique <code>(Malan et al., 2020)</code>. For instance, <code>Gao et al. (2020)</code> adopted the mixture model (i.e., EM algorithm) to analyze the production data with reservoir properties. The authors utilized EM to produce production forecasts that are reliable, i.e., high prediction confidence, with production data observed in the blind test period.\n",
    "\n",
    "4. The Expectation Maximization algorithm can **effectively handle datasets that have high correlations** among variables, as well as those in which the variances of the variables are not equal. This advantage is possible because EM, by default, utilizes the **Mahalanobis Distance** as its distance measurement. The Mahalanobis distance measures the similarity between data points and the cluster centers - which takes into account the covariance structure of the data. By doing so, EM clustering is able to account for the correlations between the variables, leading to more accurate clustering results. \n",
    "\n",
    "    - The **Euclidean distance in KMeans, however, performs poorly compared to Mahalanobis distance in EM** because the Euclidean distance assumes that the variables are independent and identically distributed (IID), and it does not account for the covariance structure of the data. Therefore, the use of Euclidean distance limits KMeans' ability to identify complex non-linear structures <code>(Davidson, I. 2002)</code>. This KMean's problem was further evaluated by <code>Patel & Kushwaha (2020)</code> by comparing **K-Means and EM** to evaluate cluster representativeness of the two methods for heterogeneity in resource usage of **Cloud workloads**. Its conclusions are very similar to <code>(Davidson, I. 2002)</code> where clusters obtained using K-Means (with Euclidean distance) give a relatively abstracted information while EM offers better grouping with separate clusters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disadvantages of EM Algorithm (+ KMeans comparison)\n",
    "1. **EM algorithm converges slowly with large fractions of missing data**. Missing data values make it difficult for EM to estimate the posterior probabilities since large portion of the data is missing. This results to slow convergence or even failure to converge. For instance, <code>Little and Rubin (2002)</code> adopted EM algorithm as an imputation technique in predicting children's weight in a school entry. They have found out that higher rates of missing values within the data provide slow model training performance. While there are several suggestions in the literature for speeding up the convergence of the EM algorithm, such as hybrid methods with Aitken acceleration, over-relaxation, line searches, Newton methods, and conjugate gradients, their general behavior is not always clear and they may not result in monotonic increases in the log likelihood over iterations <code>(McLachlan et. al, 2004)</code>.\n",
    "\n",
    "2. The **EM algorithm will converge very slowly if a poor choice of initial value was used**. This scenario is due to EM's sensitivity in initial conditions. The EM algorithm uses initial values, mostly denoted as $\\theta$, to compute the posterior probabilities of the data belonging to each cluster. The M-step then updates the parameters of the probability distribution using the posterior. If the initial values are not close to the true values, the E-step may have calculated poor posterior probability. Then the E-step may assign a proportion of the data points to the wrong cluster, leading to inaccurate parameter estimates in the M-step. This leads to **slow convergence because the EM algorithm is stuck in a local optima** and may provide an **inaccurate parameter estimates in the M-step**. \n",
    "    - On one hand, **KMeans algorithm is less sensitive to the initial values** since it uses a **deterministic** approach to find the cluster centers.\n",
    "\n",
    "3. The **EM algorithm can produce biased parameter estimates** and **underestimate the standard errors** due to its reliance on maximum likelihood estimation (MLE) and assumption of missing at random (MAR) data. Recall that EM primarily uses MLE, which presumes that the data are produced by a particular parametric distribution. Yet, it's possible that the real distribution differs from the assumedÂ distribution, which would result in inaccurate parameter estimates <code>(Meng & Rubin, 1996)</code>.\n",
    "    - In contrast, the KMeans algorithm does not make any assumptions about the data distribution due to its deterministic nature. However, it can still **suffer from bias if the cluster centers are sensitive to the choice of initial values** or if the data have a complex structure that cannot be well approximated by KMeans.\n",
    "\n",
    "4. The **iterative nature of EM clustering can result in convergence to a local optima instead of the global optimum, similar to KMeans clustering**, as both methods begin with an initial estimate of the cluster centers and iteratively optimize a specific objective. This sensitivity to the initial parameter values may lead to convergence to multiple local optima. As such, caution is warranted when interpreting results and multiple runs using different initializations may be necessary to improve the likelihood of converging to the global optimum <code>(Ghahramani & Hinton, 1996)</code>.\n",
    "\n",
    "5. The **EM algorithm can be computationally expensive**, especially for large datasets or high-dimensional data, as it requires iterative calculations of the expectation and maximization steps. The EM clustering requires you to calculate the posterior distribution of the latent variables at every iterations, which can be computationally expensive. The **KMeans, however, is computationally less expensive since it only calculates the simple Euclidean distance calculations** and updates the centroids at each iteration. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "- Davidson, I. (2002). Understanding K-means non-hierarchical clustering. Computer Science Department of State University of New York (SUNY), Albany.\n",
    "- Gao, G., Jiang, H., Vink, J. C., Chen, C., El Khamra, Y., & Ita, J. J. (2020). Gaussian mixture model fitting method for uncertainty quantification by conditioning to production data. Computational Geosciences, 24(2), 663-681.\n",
    "- Ghahramani, Z., & Hinton, G. E. (1996). The EM algorithm for mixtures of factor analyzers (Vol. 60). Technical Report CRG-TR-96-1, University of Toronto.\n",
    "- Ghomrawi, H. M., Mandl, L. A., Rutledge, J., Alexiades, M. M., & Mazumdar, M. (2011). Is there a role for expectation maximization imputation in addressing missing data in research using WOMAC questionnaire?    Comparison to the standard mean approach and a tutorial. BMC musculoskeletal disorders, 12(1), 1-7.\n",
    "- Little, R. J., & Rubin, D. B. (2019). Statistical analysis with missing data (Vol. 793). John Wiley & Sons.\n",
    "- Malan, L., Smuts, C. M., Baumgartner, J., & Ricci, C. (2020). Missing data imputation via the expectation-maximization algorithm can improve principal component analysis aimed at deriving biomarker profiles and dietary patterns. Nutrition Research, 75, 67-76.\n",
    "- Meng, X. L., & Rubin, D. B. (1992). Performing likelihood ratio tests with the EM algorithm. Biometrika, 79(1), 103-111.\n",
    "- McLachlan, G. J., Krishnan, T., & Ng, S. K. (2004). The EM algorithm (No. 2004, 24). Papers.\n",
    "- Patel, E., & Kushwaha, D. S. (2020). Clustering cloud workloads: K-means vs gaussian mixture model. Procedia Computer Science, 171, 158-167."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import xgboost as xgb\n",
    "import statsmodels.api as sm\n",
    "import math \n",
    "\n",
    "from functools import reduce\n",
    "from time import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the <code>impute-data.csv</code>, please perform the missing value imputation by using the Expectation Maximization in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/Impute-data.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check every columns from the given dataset to check their respective unique values. We have seen below that columns <code>A7_Score</code> and <code>A9_Score</code> contains <code>np.nan</code>. The aforementioned columns also deals with string values as its data type. Meanwhile, the remaining values only consists of binary values 0 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values A6_Score: \t [1 0]\n",
      "Unique values A7_Score: \t ['1' '0' '?' nan]\n",
      "Unique values A8_Score: \t [1 0]\n",
      "Unique values A9_Score: \t ['1' '0' '?' nan]\n",
      "Unique values A10_Score: \t [0 1]\n"
     ]
    }
   ],
   "source": [
    "for column in data.columns:\n",
    "    print(f'Unique values {column}: \\t {data[column].unique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A6_Score      int64\n",
       "A7_Score     object\n",
       "A8_Score      int64\n",
       "A9_Score     object\n",
       "A10_Score     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each columns of  <code>A7_Score</code> and <code>A9_Score</code> consist of one null value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A6_Score     0\n",
       "A7_Score     1\n",
       "A8_Score     0\n",
       "A9_Score     1\n",
       "A10_Score    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values A6_Score: \t [1 0]\n",
      "Unique values A7_Score: \t ['1' '0' nan]\n",
      "Unique values A8_Score: \t [1 0]\n",
      "Unique values A9_Score: \t ['1' '0' nan]\n",
      "Unique values A10_Score: \t [0 1]\n"
     ]
    }
   ],
   "source": [
    "for column in ['A7_Score', 'A9_Score']:\n",
    "    data[column] = data[column].replace('?', math.nan)\n",
    "for column in data.columns:\n",
    "    print(f'Unique values {column}: \\t {data[column].unique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A6_Score       int64\n",
       "A7_Score     float64\n",
       "A8_Score       int64\n",
       "A9_Score     float64\n",
       "A10_Score      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for column in ['A7_Score', 'A9_Score']:\n",
    "    data[column] = [int(x) if str(x).isdigit() else np.nan for x in data[column]]\n",
    "    # data[column] = data[column].astype('Int64')\n",
    "data.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code>Mu_tilde</code> represents the expected values of the missing data given the observed data and the current estimates of the covariance matrix and means. Since we are dealing with binary data, we need to represent the expected values as probabilities, naturally ranging between 0 and 1. Therefore, it is only vital the <code>Mu_tilde</code> must satisfy the condition that its values are binary, since the expected missing values should be binary. To solve this, we use the logistic function to map the expected values to the range $[0, 1]$. Therefore, we can compute the entries of Mu_tilde using the logistic function applied to the expected values obtained from the previous iteration of the EM algorithm.\n",
    "\n",
    "We have also noted in Question 1 that EM adopts covariance matrix for data imputation. However, we need to ensure that the imputed values are plausible, hence, the covariance matrix must be positiive semi-definite. This property ensures that the covariance matrix can be used to model the relationship between the variables in a consistent and meaningful way. When the missing values are binary, the covariance matrix can be represented as a matrix of pairwise covariances between binary variables. However, since the covariance between two binary variables can take on any value between -1 and 1, we can simplify the covariance matrix by converting all non-zero values to 1. This results in a binary covariance matrix which is always positive semi-definite, ensuring that the imputed values are plausible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7826087 , 0.43478261, 0.65217391, 0.86956522, 0.60869565])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def binary_cov(X):\n",
    "    nr, nc = X.shape\n",
    "    X_bar = X - X.mean(axis=0)\n",
    "    cov = X_bar.T @ X_bar / nr\n",
    "    cov[np.diag_indices_from(cov)] = X.mean(axis=0) * (1 - X.mean(axis=0))\n",
    "    return cov\n",
    "\n",
    "def logistic(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def impute_em(X, max_iter = 3000, eps = 1e-08):\n",
    "    nr, nc = X.shape\n",
    "    C = np.isnan(X) == False\n",
    "    one_to_nc = np.arange(1, nc + 1, step = 1)\n",
    "    M = one_to_nc * (C == False) - 1\n",
    "    O = one_to_nc * C - 1\n",
    "    Mu = np.nanmean(X, axis = 0)\n",
    "    observed_rows = np.where(np.isnan(sum(X.T)) == False)[0]\n",
    "    S = binary_cov(X[observed_rows, ])\n",
    "    if np.isnan(S).any():\n",
    "        S = np.diag(np.nanvar(X, axis = 0))\n",
    "    \n",
    "    # Start updating\n",
    "    Mu_tilde, S_tilde = {}, {}\n",
    "    X_tilde = X.copy()\n",
    "    no_conv = True\n",
    "    iteration = 0\n",
    "    while no_conv and iteration < max_iter:\n",
    "        for i in range(nr):\n",
    "            S_tilde[i] = np.zeros(nc ** 2).reshape(nc, nc)\n",
    "            if set(O[i, ]) != set(one_to_nc - 1): \n",
    "                M_i, O_i = M[i, ][M[i, ] != -1], O[i, ][O[i, ] != -1]\n",
    "                S_MM = S[np.ix_(M_i, M_i)]\n",
    "                S_MO = S[np.ix_(M_i, O_i)]\n",
    "                S_OM = S_MO.T\n",
    "                S_OO = S[np.ix_(O_i, O_i)]\n",
    "                \n",
    "                # Modify the computation of Mu_tilde[i] to use the logistic function instead of the matrix multiplication.\n",
    "                Mu_tilde[i] = logistic(Mu[np.ix_(M_i)] + S_MO @ np.linalg.inv(S_OO) @ (X_tilde[i, O_i] - Mu[np.ix_(O_i)]))\n",
    "\n",
    "                # Logistic condition. If value > p = 0.50, then the missing data must be 1. Otherwise, it is 0.\n",
    "                X_tilde[i, M_i] = np.clip(np.round(Mu_tilde[i]), 0, 1) \n",
    "                S_MM_O = S_MM - S_MO @ np.linalg.inv(S_OO) @ S_OM\n",
    "                S_tilde[i][np.ix_(M_i, M_i)] = S_MM_O\n",
    "        Mu_new  = np.mean(X_tilde, axis = 0)\n",
    "\n",
    "        # Modify the computation of S_new to use a binary covariance matrix\n",
    "        S_new = binary_cov(X_tilde) + reduce(np.add, S_tilde.values()) / nr\n",
    "        no_conv = np.linalg.norm(Mu - Mu_new) >= eps or np.linalg.norm(S - S_new, ord = 2) >= eps\n",
    "        Mu = Mu_new\n",
    "        S  = S_new\n",
    "        iteration += 1\n",
    "    \n",
    "    result = {\n",
    "                'mu': Mu,\n",
    "                'Sigma': S,\n",
    "                'X_imputed': X_tilde,\n",
    "                'C': C,\n",
    "                'iteration': iteration\n",
    "             }\n",
    "    \n",
    "    return result\n",
    "\n",
    "X = data.to_numpy()\n",
    "result_imputed = impute_em(X)\n",
    "\n",
    "result_imputed['mu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.17013233,  0.0510397 ,  0.01134216,  0.05860113, -0.0415879 ],\n",
       "       [ 0.0510397 ,  0.26424549,  0.02079395, -0.03024575,  0.03969754],\n",
       "       [ 0.01134216,  0.02079395,  0.2268431 , -0.00189036,  0.03780718],\n",
       "       [ 0.05860113, -0.03024575, -0.00189036,  0.12129021, -0.00756144],\n",
       "       [-0.0415879 ,  0.03969754,  0.03780718, -0.00756144,  0.23818526]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_imputed['Sigma']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final imputation is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 0.],\n",
       "       [0., 0., 0., 1., 1.],\n",
       "       [0., 0., 0., 1., 1.],\n",
       "       [1., 0., 1., 1., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 1., 0.],\n",
       "       [1., 1., 1., 1., 0.],\n",
       "       [0., 1., 1., 0., 1.],\n",
       "       [1., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 1., 0.],\n",
       "       [1., 0., 1., 1., 0.],\n",
       "       [1., 1., 0., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 0., 1., 1., 1.],\n",
       "       [1., 0., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [0., 0., 1., 0., 1.],\n",
       "       [1., 0., 1., 1., 1.],\n",
       "       [0., 0., 1., 1., 0.],\n",
       "       [1., 1., 0., 1., 1.],\n",
       "       [1., 0., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_imputed['X_imputed']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
