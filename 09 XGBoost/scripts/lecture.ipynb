{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.datasets import load_breast_cancer, load_diabetes, load_wine\n",
    "from sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores(scores):\n",
    "    print(\"Scores: {0}\\nMean: {1:.3f}\\nStd: {2:.3f}\".format(scores, np.mean(scores), np.std(scores)))\n",
    "\n",
    "def report_best_scores(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                results['mean_test_score'][candidate], results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2401475171547707\n"
     ]
    }
   ],
   "source": [
    "diabetes = load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(objective = \"reg:squarederror\", random_state=42)\n",
    "xgb_model.fit(X, y)\n",
    "\n",
    "y_pred = xgb_model.predict(X)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "print(np.sqrt(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[212   0]\n",
      " [  0 357]]\n"
     ]
    }
   ],
   "source": [
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "xgb_model = xgb.XGBClassifier(objective = \"binary:logistic\", random_state = 42)\n",
    "xgb_model.fit(X, y)\n",
    "y_pred = xgb_model.predict(X)\n",
    "print(confusion_matrix(y, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[59  0  0]\n",
      " [ 0 71  0]\n",
      " [ 0  0 48]]\n"
     ]
    }
   ],
   "source": [
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "xgb_model = xgb.XGBClassifier(objective = \"multi:softprob\", random_state=42)\n",
    "xgb_model.fit(X, y)\n",
    "y_pred = xgb_model.predict(X)\n",
    "print(confusion_matrix(y, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [63.93059871 61.44356415 67.49238017 69.51815605 59.9786771 ]\n",
      "Mean: 64.473\n",
      "Std: 3.584\n"
     ]
    }
   ],
   "source": [
    "diabetes = load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = []\n",
    "for train_index, test_index in kfold.split(X): \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\")\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "    scores.append(mean_squared_error(y_test, y_pred))\n",
    "display_scores(np.sqrt(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [62.80101886 65.78389959 62.21211593 66.40836809 67.3001013 ]\n",
      "Mean: 64.901\n",
      "Std: 2.022\n"
     ]
    }
   ],
   "source": [
    "xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=42)\n",
    "scores = cross_val_score(xgb_model, X, y, scoring=\"neg_mean_squared_error\", cv=5)\n",
    "display_scores(np.sqrt(-scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 200 candidates, totalling 600 fits\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.464 (std: 0.012)\n",
      "Parameters: {'colsample_bytree': 0.7902634929450308, 'gamma': 0.1424202471887338, 'learning_rate': 0.041066084206359835, 'max_depth': 2, 'n_estimators': 101, 'subsample': 0.8010716092915446}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "diabetes = load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "params = {\n",
    "    \"colsample_bytree\": uniform(0.7, 0.3),\n",
    "    \"gamma\": uniform(0, 0.5),\n",
    "    \"learning_rate\": uniform(0.03, 0.3), # default 0.1 \n",
    "    \"max_depth\": randint(2, 6), # default 3\n",
    "    \"n_estimators\": randint(100, 150), # default 100\n",
    "    \"subsample\": uniform(0.6, 0.4)}\n",
    "\n",
    "search = RandomizedSearchCV(xgb_model, param_distributions=params, random_state=42, n_iter=200, \n",
    "cv=3, verbose=1, n_jobs=1, return_train_score=True)\n",
    "search.fit(X, y)\n",
    "\n",
    "report_best_scores(search.cv_results_, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: 0.464 (std: 0.012)\n",
      "Parameters: {'colsample_bytree': 0.7902634929450308, 'gamma': 0.1424202471887338, 'learning_rate': 0.041066084206359835, 'max_depth': 2, 'n_estimators': 101, 'subsample': 0.8010716092915446}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_best_scores(search.cv_results_, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Early Stopping Rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.96348\tvalidation_0-error:0.04895\tvalidation_0-error@0.6:0.07692\n",
      "[1]\tvalidation_0-auc:0.97201\tvalidation_0-error:0.04895\tvalidation_0-error@0.6:0.06294\n",
      "[2]\tvalidation_0-auc:0.97035\tvalidation_0-error:0.04196\tvalidation_0-error@0.6:0.05594\n",
      "[3]\tvalidation_0-auc:0.97930\tvalidation_0-error:0.04196\tvalidation_0-error@0.6:0.06294\n",
      "[4]\tvalidation_0-auc:0.97857\tvalidation_0-error:0.03497\tvalidation_0-error@0.6:0.04895\n",
      "[5]\tvalidation_0-auc:0.97784\tvalidation_0-error:0.04196\tvalidation_0-error@0.6:0.04196\n",
      "[6]\tvalidation_0-auc:0.98408\tvalidation_0-error:0.03497\tvalidation_0-error@0.6:0.04895\n",
      "[7]\tvalidation_0-auc:0.98450\tvalidation_0-error:0.04895\tvalidation_0-error@0.6:0.04196\n",
      "[8]\tvalidation_0-auc:0.98616\tvalidation_0-error:0.04895\tvalidation_0-error@0.6:0.04895\n",
      "[9]\tvalidation_0-auc:0.99105\tvalidation_0-error:0.04895\tvalidation_0-error@0.6:0.04196\n",
      "[10]\tvalidation_0-auc:0.99126\tvalidation_0-error:0.04895\tvalidation_0-error@0.6:0.04196\n",
      "[11]\tvalidation_0-auc:0.99064\tvalidation_0-error:0.04196\tvalidation_0-error@0.6:0.04196\n",
      "[12]\tvalidation_0-auc:0.99147\tvalidation_0-error:0.03497\tvalidation_0-error@0.6:0.04196\n",
      "[13]\tvalidation_0-auc:0.99209\tvalidation_0-error:0.04196\tvalidation_0-error@0.6:0.04196\n",
      "[14]\tvalidation_0-auc:0.99209\tvalidation_0-error:0.04196\tvalidation_0-error@0.6:0.03497\n",
      "[15]\tvalidation_0-auc:0.99147\tvalidation_0-error:0.04196\tvalidation_0-error@0.6:0.03497\n",
      "[16]\tvalidation_0-auc:0.99168\tvalidation_0-error:0.04196\tvalidation_0-error@0.6:0.03497\n",
      "[17]\tvalidation_0-auc:0.99126\tvalidation_0-error:0.04196\tvalidation_0-error@0.6:0.03497\n",
      "[18]\tvalidation_0-auc:0.99189\tvalidation_0-error:0.04196\tvalidation_0-error@0.6:0.03497\n",
      "[19]\tvalidation_0-auc:0.99168\tvalidation_0-error:0.04196\tvalidation_0-error@0.6:0.04196\n"
     ]
    }
   ],
   "source": [
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", n_estimators=20, random_state=42, eval_metric=[\"auc\", \"error\", \"error@0.6\"])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "xgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)])\n",
    "y_pred = xgb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[51  3]\n",
      " [ 3 86]]\n"
     ]
    }
   ],
   "source": [
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42, eval_metric=\"auc\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "xgb_model.fit(X_train, y_train, early_stopping_rounds=10, eval_set=[(X_test, y_test)], verbose=False)\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boost vs XGBoost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, gradient boosting works by iteratively adding decision trees to the model, with each subsequent tree attempting to correct the errors of the previous tree. The trees are built by recursively splitting the data into smaller and smaller subsets based on the values of the features. In datasets with many features, this can result in a very large number of possible splits and can make it difficult for the algorithm to find the most informative splits.\n",
    "\n",
    "XGBoost uses several techniques to address this problem, such as subsampling and column subsampling, which can help to reduce the number of features that the algorithm considers. Subsampling involves training each tree on a random subset of the training data, which can help to reduce the variance of the model and prevent overfitting. Column subsampling involves randomly selecting a subset of the features for each tree, which can help to reduce the impact of noisy or irrelevant features.\n",
    "\n",
    "In addition, XGBoost uses regularization techniques, such as L1 and L2 regularization, which can help to reduce the impact of uninformative features and prevent overfitting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of features > Number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57, 12582)\n",
      "==============Gradient Boosting Classifier=================\n",
      "1.0\n",
      "[[4 0 0]\n",
      " [0 3 0]\n",
      " [0 0 5]]\n",
      "=============XGBoost Classifier===============\n",
      "0.8333333333333334\n",
      "[[4 0 0]\n",
      " [0 2 1]\n",
      " [1 0 4]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "data = pd.read_csv('../data/leukemia.csv', sep = ',',header=None)\n",
    "X = data.iloc[:, :12582]\n",
    "y = data.iloc[:,-1:]\n",
    "print(X.shape)\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y_encoded,test_size = 0.2, random_state = 0)\n",
    "\n",
    "print('==============Gradient Boosting Classifier=================')\n",
    "clf = GradientBoostingClassifier(n_estimators = 100, learning_rate = 1.0, max_depth=1, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))\n",
    "y_pred = clf.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('=============XGBoost Classifier===============')\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=100,eta=0.3, gamma=0.5, random_state=0)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "print(xgb_model.score(X_test, y_test))\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred_xgb))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of features < Number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 2)\n",
      "==============Gradient Boosting Classifier=================\n",
      "0.9932\n",
      "[[4971   29]\n",
      " [  39 4961]]\n",
      "=============XGBoost Classifier===============\n",
      "0.9933\n",
      "[[4973   27]\n",
      " [  40 4960]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X_train, y_train = make_blobs(n_samples=1000000, centers=2,random_state=7, cluster_std=1.5)\n",
    "X_test, y_test = make_blobs(n_samples=10000, centers=2, random_state=7, cluster_std=1.5)\n",
    "print(X_train.shape)\n",
    "print('==============Gradient Boosting Classifier=================')\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))\n",
    "y_pred = clf.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('=============XGBoost Classifier===============')\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=100,eta=0.3, gamma=0.5, random_state=0)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "print(xgb_model.score(X_test, y_test))\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred_xgb))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6a2dc670f3436433c0efae6fb324965c1072d8aef0b90287abce79ee9328779"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
