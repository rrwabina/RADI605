{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Gradient Boost Tree by showing step-by-step calculation and setting $M = 1$, $\\alpha = 0.3$ and <code>max_depth = 3</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import auc, accuracy_score, mean_squared_error\n",
    "from sklearn.impute  import KNNImputer\n",
    "from sklearn.utils import class_weight\n",
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from time import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From <code>Breast_Cancer.csv</code> data, please create an Adaptive Boost, a Gradient Boost Tree and a XGBoostTree to classify between cancer and non-cancer by using Python. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the original dataset into a dataframe using <code>pandas</code>. The original dataset has 15,718 samples with 46 columns. It is vital to check its properties, such as the data types and null values, to preprocess the data prior data modeling. Results show that the dataset consists of null values that needs to be imputed. Several columns, however, contains too much null values that imputation may not necessarily and sufficiently work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset: \t(15718, 46)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../data/Breast_Cancer.csv')\n",
    "print(f'Original dataset: \\t{data.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a function that determines columns that can be utilized for this probllem. While there's no concrete threshold in eliminating columns based on the number of their null values, most studies use 10%. This means that columns who have null values **less than 10%** of the samples will be retained in the dataset, otherwise removed. Results show that there are 36 features that remain in the dataset, such as <code>severity, bdate, marital, and etc.</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['severity', 'bdate', 'marital', 'breastfa', 'nobreast', 'sebreast',\n",
      "       'nosecon', 'ovafam', 'noova', 'cancerfa', 'weight', 'height', 'age_new',\n",
      "       'ht_new', 'dm_new', 'ckd_new', 'dlp_new', 'agemen', 'parity',\n",
      "       'noparity', 'abort', 'noabort', 'nochild', 'brefed', 'menopau', 'hrt',\n",
      "       'oc', 'smok', 'childsmo', 'housesmo', 'worksmo', 'alc', 'nobrefed',\n",
      "       'dur_brefed', 'diag_cancer', 'inj'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def getcolumns_rate(dataframe, rate = 0.10): \n",
    "  less_than_50pct_nonnull = (dataframe.isnull().sum() / dataframe.shape[0]) < rate\n",
    "  return dataframe.columns[less_than_50pct_nonnull]\n",
    "\n",
    "remove_columns = getcolumns_rate(data)\n",
    "print(remove_columns)\n",
    "data = data[remove_columns]\n",
    "\n",
    "data['bdate'] = data['bdate'].str[-4:]\n",
    "data['age']   = 2565 - data['bdate'].astype('int')\n",
    "data = data.drop('bdate', axis = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to determine the categorical features in the dataset. These features should be encoded into numerical values because machine learning models expect features to be either floats or integers. Using the function <code>get_objectColumns</code>, we determined the columns that needs to be converted into numerical values. Some of these features include <code>severity, marital, breastfa, and etc.</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['severity', 'marital', 'breastfa', 'sebreast', 'ovafam', 'cancerfa', 'ht_new', 'dm_new', 'ckd_new', 'dlp_new', 'parity', 'abort', 'brefed', 'menopau', 'hrt', 'oc', 'smok', 'childsmo', 'housesmo', 'worksmo', 'alc', 'diag_cancer']\n"
     ]
    }
   ],
   "source": [
    "def get_objectColumns(dataframe, type = 'object'):\n",
    "    return list(dataframe.select_dtypes(include = type).columns)\n",
    "\n",
    "encode_columns = get_objectColumns(data)\n",
    "print(encode_columns)\n",
    "for column in encode_columns:\n",
    "    data[column] = LabelEncoder().fit_transform(data[column])\n",
    "\n",
    "data['noova'] = data['noova'].replace([9.0], [3.0], inplace = False)\n",
    "encode_columns = ['nobreast', 'nosecon', 'noova', 'inj', 'noparity']\n",
    "for column in encode_columns:\n",
    "    data[column] = LabelEncoder().fit_transform(data[column])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the number of null values in the dataset using <code>data.isnull().sum()</code>. The table below suggests that these four features consists of null values. Since we already removed features that have  null values greater than 10%, this implies that these four features can be imputed using statistical techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>weight</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>height</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agemen</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dur_brefed</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0\n",
       "weight      True\n",
       "height      True\n",
       "agemen      True\n",
       "dur_brefed  True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check = pd.DataFrame(data.isnull().sum() > 0)\n",
    "check[check[0] == True]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used two techniques to impute the datasets - K-Nearest Neighbors (KNN) and Median. We adopted median than mean since median is more robust and it can mitigate the effect of outliers. The features <code>height</code> and <code>width</code> uses median imputation. The remaining features <code>agemen</code> and <code>dur_brefed</code> are imputed through KNN since studies show that KNN also work with numerical and categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_knn(df, column_name, n_neighbors = 5):\n",
    "    df_imputed = df.copy()\n",
    "    imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "    imputer.fit(df_imputed[[column_name]])\n",
    "    df_imputed[column_name] = imputer.transform(df_imputed[[column_name]])\n",
    "    return df_imputed\n",
    "\n",
    "def impute_average(df, columns, impute_type = 'median'):\n",
    "    df_imputed = df.copy()\n",
    "    for col in columns:\n",
    "        if impute_type == 'mean':\n",
    "            impute_val = df_imputed[col].mean()\n",
    "        elif impute_type == 'median':\n",
    "            impute_val = df_imputed[col].median()\n",
    "        else:\n",
    "            raise ValueError('Invalid imputation type')\n",
    "        df_imputed[col].fillna(impute_val, inplace=True)\n",
    "    return df_imputed\n",
    "\n",
    "\n",
    "data = impute_knn(data, 'agemen')\n",
    "data = impute_knn(data, 'dur_brefed')\n",
    "data = impute_average(data, ['weight'], 'median')\n",
    "data = impute_average(data, ['height'], 'median')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of samples didn't change from the original dataset. The features, however, was reduced into 36 features - which was originall 46 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset: \t(15718, 36)\n"
     ]
    }
   ],
   "source": [
    "print(f'Preprocessed dataset: \\t{data.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine all the codes above in a single Jupyter cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset: \t(15718, 46)\n",
      "Preprocessed dataset: \t(15718, 36)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../data/Breast_Cancer.csv')\n",
    "print(f'Original dataset: \\t{data.shape}')\n",
    "\n",
    "def check_binary(df):\n",
    "    return [column for column in df.columns if df[column].isin(['Yes','No']).any()]\n",
    "\n",
    "def check_pseudobi(df):\n",
    "    return [column for column in df.columns if df[column].isin(np.arange(0.0, 5.0)).any()]\n",
    "\n",
    "def check_marital(df):\n",
    "    return [column for column in df.columns if df[column].isin(np.unique(data['marital'])).any()]\n",
    "\n",
    "def getcolumns_rate(dataframe, rate = 0.10): \n",
    "  less_than_50pct_nonnull = (dataframe.isnull().sum() / dataframe.shape[0]) < rate\n",
    "  return dataframe.columns[less_than_50pct_nonnull]\n",
    "\n",
    "def get_objectColumns(dataframe, type = 'object'):\n",
    "    return list(dataframe.select_dtypes(include = type).columns)\n",
    "\n",
    "def variance_inflation(df):\n",
    "    vif = pd.DataFrame()\n",
    "    vif['VIF'] = [variance_inflation_factor(data.values, i) for i in range(data.shape[1])]\n",
    "    vif['Features'] = data.columns\n",
    "    vif.sort_values(by = 'VIF', ascending = False)\n",
    "\n",
    "def get_columnsnull(df):\n",
    "    null_columns = df.columns[df.isnull().any()]\n",
    "    return null_columns.tolist()\n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"Scores: {0}\\nMean: {1:.3f}\\nStd: {2:.3f}\".format(scores, np.mean(scores), np.std(scores)))\n",
    "\n",
    "def impute_knn(df, column_name, n_neighbors = 5):\n",
    "    df_imputed = df.copy()\n",
    "    imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "    imputer.fit(df_imputed[[column_name]])\n",
    "    df_imputed[column_name] = imputer.transform(df_imputed[[column_name]])\n",
    "    return df_imputed\n",
    "\n",
    "def impute_average(df, columns, impute_type = 'median'):\n",
    "    df_imputed = df.copy()\n",
    "    for col in columns:\n",
    "        if impute_type == 'mean':\n",
    "            impute_val = df_imputed[col].mean()\n",
    "        elif impute_type == 'median':\n",
    "            impute_val = df_imputed[col].median()\n",
    "        else:\n",
    "            raise ValueError('Invalid imputation type')\n",
    "        df_imputed[col].fillna(impute_val, inplace=True)\n",
    "    return df_imputed\n",
    "\n",
    "def report_best_scores(results, n_top = 3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                results['mean_test_score'][candidate], results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n",
    "\n",
    "remove_columns = getcolumns_rate(data)\n",
    "data = data[remove_columns]\n",
    "\n",
    "data['bdate'] = data['bdate'].str[-4:]\n",
    "data['age']   = 2565 - data['bdate'].astype('int')\n",
    "data = data.drop('bdate', axis = 1)\n",
    "\n",
    "encode_columns = get_objectColumns(data)\n",
    "for column in encode_columns:\n",
    "    data[column] = LabelEncoder().fit_transform(data[column])\n",
    "\n",
    "data['noova'] = data['noova'].replace([9.0], [3.0], inplace = False)\n",
    "encode_columns = ['nobreast', 'nosecon', 'noova', 'inj', 'noparity']\n",
    "for column in encode_columns:\n",
    "    data[column] = LabelEncoder().fit_transform(data[column])\n",
    "\n",
    "data = impute_knn(data, 'agemen')\n",
    "data = impute_knn(data, 'dur_brefed')\n",
    "data = impute_average(data, ['weight'], 'median')\n",
    "data = impute_average(data, ['height'], 'median')\n",
    "\n",
    "print(f'Preprocessed dataset: \\t{data.shape}')\n",
    "X = data.drop('diag_cancer', axis = 1).to_numpy()\n",
    "y = data['diag_cancer'].to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(model, X_train, y_train, X_test, y_test):\n",
    "    start = time()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    class_names = ['Cancer', 'No Cancer']\n",
    "    \n",
    "    confusion = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    print(\"Accuracy Score: \", np.round(accuracy_score(y_test, y_pred), 3))\n",
    "    print('Confusion Matrix : \\n', confusion)\n",
    "    \n",
    "    print(classification_report(y_test, y_pred, target_names = class_names))\n",
    "    print(f\"\\t Fit and predict time: {np.round(time() - start, 3)} seconds\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 240 candidates, totalling 1200 fits\n",
      "Accuracy Score:  0.994\n",
      "Confusion Matrix : \n",
      " [[   0   27]\n",
      " [   0 4689]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Cancer       0.00      0.00      0.00        27\n",
      "   No Cancer       0.99      1.00      1.00      4689\n",
      "\n",
      "    accuracy                           0.99      4716\n",
      "   macro avg       0.50      0.50      0.50      4716\n",
      "weighted avg       0.99      0.99      0.99      4716\n",
      "\n",
      "\t Fit and predict time: 0.15 seconds\n"
     ]
    }
   ],
   "source": [
    "def train_adaboost(X_train, y_train, X_test, y_test, use_smote = False):\n",
    "    if use_smote is True:\n",
    "        smote = SMOTE(sampling_strategy = 'minority', k_neighbors = 10, random_state = 42)\n",
    "        X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    param_grid = {'max_features': ['auto', 'sqrt', 'log2'],\n",
    "                  'ccp_alpha'   : np.arange(0.001, 0.1, 0.01),\n",
    "                  'max_depth'   : np.arange(1, 5),\n",
    "                  'criterion'   : ['gini', 'entropy']}\n",
    "\n",
    "    tree_class = DecisionTreeClassifier(max_depth = 5, random_state = 1024)\n",
    "    grid_search = GridSearchCV(estimator = tree_class, \n",
    "                            param_grid = param_grid, \n",
    "                            cv = 5, \n",
    "                            verbose = True,\n",
    "                            scoring = 'accuracy')\n",
    "\n",
    "    grid_search = grid_search.fit(X_train, y_train)\n",
    "    adaboost_dct = AdaBoostClassifier(grid_search.best_estimator_, \n",
    "                                    n_estimators  = 10, \n",
    "                                    random_state  = 42, \n",
    "                                    learning_rate = 0.001)\n",
    "    model_train(adaboost_dct, X_train, y_train, X_test, y_test)\n",
    "\n",
    "train_adaboost(X_train, y_train, X_test, y_test, use_smote = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 240 candidates, totalling 1200 fits\n",
      "Accuracy Score:  0.984\n",
      "Confusion Matrix : \n",
      " [[  21    6]\n",
      " [  69 4620]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Cancer       0.23      0.78      0.36        27\n",
      "   No Cancer       1.00      0.99      0.99      4689\n",
      "\n",
      "    accuracy                           0.98      4716\n",
      "   macro avg       0.62      0.88      0.68      4716\n",
      "weighted avg       0.99      0.98      0.99      4716\n",
      "\n",
      "\t Fit and predict time: 0.525 seconds\n"
     ]
    }
   ],
   "source": [
    "train_adaboost(X_train, y_train, X_test, y_test, use_smote = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 400 candidates, totalling 2000 fits\n",
      "Accuracy Score:  0.993\n",
      "Confusion Matrix : \n",
      " [[  14   13]\n",
      " [  18 4671]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Cancer       0.44      0.52      0.47        27\n",
      "   No Cancer       1.00      1.00      1.00      4689\n",
      "\n",
      "    accuracy                           0.99      4716\n",
      "   macro avg       0.72      0.76      0.74      4716\n",
      "weighted avg       0.99      0.99      0.99      4716\n",
      "\n",
      "\t Fit and predict time: 105.625 seconds\n"
     ]
    }
   ],
   "source": [
    "def train_gradientboost(X_train, y_train, X_test, y_test, use_smote = False):\n",
    "    if use_smote is True:\n",
    "        smote = SMOTE(sampling_strategy = 'minority', k_neighbors = 10, random_state = 42)\n",
    "        X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "        \n",
    "    param_grid = {'learning_rate': [0.1, 0.01, 0.001, 0.0001],\n",
    "                'subsample': [1.0, 0.5],\n",
    "                'max_features': np.arange(0, 10), \n",
    "                'max_depth': np.arange(0, 5)}\n",
    "\n",
    "    gradient_boosting = GradientBoostingClassifier(n_estimators = 10, \n",
    "                                    learning_rate = 0.1, \n",
    "                                    random_state = 0)\n",
    "\n",
    "    grid_search = GridSearchCV(estimator = gradient_boosting, \n",
    "                            param_grid = param_grid, \n",
    "                            cv = 5, \n",
    "                            verbose = True,\n",
    "                            scoring = 'accuracy')\n",
    "    model_train(grid_search, X_train, y_train, X_test, y_test)\n",
    "\n",
    "train_gradientboost(X_train, y_train, X_test, y_test, use_smote = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 400 candidates, totalling 2000 fits\n",
      "Accuracy Score:  0.988\n",
      "Confusion Matrix : \n",
      " [[  10   17]\n",
      " [  40 4649]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Cancer       0.20      0.37      0.26        27\n",
      "   No Cancer       1.00      0.99      0.99      4689\n",
      "\n",
      "    accuracy                           0.99      4716\n",
      "   macro avg       0.60      0.68      0.63      4716\n",
      "weighted avg       0.99      0.99      0.99      4716\n",
      "\n",
      "\t Fit and predict time: 355.522 seconds\n"
     ]
    }
   ],
   "source": [
    "train_gradientboost(X_train, y_train, X_test, y_test, use_smote = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.996\n",
      "Confusion Matrix : \n",
      " [[  11   16]\n",
      " [   3 4686]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Cancer       0.79      0.41      0.54        27\n",
      "   No Cancer       1.00      1.00      1.00      4689\n",
      "\n",
      "    accuracy                           1.00      4716\n",
      "   macro avg       0.89      0.70      0.77      4716\n",
      "weighted avg       1.00      1.00      1.00      4716\n",
      "\n",
      "\t Fit and predict time: 0.159 seconds\n"
     ]
    }
   ],
   "source": [
    "def train_xgboost(X_train, y_train, X_test, y_test, use_smote = False):\n",
    "    if use_smote is True:\n",
    "        smote = SMOTE(sampling_strategy = 'minority', k_neighbors = 10, random_state = 42)\n",
    "        X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "    xgboost = xgb.XGBClassifier(n_estimators = 10, \n",
    "                                objective  = 'binary:logistic',\n",
    "                                reg_lambda = 0,\n",
    "                                gamma  = 1,\n",
    "                                max_depth = 6,\n",
    "                                eta = 0.3,\n",
    "                                scale_pos_weight = 10)\n",
    "    xgboost.fit(X_train, y_train)\n",
    "    model_train(xgboost, X_train, y_train, X_test, y_test)\n",
    "train_xgboost(X_train, y_train, X_test, y_test, use_smote = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.996\n",
      "Confusion Matrix : \n",
      " [[  12   15]\n",
      " [   4 4685]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Cancer       0.75      0.44      0.56        27\n",
      "   No Cancer       1.00      1.00      1.00      4689\n",
      "\n",
      "    accuracy                           1.00      4716\n",
      "   macro avg       0.87      0.72      0.78      4716\n",
      "weighted avg       1.00      1.00      1.00      4716\n",
      "\n",
      "\t Fit and predict time: 0.383 seconds\n"
     ]
    }
   ],
   "source": [
    "train_xgboost(X_train, y_train, X_test, y_test, use_smote = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6a2dc670f3436433c0efae6fb324965c1072d8aef0b90287abce79ee9328779"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
